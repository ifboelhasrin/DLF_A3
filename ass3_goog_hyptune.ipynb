{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19-08-2004</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>2.490664</td>\n",
       "      <td>897427216</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20-08-2004</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>2.515820</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-08-2004</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>2.758411</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-08-2004</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>2.770615</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-08-2004</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>2.614201</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.640104</td>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Low      Open     Volume      High     Close  \\\n",
       "0  19-08-2004  2.390042  2.490664  897427216  2.591785  2.499133   \n",
       "1  20-08-2004  2.503118  2.515820  458857488  2.716817  2.697639   \n",
       "2  23-08-2004  2.716070  2.758411  366857939  2.826406  2.724787   \n",
       "3  24-08-2004  2.579581  2.770615  306396159  2.779581  2.611960   \n",
       "4  25-08-2004  2.587302  2.614201  184645512  2.689918  2.640104   \n",
       "\n",
       "   Adjusted Close  \n",
       "0        2.499133  \n",
       "1        2.697639  \n",
       "2        2.724787  \n",
       "3        2.611960  \n",
       "4        2.640104  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GOOG stock data\n",
    "\n",
    "path = 'data/stock_market_data/GOOG.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = data[['Close']].copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (3215, 19, 1)\n",
      "y_train.shape = (3215, 1)\n",
      "x_val.shape = (459, 19, 1)\n",
      "y_val.shape = (459, 1)\n",
      "x_test.shape = (918, 19, 1)\n",
      "y_test.shape = (918, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set with sliding window method\n",
    "\n",
    "def split_data(stock, lookback, val_size=0.1):\n",
    "    data_raw = stock.to_numpy()  # Convert to numpy array\n",
    "    data = []\n",
    "\n",
    "    # Create sequences of length `lookback`\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    test_set_size = int(np.round(0.2 * data.shape[0]))\n",
    "    val_set_size = int(np.round(val_size * data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (test_set_size + val_set_size)\n",
    "\n",
    "    # Split into training, validation, and test sets\n",
    "    x_train = data[:train_set_size, :-1, :]\n",
    "    y_train = data[:train_set_size, -1, :]\n",
    "    \n",
    "    x_val = data[train_set_size:train_set_size+val_set_size, :-1, :]\n",
    "    y_val = data[train_set_size:train_set_size+val_set_size, -1, :]\n",
    "    \n",
    "    x_test = data[train_set_size+val_set_size:, :-1, :]\n",
    "    y_test = data[train_set_size+val_set_size:, -1, :]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "lookback = 20 # choose sequence length\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_data(price, lookback)\n",
    "\n",
    "# Check shapes\n",
    "print('x_train.shape =', x_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('x_val.shape =', x_val.shape)\n",
    "print('y_val.shape =', y_val.shape)\n",
    "print('x_test.shape =', x_test.shape)\n",
    "print('y_test.shape =', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "def train_model(model, \n",
    "                criterion, \n",
    "                optimiser, \n",
    "                x_train, y_train,\n",
    "                x_val=None, y_val=None, \n",
    "                num_epochs = 100):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        if x_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_val)\n",
    "                val_epoch_loss = criterion(val_outputs, y_val)\n",
    "                val_loss.append(val_epoch_loss.item())\n",
    "        else:\n",
    "            val_loss.append(None)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            if x_val is not None:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}, val loss: {val_epoch_loss.item()}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time: {training_time}')\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "x_val = torch.from_numpy(x_val).type(torch.Tensor)\n",
    "\n",
    "# Hyperparameters defining\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "# Tunable hyperparameters\n",
    "models = [RNN, LSTM, GRU]\n",
    "hidden_dim = [32, 64, 128]\n",
    "num_layers = [2, 3]\n",
    "num_epochs = [100, 200]\n",
    "learning_rate = [0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.5176611542701721, val loss: 0.12967555224895477\n",
      "Epoch 10, train loss: 0.03814549744129181, val loss: 0.29097673296928406\n",
      "Epoch 20, train loss: 0.021211137995123863, val loss: 0.24434535205364227\n",
      "Epoch 30, train loss: 0.013555091805756092, val loss: 0.18204212188720703\n",
      "Epoch 40, train loss: 0.008368130773305893, val loss: 0.09528682380914688\n",
      "Epoch 50, train loss: 0.0041682240553200245, val loss: 0.01865115389227867\n",
      "Epoch 60, train loss: 0.005085524637252092, val loss: 0.03639042750000954\n",
      "Epoch 70, train loss: 0.0006712671602144837, val loss: 0.018145635724067688\n",
      "Epoch 80, train loss: 0.00044841435737907887, val loss: 0.0015860969433560967\n",
      "Epoch 90, train loss: 0.00011557993275346234, val loss: 0.0032537037041038275\n",
      "Training time: 3.0275866985321045\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.4599166512489319, val loss: 0.01790691167116165\n",
      "Epoch 10, train loss: 0.027186047285795212, val loss: 0.156875342130661\n",
      "Epoch 20, train loss: 0.03484225645661354, val loss: 0.2977215051651001\n",
      "Epoch 30, train loss: 0.019793039187788963, val loss: 0.13917921483516693\n",
      "Epoch 40, train loss: 0.015968870371580124, val loss: 0.17505259811878204\n",
      "Epoch 50, train loss: 0.015736930072307587, val loss: 0.1937434822320938\n",
      "Epoch 60, train loss: 0.014424897730350494, val loss: 0.1555749922990799\n",
      "Epoch 70, train loss: 0.01356340479105711, val loss: 0.15615110099315643\n",
      "Epoch 80, train loss: 0.012702703475952148, val loss: 0.14756155014038086\n",
      "Epoch 90, train loss: 0.011665081605315208, val loss: 0.1297120451927185\n",
      "Training time: 2.9027578830718994\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.9151701331138611, val loss: 0.18988928198814392\n",
      "Epoch 10, train loss: 0.7810679078102112, val loss: 0.13909859955310822\n",
      "Epoch 20, train loss: 0.659159779548645, val loss: 0.0966922864317894\n",
      "Epoch 30, train loss: 0.5450918078422546, val loss: 0.06112603843212128\n",
      "Epoch 40, train loss: 0.4358476996421814, val loss: 0.03242800384759903\n",
      "Epoch 50, train loss: 0.3312031924724579, val loss: 0.012245845049619675\n",
      "Epoch 60, train loss: 0.2341628074645996, val loss: 0.003373265964910388\n",
      "Epoch 70, train loss: 0.1500810831785202, val loss: 0.008753716945648193\n",
      "Epoch 80, train loss: 0.08479801565408707, val loss: 0.029959753155708313\n",
      "Epoch 90, train loss: 0.04213285446166992, val loss: 0.06497617810964584\n",
      "Training time: 2.873499870300293\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6136428117752075, val loss: 0.08710949867963791\n",
      "Epoch 10, train loss: 0.042407289147377014, val loss: 0.30770206451416016\n",
      "Epoch 20, train loss: 0.015102195553481579, val loss: 0.13019809126853943\n",
      "Epoch 30, train loss: 0.006969099398702383, val loss: 0.013918088749051094\n",
      "Epoch 40, train loss: 0.0005561670986935496, val loss: 0.033348292112350464\n",
      "Epoch 50, train loss: 0.001708685071207583, val loss: 0.0012803628342226148\n",
      "Epoch 60, train loss: 0.000581374391913414, val loss: 0.0007560933008790016\n",
      "Epoch 70, train loss: 0.0004068522830493748, val loss: 0.005703472066670656\n",
      "Epoch 80, train loss: 0.0001796824362827465, val loss: 0.0032194731757044792\n",
      "Epoch 90, train loss: 0.00015928315406199545, val loss: 0.0009678339120000601\n",
      "Epoch 100, train loss: 0.00012065257760696113, val loss: 0.0007687747711315751\n",
      "Epoch 110, train loss: 9.906119521474466e-05, val loss: 0.0006515838904306293\n",
      "Epoch 120, train loss: 8.58160710777156e-05, val loss: 0.0004012371937278658\n",
      "Epoch 130, train loss: 7.712811202509329e-05, val loss: 0.0002829357690643519\n",
      "Epoch 140, train loss: 6.986870721448213e-05, val loss: 0.00026722109760157764\n",
      "Epoch 150, train loss: 6.46651242277585e-05, val loss: 0.00028055760776624084\n",
      "Epoch 160, train loss: 6.074009070289321e-05, val loss: 0.0003188827831763774\n",
      "Epoch 170, train loss: 5.772520671598613e-05, val loss: 0.0003693430044222623\n",
      "Epoch 180, train loss: 5.541943392017856e-05, val loss: 0.0004313198442105204\n",
      "Epoch 190, train loss: 5.364918615669012e-05, val loss: 0.0004896186292171478\n",
      "Training time: 5.714321851730347\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9640709161758423, val loss: 0.18443278968334198\n",
      "Epoch 10, train loss: 0.24825817346572876, val loss: 0.0031854952685534954\n",
      "Epoch 20, train loss: 0.037995778024196625, val loss: 0.34072789549827576\n",
      "Epoch 30, train loss: 0.017648449167609215, val loss: 0.18262727558612823\n",
      "Epoch 40, train loss: 0.02161482162773609, val loss: 0.09407136589288712\n",
      "Epoch 50, train loss: 0.012652507051825523, val loss: 0.1578478366136551\n",
      "Epoch 60, train loss: 0.01255100779235363, val loss: 0.14927630126476288\n",
      "Epoch 70, train loss: 0.011401222087442875, val loss: 0.11137977987527847\n",
      "Epoch 80, train loss: 0.010014205239713192, val loss: 0.11518383771181107\n",
      "Epoch 90, train loss: 0.008700262755155563, val loss: 0.09295791387557983\n",
      "Epoch 100, train loss: 0.0070428079925477505, val loss: 0.07071494311094284\n",
      "Epoch 110, train loss: 0.004766195081174374, val loss: 0.04137768968939781\n",
      "Epoch 120, train loss: 0.0017675162525847554, val loss: 0.007481472101062536\n",
      "Epoch 130, train loss: 0.00025910179829224944, val loss: 0.012391142547130585\n",
      "Epoch 140, train loss: 0.00019476242596283555, val loss: 0.0054510487243533134\n",
      "Epoch 150, train loss: 0.00021618910250253975, val loss: 0.0009207246475853026\n",
      "Epoch 160, train loss: 0.00014181251754052937, val loss: 0.003360916394740343\n",
      "Epoch 170, train loss: 0.00014875372289679945, val loss: 0.00473644258454442\n",
      "Epoch 180, train loss: 0.00013748968194704503, val loss: 0.002925545210018754\n",
      "Epoch 190, train loss: 0.00013593770563602448, val loss: 0.0030099512077867985\n",
      "Training time: 5.641065835952759\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6330026984214783, val loss: 0.09768548607826233\n",
      "Epoch 10, train loss: 0.5266061425209045, val loss: 0.06031937152147293\n",
      "Epoch 20, train loss: 0.4295912981033325, val loss: 0.0322083905339241\n",
      "Epoch 30, train loss: 0.34228116273880005, val loss: 0.013535606674849987\n",
      "Epoch 40, train loss: 0.26458558440208435, val loss: 0.004376870114356279\n",
      "Epoch 50, train loss: 0.19667686522006989, val loss: 0.004883999470621347\n",
      "Epoch 60, train loss: 0.13926731050014496, val loss: 0.015129479579627514\n",
      "Epoch 70, train loss: 0.09334476292133331, val loss: 0.03462911769747734\n",
      "Epoch 80, train loss: 0.05948589742183685, val loss: 0.061677612364292145\n",
      "Epoch 90, train loss: 0.03715910390019417, val loss: 0.09299901872873306\n",
      "Epoch 100, train loss: 0.024455910548567772, val loss: 0.1241571456193924\n",
      "Epoch 110, train loss: 0.01849246397614479, val loss: 0.15070612728595734\n",
      "Epoch 120, train loss: 0.016308479011058807, val loss: 0.16965068876743317\n",
      "Epoch 130, train loss: 0.015709102153778076, val loss: 0.1803620308637619\n",
      "Epoch 140, train loss: 0.015539506450295448, val loss: 0.18430136144161224\n",
      "Epoch 150, train loss: 0.015404734760522842, val loss: 0.1838768869638443\n",
      "Epoch 160, train loss: 0.01524603832513094, val loss: 0.18132421374320984\n",
      "Epoch 170, train loss: 0.0150790149345994, val loss: 0.17815300822257996\n",
      "Epoch 180, train loss: 0.014911743812263012, val loss: 0.1751183122396469\n",
      "Epoch 190, train loss: 0.014742384664714336, val loss: 0.17244040966033936\n",
      "Training time: 5.735074281692505\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.5555130839347839, val loss: 0.044061627238988876\n",
      "Epoch 10, train loss: 0.02866344340145588, val loss: 0.16424895823001862\n",
      "Epoch 20, train loss: 0.0183134526014328, val loss: 0.23319759964942932\n",
      "Epoch 30, train loss: 0.014622787944972515, val loss: 0.17872369289398193\n",
      "Epoch 40, train loss: 0.08126436173915863, val loss: 0.1686631143093109\n",
      "Epoch 50, train loss: 0.01715085096657276, val loss: 0.10833955556154251\n",
      "Epoch 60, train loss: 0.01554846204817295, val loss: 0.09057600051164627\n",
      "Epoch 70, train loss: 0.002761825453490019, val loss: 0.0012713464675471187\n",
      "Epoch 80, train loss: 0.00035823186044581234, val loss: 0.008360615000128746\n",
      "Epoch 90, train loss: 0.00013313003000803292, val loss: 0.0008542998693883419\n",
      "Training time: 4.113602638244629\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7164455652236938, val loss: 0.09673535078763962\n",
      "Epoch 10, train loss: 0.04090316221117973, val loss: 0.11848805099725723\n",
      "Epoch 20, train loss: 0.03797402232885361, val loss: 0.28487902879714966\n",
      "Epoch 30, train loss: 0.02612985111773014, val loss: 0.1183280199766159\n",
      "Epoch 40, train loss: 0.015993434935808182, val loss: 0.20328836143016815\n",
      "Epoch 50, train loss: 0.015924206003546715, val loss: 0.19106534123420715\n",
      "Epoch 60, train loss: 0.015359812416136265, val loss: 0.15619854629039764\n",
      "Epoch 70, train loss: 0.014315304346382618, val loss: 0.17239874601364136\n",
      "Epoch 80, train loss: 0.013416836969554424, val loss: 0.14920644462108612\n",
      "Epoch 90, train loss: 0.01239227969199419, val loss: 0.14138749241828918\n",
      "Training time: 4.303973436355591\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 1.1273359060287476, val loss: 0.3035868704319\n",
      "Epoch 10, train loss: 0.9453258514404297, val loss: 0.21749502420425415\n",
      "Epoch 20, train loss: 0.779314398765564, val loss: 0.1456705778837204\n",
      "Epoch 30, train loss: 0.6272532939910889, val loss: 0.08746013045310974\n",
      "Epoch 40, train loss: 0.48730897903442383, val loss: 0.04303733631968498\n",
      "Epoch 50, train loss: 0.3606569468975067, val loss: 0.014283854514360428\n",
      "Epoch 60, train loss: 0.25127407908439636, val loss: 0.0035796011798083782\n",
      "Epoch 70, train loss: 0.16344405710697174, val loss: 0.01185375452041626\n",
      "Epoch 80, train loss: 0.09929783642292023, val loss: 0.03706932067871094\n",
      "Epoch 90, train loss: 0.057582251727581024, val loss: 0.07390275597572327\n",
      "Training time: 4.377325057983398\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7402750253677368, val loss: 0.02933506853878498\n",
      "Epoch 10, train loss: 0.030138613656163216, val loss: 0.23690883815288544\n",
      "Epoch 20, train loss: 0.01979394070804119, val loss: 0.21134799718856812\n",
      "Epoch 30, train loss: 0.019272351637482643, val loss: 0.22338847815990448\n",
      "Epoch 40, train loss: 0.01652202568948269, val loss: 0.18663600087165833\n",
      "Epoch 50, train loss: 0.0009281451930291951, val loss: 0.006513854488730431\n",
      "Epoch 60, train loss: 0.01515921764075756, val loss: 0.19392427802085876\n",
      "Epoch 70, train loss: 0.012389231473207474, val loss: 0.07077878713607788\n",
      "Epoch 80, train loss: 0.005563714541494846, val loss: 0.032872479408979416\n",
      "Epoch 90, train loss: 0.000901461171451956, val loss: 0.0006347167072817683\n",
      "Epoch 100, train loss: 0.0011697545414790511, val loss: 0.0005088152829557657\n",
      "Epoch 110, train loss: 0.00041900252108462155, val loss: 0.0003906722704414278\n",
      "Epoch 120, train loss: 0.00013339782890398055, val loss: 0.00042420884710736573\n",
      "Epoch 130, train loss: 9.626573591958731e-05, val loss: 0.0009120406466536224\n",
      "Epoch 140, train loss: 8.654628618387505e-05, val loss: 0.000536030565854162\n",
      "Epoch 150, train loss: 6.451221270253882e-05, val loss: 0.00048018849338404834\n",
      "Epoch 160, train loss: 5.9926162066403776e-05, val loss: 0.0003460550797171891\n",
      "Epoch 170, train loss: 5.4602453019469976e-05, val loss: 0.00036828708834946156\n",
      "Epoch 180, train loss: 5.1444480050122365e-05, val loss: 0.00045383983524516225\n",
      "Epoch 190, train loss: 4.926651672576554e-05, val loss: 0.0005719203036278486\n",
      "Training time: 8.68381643295288\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6294667720794678, val loss: 0.03688010573387146\n",
      "Epoch 10, train loss: 0.04242538660764694, val loss: 0.4020557999610901\n",
      "Epoch 20, train loss: 0.017461169511079788, val loss: 0.19039466977119446\n",
      "Epoch 30, train loss: 0.021709013730287552, val loss: 0.14929823577404022\n",
      "Epoch 40, train loss: 0.01801811344921589, val loss: 0.2267366200685501\n",
      "Epoch 50, train loss: 0.01554794330149889, val loss: 0.1660202592611313\n",
      "Epoch 60, train loss: 0.014444096945226192, val loss: 0.1633133590221405\n",
      "Epoch 70, train loss: 0.012857135385274887, val loss: 0.1431436687707901\n",
      "Epoch 80, train loss: 0.010229150764644146, val loss: 0.10531428456306458\n",
      "Epoch 90, train loss: 0.005597616545855999, val loss: 0.03972047194838524\n",
      "Epoch 100, train loss: 0.00023841149231884629, val loss: 0.007103721611201763\n",
      "Epoch 110, train loss: 0.0003637946501839906, val loss: 0.00518031744286418\n",
      "Epoch 120, train loss: 0.0004318226419854909, val loss: 0.00036673113936558366\n",
      "Epoch 130, train loss: 0.00019686507584992796, val loss: 0.0034594230819493532\n",
      "Epoch 140, train loss: 0.0001906497636809945, val loss: 0.0025427164509892464\n",
      "Epoch 150, train loss: 0.00018087505304720253, val loss: 0.0010885238880291581\n",
      "Epoch 160, train loss: 0.00016241036064457148, val loss: 0.0018365359865128994\n",
      "Epoch 170, train loss: 0.00015307386638596654, val loss: 0.0013784116599708796\n",
      "Epoch 180, train loss: 0.00014530360931530595, val loss: 0.0010760865407064557\n",
      "Epoch 190, train loss: 0.00013744209718424827, val loss: 0.001083188340999186\n",
      "Training time: 8.433762311935425\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.21516934037208557, val loss: 0.004893044009804726\n",
      "Epoch 10, train loss: 0.15711350739002228, val loss: 0.015104290097951889\n",
      "Epoch 20, train loss: 0.10755202919244766, val loss: 0.0364934504032135\n",
      "Epoch 30, train loss: 0.06836183369159698, val loss: 0.06934140622615814\n",
      "Epoch 40, train loss: 0.04102841019630432, val loss: 0.11193288862705231\n",
      "Epoch 50, train loss: 0.02566247433423996, val loss: 0.1587950438261032\n",
      "Epoch 60, train loss: 0.01981745846569538, val loss: 0.20013919472694397\n",
      "Epoch 70, train loss: 0.018884506076574326, val loss: 0.2260478436946869\n",
      "Epoch 80, train loss: 0.01897697150707245, val loss: 0.23406347632408142\n",
      "Epoch 90, train loss: 0.018852941691875458, val loss: 0.23054221272468567\n",
      "Epoch 100, train loss: 0.018673615530133247, val loss: 0.22380021214485168\n",
      "Epoch 110, train loss: 0.01856314018368721, val loss: 0.2186616063117981\n",
      "Epoch 120, train loss: 0.018475666642189026, val loss: 0.2161545753479004\n",
      "Epoch 130, train loss: 0.018380971625447273, val loss: 0.21534936130046844\n",
      "Epoch 140, train loss: 0.01828097552061081, val loss: 0.21498416364192963\n",
      "Epoch 150, train loss: 0.018177922815084457, val loss: 0.2142961025238037\n",
      "Epoch 160, train loss: 0.018070509657263756, val loss: 0.21313677728176117\n",
      "Epoch 170, train loss: 0.01795797236263752, val loss: 0.21169227361679077\n",
      "Epoch 180, train loss: 0.017840007320046425, val loss: 0.21016894280910492\n",
      "Epoch 190, train loss: 0.017716113477945328, val loss: 0.2086489051580429\n",
      "Training time: 8.372608423233032\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.9159342050552368, val loss: 0.048031944781541824\n",
      "Epoch 10, train loss: 0.03726864233613014, val loss: 0.2027636468410492\n",
      "Epoch 20, train loss: 0.1524541825056076, val loss: 0.21419654786586761\n",
      "Epoch 30, train loss: 0.028601795434951782, val loss: 0.2670878767967224\n",
      "Epoch 40, train loss: 0.09036833792924881, val loss: 0.39767709374427795\n",
      "Epoch 50, train loss: 0.35850653052330017, val loss: 0.03266608715057373\n",
      "Epoch 60, train loss: 0.04140155762434006, val loss: 0.18735529482364655\n",
      "Epoch 70, train loss: 0.019746094942092896, val loss: 0.24877621233463287\n",
      "Epoch 80, train loss: 0.020090505480766296, val loss: 0.22919049859046936\n",
      "Epoch 90, train loss: 0.01972806081175804, val loss: 0.21982474625110626\n",
      "Training time: 5.232081174850464\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.4018529951572418, val loss: 0.005057852249592543\n",
      "Epoch 10, train loss: 0.0325327105820179, val loss: 0.25586992502212524\n",
      "Epoch 20, train loss: 0.02341070957481861, val loss: 0.14798754453659058\n",
      "Epoch 30, train loss: 0.018125951290130615, val loss: 0.21020686626434326\n",
      "Epoch 40, train loss: 0.015399129129946232, val loss: 0.14512215554714203\n",
      "Epoch 50, train loss: 0.01313115656375885, val loss: 0.15922768414020538\n",
      "Epoch 60, train loss: 0.010842973366379738, val loss: 0.12122151255607605\n",
      "Epoch 70, train loss: 0.00764522235840559, val loss: 0.07854665815830231\n",
      "Epoch 80, train loss: 0.0010780802695080638, val loss: 0.001620155293494463\n",
      "Epoch 90, train loss: 9.70107939792797e-05, val loss: 0.0008455657516606152\n",
      "Training time: 5.264743328094482\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5207815766334534, val loss: 0.05900627002120018\n",
      "Epoch 10, train loss: 0.3501555323600769, val loss: 0.01738397777080536\n",
      "Epoch 20, train loss: 0.1956937611103058, val loss: 0.0032652236986905336\n",
      "Epoch 30, train loss: 0.07406570762395859, val loss: 0.03496360406279564\n",
      "Epoch 40, train loss: 0.01622328720986843, val loss: 0.12479910254478455\n",
      "Epoch 50, train loss: 0.016934961080551147, val loss: 0.201466366648674\n",
      "Epoch 60, train loss: 0.01534163299947977, val loss: 0.1814112812280655\n",
      "Epoch 70, train loss: 0.012642769142985344, val loss: 0.1428196132183075\n",
      "Epoch 80, train loss: 0.012805167585611343, val loss: 0.13147728145122528\n",
      "Epoch 90, train loss: 0.012333018705248833, val loss: 0.1372092366218567\n",
      "Training time: 5.292448043823242\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5438255071640015, val loss: 0.28743645548820496\n",
      "Epoch 10, train loss: 0.0388997383415699, val loss: 0.2603319585323334\n",
      "Epoch 20, train loss: 0.06417757272720337, val loss: 0.16818538308143616\n",
      "Epoch 30, train loss: 0.06004543602466583, val loss: 0.389983594417572\n",
      "Epoch 40, train loss: 0.027431588619947433, val loss: 0.35606253147125244\n",
      "Epoch 50, train loss: 0.01964159496128559, val loss: 0.2704343795776367\n",
      "Epoch 60, train loss: 0.02017081156373024, val loss: 0.23476135730743408\n",
      "Epoch 70, train loss: 0.020035335794091225, val loss: 0.22731401026248932\n",
      "Epoch 80, train loss: 0.019793275743722916, val loss: 0.22875195741653442\n",
      "Epoch 90, train loss: 0.019673990085721016, val loss: 0.23164880275726318\n",
      "Epoch 100, train loss: 0.019634492695331573, val loss: 0.23374509811401367\n",
      "Epoch 110, train loss: 0.019627774134278297, val loss: 0.23439714312553406\n",
      "Epoch 120, train loss: 0.019625341519713402, val loss: 0.23367451131343842\n",
      "Epoch 130, train loss: 0.019617600366473198, val loss: 0.23228400945663452\n",
      "Epoch 140, train loss: 0.019607923924922943, val loss: 0.23122496902942657\n",
      "Epoch 150, train loss: 0.019596939906477928, val loss: 0.2310139238834381\n",
      "Epoch 160, train loss: 0.01958138309419155, val loss: 0.23123791813850403\n",
      "Epoch 170, train loss: 0.019559573382139206, val loss: 0.23115234076976776\n",
      "Epoch 180, train loss: 0.01952977292239666, val loss: 0.23065285384655\n",
      "Epoch 190, train loss: 0.019493261352181435, val loss: 0.23016253113746643\n",
      "Training time: 10.660998344421387\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8803840279579163, val loss: 0.1140761598944664\n",
      "Epoch 10, train loss: 0.03639120236039162, val loss: 0.38130292296409607\n",
      "Epoch 20, train loss: 0.02091825194656849, val loss: 0.10204329341650009\n",
      "Epoch 30, train loss: 0.01438360009342432, val loss: 0.175018772482872\n",
      "Epoch 40, train loss: 0.014456638135015965, val loss: 0.17173507809638977\n",
      "Epoch 50, train loss: 0.013414786197245121, val loss: 0.13155272603034973\n",
      "Epoch 60, train loss: 0.011638985015451908, val loss: 0.14108501374721527\n",
      "Epoch 70, train loss: 0.009584936313331127, val loss: 0.10084530711174011\n",
      "Epoch 80, train loss: 0.006552509032189846, val loss: 0.06905678659677505\n",
      "Epoch 90, train loss: 0.0011787042021751404, val loss: 0.003956677857786417\n",
      "Epoch 100, train loss: 0.00024709460558369756, val loss: 0.0013270805357024074\n",
      "Epoch 110, train loss: 0.0003766991721931845, val loss: 0.001237384625710547\n",
      "Epoch 120, train loss: 0.00017921965627465397, val loss: 0.0032746486831456423\n",
      "Epoch 130, train loss: 9.964101627701893e-05, val loss: 0.0003047322388738394\n",
      "Epoch 140, train loss: 7.523984822910279e-05, val loss: 0.0011184441391378641\n",
      "Epoch 150, train loss: 7.278568227775395e-05, val loss: 0.0006137204472906888\n",
      "Epoch 160, train loss: 7.22670738468878e-05, val loss: 0.0006978556048125029\n",
      "Epoch 170, train loss: 7.178313535405323e-05, val loss: 0.00072357157478109\n",
      "Epoch 180, train loss: 7.119865040294826e-05, val loss: 0.0006539640598930418\n",
      "Epoch 190, train loss: 7.06268911017105e-05, val loss: 0.0006957651930861175\n",
      "Training time: 10.554091930389404\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.5069373250007629, val loss: 0.04795452207326889\n",
      "Epoch 10, train loss: 0.3334253430366516, val loss: 0.011384322308003902\n",
      "Epoch 20, train loss: 0.18916365504264832, val loss: 0.004972946364432573\n",
      "Epoch 30, train loss: 0.08042673766613007, val loss: 0.038064923137426376\n",
      "Epoch 40, train loss: 0.02302739769220352, val loss: 0.11417459696531296\n",
      "Epoch 50, train loss: 0.014916272833943367, val loss: 0.1924988031387329\n",
      "Epoch 60, train loss: 0.01698385924100876, val loss: 0.20561380684375763\n",
      "Epoch 70, train loss: 0.014377836138010025, val loss: 0.17585699260234833\n",
      "Epoch 80, train loss: 0.013864311389625072, val loss: 0.15465112030506134\n",
      "Epoch 90, train loss: 0.0137861929833889, val loss: 0.15137626230716705\n",
      "Epoch 100, train loss: 0.013487289659678936, val loss: 0.15568922460079193\n",
      "Epoch 110, train loss: 0.01330523006618023, val loss: 0.15727835893630981\n",
      "Epoch 120, train loss: 0.013110260479152203, val loss: 0.1541156768798828\n",
      "Epoch 130, train loss: 0.012901900336146355, val loss: 0.14987173676490784\n",
      "Epoch 140, train loss: 0.012687302194535732, val loss: 0.14690639078617096\n",
      "Epoch 150, train loss: 0.012458170764148235, val loss: 0.14461275935173035\n",
      "Epoch 160, train loss: 0.012215816415846348, val loss: 0.14183317124843597\n",
      "Epoch 170, train loss: 0.011957916431128979, val loss: 0.13846346735954285\n",
      "Epoch 180, train loss: 0.011683094315230846, val loss: 0.13492131233215332\n",
      "Epoch 190, train loss: 0.011389318853616714, val loss: 0.13129031658172607\n",
      "Training time: 10.459739208221436\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.8055545687675476, val loss: 0.5218326449394226\n",
      "Epoch 10, train loss: 0.03810305520892143, val loss: 0.28748998045921326\n",
      "Epoch 20, train loss: 0.0652463510632515, val loss: 0.3128083050251007\n",
      "Epoch 30, train loss: 0.042423855513334274, val loss: 0.4333932101726532\n",
      "Epoch 40, train loss: 0.021530186757445335, val loss: 0.24537818133831024\n",
      "Epoch 50, train loss: 0.02441602759063244, val loss: 0.18462975323200226\n",
      "Epoch 60, train loss: 0.020637033507227898, val loss: 0.19631627202033997\n",
      "Epoch 70, train loss: 0.019696561619639397, val loss: 0.2160886526107788\n",
      "Epoch 80, train loss: 0.01967298798263073, val loss: 0.22672440111637115\n",
      "Epoch 90, train loss: 0.0196770578622818, val loss: 0.2304052859544754\n",
      "Training time: 7.931915521621704\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7857223749160767, val loss: 0.07284749299287796\n",
      "Epoch 10, train loss: 0.040903810411691666, val loss: 0.2347533106803894\n",
      "Epoch 20, train loss: 0.017795875668525696, val loss: 0.1763874590396881\n",
      "Epoch 30, train loss: 0.014746041968464851, val loss: 0.1523640751838684\n",
      "Epoch 40, train loss: 0.013865710236132145, val loss: 0.17508329451084137\n",
      "Epoch 50, train loss: 0.012570786289870739, val loss: 0.11966119706630707\n",
      "Epoch 60, train loss: 0.009702921845018864, val loss: 0.10380930453538895\n",
      "Epoch 70, train loss: 0.002164779929444194, val loss: 0.0023160558193922043\n",
      "Epoch 80, train loss: 0.0002591801749076694, val loss: 0.0010944692185148597\n",
      "Epoch 90, train loss: 0.0003952984407078475, val loss: 0.003179742954671383\n",
      "Training time: 7.8347344398498535\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.7948850989341736, val loss: 0.1517164409160614\n",
      "Epoch 10, train loss: 0.5913881063461304, val loss: 0.07773450762033463\n",
      "Epoch 20, train loss: 0.40755125880241394, val loss: 0.026235971599817276\n",
      "Epoch 30, train loss: 0.23472176492214203, val loss: 0.0032490058802068233\n",
      "Epoch 40, train loss: 0.09043241292238235, val loss: 0.03360232710838318\n",
      "Epoch 50, train loss: 0.019765516743063927, val loss: 0.13614875078201294\n",
      "Epoch 60, train loss: 0.01861610636115074, val loss: 0.22939103841781616\n",
      "Epoch 70, train loss: 0.018412120640277863, val loss: 0.21849866211414337\n",
      "Epoch 80, train loss: 0.014943174086511135, val loss: 0.17533284425735474\n",
      "Epoch 90, train loss: 0.01506027765572071, val loss: 0.157730832695961\n",
      "Training time: 7.760408401489258\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5456636548042297, val loss: 0.6916621327400208\n",
      "Epoch 10, train loss: 1.3738149404525757, val loss: 1.3087698221206665\n",
      "Epoch 20, train loss: 0.014754453673958778, val loss: 0.0931975245475769\n",
      "Epoch 30, train loss: 0.041891906410455704, val loss: 0.2983842194080353\n",
      "Epoch 40, train loss: 0.029709335416555405, val loss: 0.35301631689071655\n",
      "Epoch 50, train loss: 0.020091885700821877, val loss: 0.24530664086341858\n",
      "Epoch 60, train loss: 0.021456239745020866, val loss: 0.201605886220932\n",
      "Epoch 70, train loss: 0.019892308861017227, val loss: 0.21054263412952423\n",
      "Epoch 80, train loss: 0.0196231622248888, val loss: 0.22617852687835693\n",
      "Epoch 90, train loss: 0.01965252123773098, val loss: 0.23350556194782257\n",
      "Epoch 100, train loss: 0.019628221169114113, val loss: 0.23480427265167236\n",
      "Epoch 110, train loss: 0.01959945075213909, val loss: 0.2340390682220459\n",
      "Epoch 120, train loss: 0.019577322527766228, val loss: 0.23300082981586456\n",
      "Epoch 130, train loss: 0.019555263221263885, val loss: 0.23211383819580078\n",
      "Epoch 140, train loss: 0.01952667534351349, val loss: 0.23133514821529388\n",
      "Epoch 150, train loss: 0.01948251761496067, val loss: 0.23048487305641174\n",
      "Epoch 160, train loss: 0.0194007009267807, val loss: 0.22918565571308136\n",
      "Epoch 170, train loss: 0.019201161339879036, val loss: 0.22604569792747498\n",
      "Epoch 180, train loss: 0.01834847219288349, val loss: 0.20899005234241486\n",
      "Epoch 190, train loss: 0.010527229867875576, val loss: 0.09584519267082214\n",
      "Training time: 15.554666757583618\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5402055382728577, val loss: 0.010762433521449566\n",
      "Epoch 10, train loss: 0.023788725957274437, val loss: 0.20695245265960693\n",
      "Epoch 20, train loss: 0.01960226148366928, val loss: 0.26987236738204956\n",
      "Epoch 30, train loss: 0.018761994317173958, val loss: 0.1537560522556305\n",
      "Epoch 40, train loss: 0.0160499457269907, val loss: 0.19732160866260529\n",
      "Epoch 50, train loss: 0.012040218338370323, val loss: 0.13133732974529266\n",
      "Epoch 60, train loss: 0.004112502094358206, val loss: 0.022992052137851715\n",
      "Epoch 70, train loss: 0.00016236657393164933, val loss: 0.0008548845653422177\n",
      "Epoch 80, train loss: 0.000413956877309829, val loss: 0.00047464531962759793\n",
      "Epoch 90, train loss: 0.00013947038678452373, val loss: 0.0011846747947856784\n",
      "Epoch 100, train loss: 0.00012142544437665492, val loss: 0.0005120087298564613\n",
      "Epoch 110, train loss: 9.75162984104827e-05, val loss: 0.001038556220009923\n",
      "Epoch 120, train loss: 9.125697397394106e-05, val loss: 0.000840340624563396\n",
      "Epoch 130, train loss: 8.751393761485815e-05, val loss: 0.0008773669833317399\n",
      "Epoch 140, train loss: 8.620566950412467e-05, val loss: 0.0008956114179454744\n",
      "Epoch 150, train loss: 8.5159030277282e-05, val loss: 0.0007769651710987091\n",
      "Epoch 160, train loss: 8.411608723690733e-05, val loss: 0.000854365702252835\n",
      "Epoch 170, train loss: 8.308719407068565e-05, val loss: 0.0007502572261728346\n",
      "Epoch 180, train loss: 8.207694918382913e-05, val loss: 0.0007777215796522796\n",
      "Epoch 190, train loss: 8.109727059490979e-05, val loss: 0.0007299678400158882\n",
      "Training time: 15.338462114334106\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.41575151681900024, val loss: 0.018186122179031372\n",
      "Epoch 10, train loss: 0.2320873886346817, val loss: 0.006252750288695097\n",
      "Epoch 20, train loss: 0.0939309224486351, val loss: 0.05966237932443619\n",
      "Epoch 30, train loss: 0.024507245048880577, val loss: 0.1925068497657776\n",
      "Epoch 40, train loss: 0.02416018582880497, val loss: 0.30500200390815735\n",
      "Epoch 50, train loss: 0.022101785987615585, val loss: 0.2742663323879242\n",
      "Epoch 60, train loss: 0.019307652488350868, val loss: 0.21919052302837372\n",
      "Epoch 70, train loss: 0.01945118047297001, val loss: 0.20709200203418732\n",
      "Epoch 80, train loss: 0.018807174637913704, val loss: 0.21890299022197723\n",
      "Epoch 90, train loss: 0.018549766391515732, val loss: 0.224331796169281\n",
      "Epoch 100, train loss: 0.018228672444820404, val loss: 0.21705466508865356\n",
      "Epoch 110, train loss: 0.017910776659846306, val loss: 0.20951057970523834\n",
      "Epoch 120, train loss: 0.017569301649928093, val loss: 0.2064397633075714\n",
      "Epoch 130, train loss: 0.01719886064529419, val loss: 0.2033708542585373\n",
      "Epoch 140, train loss: 0.016792042180895805, val loss: 0.19796088337898254\n",
      "Epoch 150, train loss: 0.016340380534529686, val loss: 0.19190722703933716\n",
      "Epoch 160, train loss: 0.015832237899303436, val loss: 0.18580955266952515\n",
      "Epoch 170, train loss: 0.01525224931538105, val loss: 0.17860528826713562\n",
      "Epoch 180, train loss: 0.014578609727323055, val loss: 0.16991034150123596\n",
      "Epoch 190, train loss: 0.013780188746750355, val loss: 0.15966032445430756\n",
      "Training time: 15.629242181777954\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7535833716392517, val loss: 2.0063436031341553\n",
      "Epoch 10, train loss: 1.3804651498794556, val loss: 0.07102220505475998\n",
      "Epoch 20, train loss: 0.05724223330616951, val loss: 0.19665397703647614\n",
      "Epoch 30, train loss: 0.0283708106726408, val loss: 0.18673647940158844\n",
      "Epoch 40, train loss: 0.017374059185385704, val loss: 0.2577192485332489\n",
      "Epoch 50, train loss: 0.019268536940217018, val loss: 0.24957355856895447\n",
      "Epoch 60, train loss: 0.01544337160885334, val loss: 0.16463936865329742\n",
      "Epoch 70, train loss: 0.009555208496749401, val loss: 0.12559348344802856\n",
      "Epoch 80, train loss: 0.024518560618162155, val loss: 0.22252823412418365\n",
      "Epoch 90, train loss: 0.02806777134537697, val loss: 0.28326302766799927\n",
      "Training time: 12.177926540374756\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5338287353515625, val loss: 0.007898916490375996\n",
      "Epoch 10, train loss: 0.04190853610634804, val loss: 0.06771086156368256\n",
      "Epoch 20, train loss: 0.020614130422472954, val loss: 0.19677451252937317\n",
      "Epoch 30, train loss: 0.011589521542191505, val loss: 0.13624969124794006\n",
      "Epoch 40, train loss: 0.009283916093409061, val loss: 0.07680074125528336\n",
      "Epoch 50, train loss: 0.0028501988854259253, val loss: 0.014531932771205902\n",
      "Epoch 60, train loss: 0.0002744215598795563, val loss: 0.001957567408680916\n",
      "Epoch 70, train loss: 0.0003972455160692334, val loss: 0.0026627734769135714\n",
      "Epoch 80, train loss: 0.00020647671772167087, val loss: 0.002560189925134182\n",
      "Epoch 90, train loss: 9.568636596668512e-05, val loss: 0.00030757937929593027\n",
      "Training time: 12.128732442855835\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.702435314655304, val loss: 0.11616605520248413\n",
      "Epoch 10, train loss: 0.38057640194892883, val loss: 0.020413214340806007\n",
      "Epoch 20, train loss: 0.12780974805355072, val loss: 0.016830848529934883\n",
      "Epoch 30, train loss: 0.01355824526399374, val loss: 0.18948806822299957\n",
      "Epoch 40, train loss: 0.027489978820085526, val loss: 0.2430374026298523\n",
      "Epoch 50, train loss: 0.013339783065021038, val loss: 0.13491620123386383\n",
      "Epoch 60, train loss: 0.014614105224609375, val loss: 0.12354589253664017\n",
      "Epoch 70, train loss: 0.012459254823625088, val loss: 0.15195611119270325\n",
      "Epoch 80, train loss: 0.012319697998464108, val loss: 0.15089592337608337\n",
      "Epoch 90, train loss: 0.011753363534808159, val loss: 0.13340537250041962\n",
      "Training time: 12.161688566207886\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7258384227752686, val loss: 0.8142940998077393\n",
      "Epoch 10, train loss: 0.09275086224079132, val loss: 1.3059366941452026\n",
      "Epoch 20, train loss: 3.4806807041168213, val loss: 0.03978898748755455\n",
      "Epoch 30, train loss: 0.367372989654541, val loss: 0.007306199055165052\n",
      "Epoch 40, train loss: 0.0670974999666214, val loss: 0.13281892240047455\n",
      "Epoch 50, train loss: 0.021485360339283943, val loss: 0.2555239200592041\n",
      "Epoch 60, train loss: 0.01951427571475506, val loss: 0.25032755732536316\n",
      "Epoch 70, train loss: 0.019618410617113113, val loss: 0.21386975049972534\n",
      "Epoch 80, train loss: 0.016366392374038696, val loss: 0.20920529961585999\n",
      "Epoch 90, train loss: 0.031170466914772987, val loss: 4.692267417907715\n",
      "Epoch 100, train loss: 0.040551163256168365, val loss: 0.024433795362710953\n",
      "Epoch 110, train loss: 0.1254635602235794, val loss: 0.4204637110233307\n",
      "Epoch 120, train loss: 0.0325496569275856, val loss: 0.3749961853027344\n",
      "Epoch 130, train loss: 0.022312430664896965, val loss: 0.21700403094291687\n",
      "Epoch 140, train loss: 0.02111615613102913, val loss: 0.1905829757452011\n",
      "Epoch 150, train loss: 0.019935261458158493, val loss: 0.23535987734794617\n",
      "Epoch 160, train loss: 0.01990731619298458, val loss: 0.24824205040931702\n",
      "Epoch 170, train loss: 0.01966269128024578, val loss: 0.23499123752117157\n",
      "Epoch 180, train loss: 0.01969616860151291, val loss: 0.2276749461889267\n",
      "Epoch 190, train loss: 0.019662102684378624, val loss: 0.2292545586824417\n",
      "Training time: 24.314498901367188\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5244651436805725, val loss: 0.004347341600805521\n",
      "Epoch 10, train loss: 0.04264486953616142, val loss: 0.07318990677595139\n",
      "Epoch 20, train loss: 0.019613469019532204, val loss: 0.1932171732187271\n",
      "Epoch 30, train loss: 0.012919851578772068, val loss: 0.1381329745054245\n",
      "Epoch 40, train loss: 0.010591120459139347, val loss: 0.10899695754051208\n",
      "Epoch 50, train loss: 0.00827042292803526, val loss: 0.09532346576452255\n",
      "Epoch 60, train loss: 0.0021954108960926533, val loss: 0.01076579187065363\n",
      "Epoch 70, train loss: 0.00019907881505787373, val loss: 0.0017784556839615107\n",
      "Epoch 80, train loss: 0.00021402891434263438, val loss: 0.00027126376517117023\n",
      "Epoch 90, train loss: 6.538206798722968e-05, val loss: 0.0004637185193132609\n",
      "Epoch 100, train loss: 7.072638254612684e-05, val loss: 0.0002746571262832731\n",
      "Epoch 110, train loss: 5.356408655643463e-05, val loss: 0.0006206760299392045\n",
      "Epoch 120, train loss: 5.125188181409612e-05, val loss: 0.000372776499716565\n",
      "Epoch 130, train loss: 4.8880308895604685e-05, val loss: 0.00048308965051546693\n",
      "Epoch 140, train loss: 4.799990711035207e-05, val loss: 0.0004340634332038462\n",
      "Epoch 150, train loss: 4.763427932630293e-05, val loss: 0.0004399484023451805\n",
      "Epoch 160, train loss: 4.741827433463186e-05, val loss: 0.00044578342931345105\n",
      "Epoch 170, train loss: 4.7242705477401614e-05, val loss: 0.0004284839960746467\n",
      "Epoch 180, train loss: 4.708297274191864e-05, val loss: 0.0004406148800626397\n",
      "Epoch 190, train loss: 4.691598951467313e-05, val loss: 0.00042436213698238134\n",
      "Training time: 24.290301084518433\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6493727564811707, val loss: 0.0923510417342186\n",
      "Epoch 10, train loss: 0.36068204045295715, val loss: 0.014485021121799946\n",
      "Epoch 20, train loss: 0.13302569091320038, val loss: 0.01896199956536293\n",
      "Epoch 30, train loss: 0.01483075600117445, val loss: 0.19314053654670715\n",
      "Epoch 40, train loss: 0.02795301005244255, val loss: 0.2576717436313629\n",
      "Epoch 50, train loss: 0.01470058225095272, val loss: 0.14497724175453186\n",
      "Epoch 60, train loss: 0.01549019105732441, val loss: 0.136942058801651\n",
      "Epoch 70, train loss: 0.013446350581943989, val loss: 0.16582436859607697\n",
      "Epoch 80, train loss: 0.013097519055008888, val loss: 0.1597646027803421\n",
      "Epoch 90, train loss: 0.012470749206840992, val loss: 0.14114181697368622\n",
      "Epoch 100, train loss: 0.01190204732120037, val loss: 0.13852447271347046\n",
      "Epoch 110, train loss: 0.011330329813063145, val loss: 0.1344192624092102\n",
      "Epoch 120, train loss: 0.010693512856960297, val loss: 0.12396594136953354\n",
      "Epoch 130, train loss: 0.010005716234445572, val loss: 0.11607252806425095\n",
      "Epoch 140, train loss: 0.009243961423635483, val loss: 0.107100710272789\n",
      "Epoch 150, train loss: 0.008389326743781567, val loss: 0.09590286016464233\n",
      "Epoch 160, train loss: 0.007419122848659754, val loss: 0.08409111946821213\n",
      "Epoch 170, train loss: 0.006304563954472542, val loss: 0.0699460580945015\n",
      "Epoch 180, train loss: 0.005014069378376007, val loss: 0.053856756538152695\n",
      "Epoch 190, train loss: 0.00353053561411798, val loss: 0.035427749156951904\n",
      "Training time: 24.36903429031372\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7253203988075256, val loss: 1.9425621032714844\n",
      "Epoch 10, train loss: 1.0391604900360107, val loss: 0.1636992245912552\n",
      "Epoch 20, train loss: 0.37818101048469543, val loss: 0.9860643744468689\n",
      "Epoch 30, train loss: 0.0683051347732544, val loss: 0.15741904079914093\n",
      "Epoch 40, train loss: 0.026284070685505867, val loss: 0.11105893552303314\n",
      "Epoch 50, train loss: 0.03685614466667175, val loss: 0.3465454578399658\n",
      "Epoch 60, train loss: 0.019659873098134995, val loss: 0.2663887143135071\n",
      "Epoch 70, train loss: 0.02192574366927147, val loss: 0.19790396094322205\n",
      "Epoch 80, train loss: 0.01979806460440159, val loss: 0.21165187656879425\n",
      "Epoch 90, train loss: 0.019778285175561905, val loss: 0.2345677614212036\n",
      "Training time: 18.883927822113037\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.49607545137405396, val loss: 0.02066102810204029\n",
      "Epoch 10, train loss: 0.016744501888751984, val loss: 0.279487282037735\n",
      "Epoch 20, train loss: 0.01880989782512188, val loss: 0.144058957695961\n",
      "Epoch 30, train loss: 0.013413690961897373, val loss: 0.10340878367424011\n",
      "Epoch 40, train loss: 0.006195375230163336, val loss: 0.04033920168876648\n",
      "Epoch 50, train loss: 0.0008378412458114326, val loss: 0.00537979556247592\n",
      "Epoch 60, train loss: 0.0007423209608532488, val loss: 0.001662489608861506\n",
      "Epoch 70, train loss: 0.00029878303757868707, val loss: 0.0027461384888738394\n",
      "Epoch 80, train loss: 0.00015492054808419198, val loss: 0.0003414455568417907\n",
      "Epoch 90, train loss: 9.657170448917896e-05, val loss: 0.0014747604727745056\n",
      "Training time: 18.804622173309326\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.7622705698013306, val loss: 0.13141882419586182\n",
      "Epoch 10, train loss: 0.3271051049232483, val loss: 0.007999264635145664\n",
      "Epoch 20, train loss: 0.02740955725312233, val loss: 0.13489823043346405\n",
      "Epoch 30, train loss: 0.04524236172437668, val loss: 0.3209230601787567\n",
      "Epoch 40, train loss: 0.016098259016871452, val loss: 0.13588346540927887\n",
      "Epoch 50, train loss: 0.017440304160118103, val loss: 0.13435661792755127\n",
      "Epoch 60, train loss: 0.014565757475793362, val loss: 0.18447712063789368\n",
      "Epoch 70, train loss: 0.013537061400711536, val loss: 0.16001571714878082\n",
      "Epoch 80, train loss: 0.013082027435302734, val loss: 0.14095036685466766\n",
      "Epoch 90, train loss: 0.012287987396121025, val loss: 0.1453760713338852\n",
      "Training time: 18.702951431274414\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.4802165627479553, val loss: 1.67811119556427\n",
      "Epoch 10, train loss: 0.11776242405176163, val loss: 0.24587731063365936\n",
      "Epoch 20, train loss: 0.029593098908662796, val loss: 0.17285218834877014\n",
      "Epoch 30, train loss: 0.019861172884702682, val loss: 0.15764117240905762\n",
      "Epoch 40, train loss: 0.02219817228615284, val loss: 0.15490545332431793\n",
      "Epoch 50, train loss: 0.023260436952114105, val loss: 0.19245634973049164\n",
      "Epoch 60, train loss: 0.019748535007238388, val loss: 0.24893638491630554\n",
      "Epoch 70, train loss: 0.020133284851908684, val loss: 0.2480606883764267\n",
      "Epoch 80, train loss: 0.019674383103847504, val loss: 0.22140993177890778\n",
      "Epoch 90, train loss: 0.01967836730182171, val loss: 0.23437222838401794\n",
      "Epoch 100, train loss: 0.019679240882396698, val loss: 0.233242005109787\n",
      "Epoch 110, train loss: 0.019668390974402428, val loss: 0.23080186545848846\n",
      "Epoch 120, train loss: 0.019663190469145775, val loss: 0.23308508098125458\n",
      "Epoch 130, train loss: 0.019661279395222664, val loss: 0.2317495197057724\n",
      "Epoch 140, train loss: 0.01966053992509842, val loss: 0.2322283685207367\n",
      "Epoch 150, train loss: 0.019660282880067825, val loss: 0.23232370615005493\n",
      "Epoch 160, train loss: 0.01966024562716484, val loss: 0.23198334872722626\n",
      "Epoch 170, train loss: 0.019660253077745438, val loss: 0.23226766288280487\n",
      "Epoch 180, train loss: 0.01966024748980999, val loss: 0.23218657076358795\n",
      "Epoch 190, train loss: 0.019660238176584244, val loss: 0.23212949931621552\n",
      "Training time: 37.68603038787842\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6766160130500793, val loss: 0.0038789166137576103\n",
      "Epoch 10, train loss: 0.03310622647404671, val loss: 0.15573133528232574\n",
      "Epoch 20, train loss: 0.024886198341846466, val loss: 0.10235609114170074\n",
      "Epoch 30, train loss: 0.01534298062324524, val loss: 0.16547095775604248\n",
      "Epoch 40, train loss: 0.010450239293277264, val loss: 0.12914228439331055\n",
      "Epoch 50, train loss: 0.0019912729039788246, val loss: 0.0008350562420673668\n",
      "Epoch 60, train loss: 0.00105150300078094, val loss: 0.00901359599083662\n",
      "Epoch 70, train loss: 0.0004621480475179851, val loss: 0.011674344539642334\n",
      "Epoch 80, train loss: 0.00022668991005048156, val loss: 0.0003552075068000704\n",
      "Epoch 90, train loss: 0.00016185764980036765, val loss: 0.0034182409290224314\n",
      "Epoch 100, train loss: 0.0001019112896756269, val loss: 0.0010686872992664576\n",
      "Epoch 110, train loss: 8.73855606187135e-05, val loss: 0.0015108281513676047\n",
      "Epoch 120, train loss: 8.580624125897884e-05, val loss: 0.0017610617214813828\n",
      "Epoch 130, train loss: 8.502860873704776e-05, val loss: 0.001313652261160314\n",
      "Epoch 140, train loss: 8.368760609300807e-05, val loss: 0.0015864032320678234\n",
      "Epoch 150, train loss: 8.242368494393304e-05, val loss: 0.0014395659090951085\n",
      "Epoch 160, train loss: 8.134416566463187e-05, val loss: 0.0013912046561017632\n",
      "Epoch 170, train loss: 8.030639583012089e-05, val loss: 0.001413036254234612\n",
      "Epoch 180, train loss: 7.929722050903365e-05, val loss: 0.001347597106359899\n",
      "Epoch 190, train loss: 7.831212133169174e-05, val loss: 0.0013123383978381753\n",
      "Training time: 37.57550668716431\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6810935735702515, val loss: 0.09656918048858643\n",
      "Epoch 10, train loss: 0.25206348299980164, val loss: 0.003656986402347684\n",
      "Epoch 20, train loss: 0.016262533143162727, val loss: 0.22613033652305603\n",
      "Epoch 30, train loss: 0.03550518676638603, val loss: 0.2972692549228668\n",
      "Epoch 40, train loss: 0.018773071467876434, val loss: 0.13873684406280518\n",
      "Epoch 50, train loss: 0.01712684892117977, val loss: 0.15766069293022156\n",
      "Epoch 60, train loss: 0.015924695879220963, val loss: 0.2016235589981079\n",
      "Epoch 70, train loss: 0.014706083573400974, val loss: 0.17147384583950043\n",
      "Epoch 80, train loss: 0.014364264905452728, val loss: 0.15930800139904022\n",
      "Epoch 90, train loss: 0.013742774724960327, val loss: 0.16468535363674164\n",
      "Epoch 100, train loss: 0.013130313716828823, val loss: 0.1524757593870163\n",
      "Epoch 110, train loss: 0.012472894042730331, val loss: 0.14361770451068878\n",
      "Epoch 120, train loss: 0.011707122437655926, val loss: 0.13594317436218262\n",
      "Epoch 130, train loss: 0.01080240961164236, val loss: 0.12290424108505249\n",
      "Epoch 140, train loss: 0.009704296477138996, val loss: 0.10961715131998062\n",
      "Epoch 150, train loss: 0.008324149064719677, val loss: 0.09128239750862122\n",
      "Epoch 160, train loss: 0.006525959353893995, val loss: 0.06812647730112076\n",
      "Epoch 170, train loss: 0.004144469741731882, val loss: 0.03757913038134575\n",
      "Epoch 180, train loss: 0.0013429760001599789, val loss: 0.006262458395212889\n",
      "Epoch 190, train loss: 0.00015939016884658486, val loss: 0.006875036284327507\n",
      "Training time: 37.469539403915405\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.4562826454639435, val loss: 0.0044403076171875\n",
      "Epoch 10, train loss: 0.026466067880392075, val loss: 0.139542818069458\n",
      "Epoch 20, train loss: 0.01616854779422283, val loss: 0.15264743566513062\n",
      "Epoch 30, train loss: 0.016125990077853203, val loss: 0.2087889015674591\n",
      "Epoch 40, train loss: 0.015270388685166836, val loss: 0.14549745619297028\n",
      "Epoch 50, train loss: 0.012072304263710976, val loss: 0.1353856325149536\n",
      "Epoch 60, train loss: 0.00869021750986576, val loss: 0.011589854024350643\n",
      "Epoch 70, train loss: 0.0032939801458269358, val loss: 0.05492852255702019\n",
      "Epoch 80, train loss: 0.0013441708870232105, val loss: 0.008644625544548035\n",
      "Epoch 90, train loss: 0.00042090023634955287, val loss: 0.0032212617807090282\n",
      "Training time: 5.369277238845825\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.4657725989818573, val loss: 0.031930431723594666\n",
      "Epoch 10, train loss: 0.24244675040245056, val loss: 0.003888901090249419\n",
      "Epoch 20, train loss: 0.03669910877943039, val loss: 0.11811421066522598\n",
      "Epoch 30, train loss: 0.030671553686261177, val loss: 0.26296961307525635\n",
      "Epoch 40, train loss: 0.020139869302511215, val loss: 0.13292376697063446\n",
      "Epoch 50, train loss: 0.01547278743237257, val loss: 0.17610050737857819\n",
      "Epoch 60, train loss: 0.015779750421643257, val loss: 0.19780099391937256\n",
      "Epoch 70, train loss: 0.014791011810302734, val loss: 0.16413946449756622\n",
      "Epoch 80, train loss: 0.014219758100807667, val loss: 0.17195965349674225\n",
      "Epoch 90, train loss: 0.013799799606204033, val loss: 0.167925164103508\n",
      "Training time: 5.291754484176636\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.6865037083625793, val loss: 0.11488671600818634\n",
      "Epoch 10, train loss: 0.6592758297920227, val loss: 0.10459252446889877\n",
      "Epoch 20, train loss: 0.63248610496521, val loss: 0.09473539143800735\n",
      "Epoch 30, train loss: 0.6058785319328308, val loss: 0.08523863554000854\n",
      "Epoch 40, train loss: 0.5791720151901245, val loss: 0.0760292336344719\n",
      "Epoch 50, train loss: 0.5520744919776917, val loss: 0.06706496328115463\n",
      "Epoch 60, train loss: 0.5242528915405273, val loss: 0.05830899253487587\n",
      "Epoch 70, train loss: 0.495329886674881, val loss: 0.049726057797670364\n",
      "Epoch 80, train loss: 0.4648727476596832, val loss: 0.04129499942064285\n",
      "Epoch 90, train loss: 0.4323783218860626, val loss: 0.03302796930074692\n",
      "Training time: 5.287793397903442\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5739470720291138, val loss: 0.023427344858646393\n",
      "Epoch 10, train loss: 0.07829810678958893, val loss: 0.05444302782416344\n",
      "Epoch 20, train loss: 0.03164247050881386, val loss: 0.3114970028400421\n",
      "Epoch 30, train loss: 0.021866537630558014, val loss: 0.15356457233428955\n",
      "Epoch 40, train loss: 0.018313147127628326, val loss: 0.23543502390384674\n",
      "Epoch 50, train loss: 0.017243187874555588, val loss: 0.1830851435661316\n",
      "Epoch 60, train loss: 0.016436969861388206, val loss: 0.19966690242290497\n",
      "Epoch 70, train loss: 0.015295740216970444, val loss: 0.1708126962184906\n",
      "Epoch 80, train loss: 0.012207458727061749, val loss: 0.14557158946990967\n",
      "Epoch 90, train loss: 0.016842445358633995, val loss: 0.02518066205084324\n",
      "Epoch 100, train loss: 0.005492484197020531, val loss: 0.05167919024825096\n",
      "Epoch 110, train loss: 0.0017525075236335397, val loss: 0.02374253422021866\n",
      "Epoch 120, train loss: 0.0008438986842520535, val loss: 0.02145128697156906\n",
      "Epoch 130, train loss: 0.00045314020826481283, val loss: 0.024089287966489792\n",
      "Epoch 140, train loss: 0.00034219369990751147, val loss: 0.014318774454295635\n",
      "Epoch 150, train loss: 0.0002751031133811921, val loss: 0.013823161832988262\n",
      "Epoch 160, train loss: 0.0002270514814881608, val loss: 0.011420559138059616\n",
      "Epoch 170, train loss: 0.00018872729560825974, val loss: 0.008856228552758694\n",
      "Epoch 180, train loss: 0.00016002570919226855, val loss: 0.007562454789876938\n",
      "Epoch 190, train loss: 0.0001390405959682539, val loss: 0.0056699407286942005\n",
      "Training time: 10.73547911643982\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6366164684295654, val loss: 0.09190211445093155\n",
      "Epoch 10, train loss: 0.4189191162586212, val loss: 0.027503909543156624\n",
      "Epoch 20, train loss: 0.1889263093471527, val loss: 0.005761053413152695\n",
      "Epoch 30, train loss: 0.021986082196235657, val loss: 0.26989033818244934\n",
      "Epoch 40, train loss: 0.01348135620355606, val loss: 0.14636559784412384\n",
      "Epoch 50, train loss: 0.018734486773610115, val loss: 0.11425113677978516\n",
      "Epoch 60, train loss: 0.013649716973304749, val loss: 0.17306016385555267\n",
      "Epoch 70, train loss: 0.013000012375414371, val loss: 0.1539449244737625\n",
      "Epoch 80, train loss: 0.012708515860140324, val loss: 0.13595207035541534\n",
      "Epoch 90, train loss: 0.012021142989397049, val loss: 0.144567608833313\n",
      "Epoch 100, train loss: 0.011417577974498272, val loss: 0.13228270411491394\n",
      "Epoch 110, train loss: 0.010772822424769402, val loss: 0.12548965215682983\n",
      "Epoch 120, train loss: 0.010000293143093586, val loss: 0.11824840307235718\n",
      "Epoch 130, train loss: 0.00903791468590498, val loss: 0.10674578696489334\n",
      "Epoch 140, train loss: 0.00775692006573081, val loss: 0.09411878138780594\n",
      "Epoch 150, train loss: 0.005872244480997324, val loss: 0.07510719448328018\n",
      "Epoch 160, train loss: 0.0030060179997235537, val loss: 0.047099769115448\n",
      "Epoch 170, train loss: 0.0004039532213937491, val loss: 0.017194373533129692\n",
      "Epoch 180, train loss: 0.0006176075548864901, val loss: 0.010217282921075821\n",
      "Epoch 190, train loss: 0.00029400610947050154, val loss: 0.015826687216758728\n",
      "Training time: 10.583394765853882\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.7822728753089905, val loss: 0.1549803912639618\n",
      "Epoch 10, train loss: 0.7552385330200195, val loss: 0.14388197660446167\n",
      "Epoch 20, train loss: 0.7285995483398438, val loss: 0.133028045296669\n",
      "Epoch 30, train loss: 0.7020047903060913, val loss: 0.12239241600036621\n",
      "Epoch 40, train loss: 0.6751134395599365, val loss: 0.11187738180160522\n",
      "Epoch 50, train loss: 0.6475319266319275, val loss: 0.10137192159891129\n",
      "Epoch 60, train loss: 0.6188263297080994, val loss: 0.09077703207731247\n",
      "Epoch 70, train loss: 0.5885576009750366, val loss: 0.08001819998025894\n",
      "Epoch 80, train loss: 0.5562570095062256, val loss: 0.06904741376638412\n",
      "Epoch 90, train loss: 0.5213861465454102, val loss: 0.05785102769732475\n",
      "Epoch 100, train loss: 0.4833061099052429, val loss: 0.04647240415215492\n",
      "Epoch 110, train loss: 0.44125574827194214, val loss: 0.03505842015147209\n",
      "Epoch 120, train loss: 0.39437344670295715, val loss: 0.023950986564159393\n",
      "Epoch 130, train loss: 0.341823011636734, val loss: 0.013859071768820286\n",
      "Epoch 140, train loss: 0.28311946988105774, val loss: 0.006159287411719561\n",
      "Epoch 150, train loss: 0.21882976591587067, val loss: 0.003384890966117382\n",
      "Epoch 160, train loss: 0.1518799066543579, val loss: 0.009816582314670086\n",
      "Epoch 170, train loss: 0.08926226198673248, val loss: 0.03139210492372513\n",
      "Epoch 180, train loss: 0.04196960851550102, val loss: 0.07246393710374832\n",
      "Epoch 190, train loss: 0.01854240894317627, val loss: 0.12666261196136475\n",
      "Training time: 10.6717529296875\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.45805445313453674, val loss: 0.00517204450443387\n",
      "Epoch 10, train loss: 0.020665394142270088, val loss: 0.24034185707569122\n",
      "Epoch 20, train loss: 0.022008484229445457, val loss: 0.17753125727176666\n",
      "Epoch 30, train loss: 0.019502535462379456, val loss: 0.23470887541770935\n",
      "Epoch 40, train loss: 0.017963269725441933, val loss: 0.21872040629386902\n",
      "Epoch 50, train loss: 0.005869436077773571, val loss: 0.0027091768570244312\n",
      "Epoch 60, train loss: 0.0009172512218356133, val loss: 0.04017181321978569\n",
      "Epoch 70, train loss: 0.0012781908735632896, val loss: 0.04660847783088684\n",
      "Epoch 80, train loss: 0.0006206570542417467, val loss: 0.02546125464141369\n",
      "Epoch 90, train loss: 0.0003047562204301357, val loss: 0.019737932831048965\n",
      "Training time: 7.895464658737183\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.43424734473228455, val loss: 0.027382321655750275\n",
      "Epoch 10, train loss: 0.2435293048620224, val loss: 0.004138443153351545\n",
      "Epoch 20, train loss: 0.02201053500175476, val loss: 0.3474702835083008\n",
      "Epoch 30, train loss: 0.018074965104460716, val loss: 0.15900106728076935\n",
      "Epoch 40, train loss: 0.02079036831855774, val loss: 0.16349995136260986\n",
      "Epoch 50, train loss: 0.018970713019371033, val loss: 0.2361581027507782\n",
      "Epoch 60, train loss: 0.016760997474193573, val loss: 0.17445984482765198\n",
      "Epoch 70, train loss: 0.01571374014019966, val loss: 0.189909890294075\n",
      "Epoch 80, train loss: 0.014629964716732502, val loss: 0.1702709197998047\n",
      "Epoch 90, train loss: 0.012656225822865963, val loss: 0.15097171068191528\n",
      "Training time: 7.979665040969849\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5655261874198914, val loss: 0.07038052380084991\n",
      "Epoch 10, train loss: 0.5416387915611267, val loss: 0.06249835342168808\n",
      "Epoch 20, train loss: 0.518368661403656, val loss: 0.055125873535871506\n",
      "Epoch 30, train loss: 0.4951692819595337, val loss: 0.04810650646686554\n",
      "Epoch 40, train loss: 0.471310555934906, val loss: 0.041266750544309616\n",
      "Epoch 50, train loss: 0.44605812430381775, val loss: 0.03448714688420296\n",
      "Epoch 60, train loss: 0.4186903238296509, val loss: 0.027720008045434952\n",
      "Epoch 70, train loss: 0.38841304183006287, val loss: 0.021004298701882362\n",
      "Epoch 80, train loss: 0.35426074266433716, val loss: 0.014511839486658573\n",
      "Epoch 90, train loss: 0.3150951862335205, val loss: 0.008680015802383423\n",
      "Training time: 7.933088064193726\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7296398878097534, val loss: 0.05246352031826973\n",
      "Epoch 10, train loss: 0.07165682315826416, val loss: 0.07687228918075562\n",
      "Epoch 20, train loss: 0.030957920476794243, val loss: 0.3333216905593872\n",
      "Epoch 30, train loss: 0.02291814051568508, val loss: 0.17690114676952362\n",
      "Epoch 40, train loss: 0.020190145820379257, val loss: 0.26102209091186523\n",
      "Epoch 50, train loss: 0.019585516303777695, val loss: 0.21335230767726898\n",
      "Epoch 60, train loss: 0.019386811181902885, val loss: 0.23736070096492767\n",
      "Epoch 70, train loss: 0.019249146804213524, val loss: 0.22154474258422852\n",
      "Epoch 80, train loss: 0.018988223746418953, val loss: 0.22829751670360565\n",
      "Epoch 90, train loss: 0.017082134261727333, val loss: 0.20912207663059235\n",
      "Epoch 100, train loss: 0.00821057241410017, val loss: 0.1009107455611229\n",
      "Epoch 110, train loss: 0.0040314593352377415, val loss: 0.06071489676833153\n",
      "Epoch 120, train loss: 0.003632414387539029, val loss: 0.07072612643241882\n",
      "Epoch 130, train loss: 0.0023272030521184206, val loss: 0.06490752100944519\n",
      "Epoch 140, train loss: 0.0010155605850741267, val loss: 0.028660202398896217\n",
      "Epoch 150, train loss: 0.0003932132094632834, val loss: 0.008116109296679497\n",
      "Epoch 160, train loss: 0.0001787078654160723, val loss: 0.008729598484933376\n",
      "Epoch 170, train loss: 0.00014152552466839552, val loss: 0.004199101589620113\n",
      "Epoch 180, train loss: 0.00012928179057780653, val loss: 0.005257810465991497\n",
      "Epoch 190, train loss: 0.00012244319077581167, val loss: 0.004516707733273506\n",
      "Training time: 15.90150499343872\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.44071662425994873, val loss: 0.027467429637908936\n",
      "Epoch 10, train loss: 0.22568480670452118, val loss: 0.005656007677316666\n",
      "Epoch 20, train loss: 0.018032848834991455, val loss: 0.25813132524490356\n",
      "Epoch 30, train loss: 0.01839391328394413, val loss: 0.20772810280323029\n",
      "Epoch 40, train loss: 0.02328427881002426, val loss: 0.15971648693084717\n",
      "Epoch 50, train loss: 0.018548518419265747, val loss: 0.23997032642364502\n",
      "Epoch 60, train loss: 0.017571033909916878, val loss: 0.20697885751724243\n",
      "Epoch 70, train loss: 0.017365461215376854, val loss: 0.19431723654270172\n",
      "Epoch 80, train loss: 0.016725238412618637, val loss: 0.2054661363363266\n",
      "Epoch 90, train loss: 0.01583668775856495, val loss: 0.18580196797847748\n",
      "Epoch 100, train loss: 0.014410477131605148, val loss: 0.17733919620513916\n",
      "Epoch 110, train loss: 0.01156641822308302, val loss: 0.14699774980545044\n",
      "Epoch 120, train loss: 0.0057515050284564495, val loss: 0.09380096197128296\n",
      "Epoch 130, train loss: 0.0008741557248868048, val loss: 0.04104342311620712\n",
      "Epoch 140, train loss: 0.0009049443760886788, val loss: 0.03285966068506241\n",
      "Epoch 150, train loss: 0.0006614835001528263, val loss: 0.03647381067276001\n",
      "Epoch 160, train loss: 0.0004628965980373323, val loss: 0.03034871257841587\n",
      "Epoch 170, train loss: 0.00046214318717829883, val loss: 0.027989152818918228\n",
      "Epoch 180, train loss: 0.0004296625847928226, val loss: 0.02823595143854618\n",
      "Epoch 190, train loss: 0.0004039645427837968, val loss: 0.02630782686173916\n",
      "Training time: 15.907662868499756\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.885196328163147, val loss: 0.20765039324760437\n",
      "Epoch 10, train loss: 0.8493220806121826, val loss: 0.19072505831718445\n",
      "Epoch 20, train loss: 0.8144403696060181, val loss: 0.17464987933635712\n",
      "Epoch 30, train loss: 0.780080258846283, val loss: 0.15921545028686523\n",
      "Epoch 40, train loss: 0.7454642057418823, val loss: 0.1441059559583664\n",
      "Epoch 50, train loss: 0.7096846699714661, val loss: 0.12899203598499298\n",
      "Epoch 60, train loss: 0.6718654036521912, val loss: 0.11362013965845108\n",
      "Epoch 70, train loss: 0.6311143636703491, val loss: 0.09780816733837128\n",
      "Epoch 80, train loss: 0.5863733291625977, val loss: 0.08141955733299255\n",
      "Epoch 90, train loss: 0.5363062024116516, val loss: 0.06439180672168732\n",
      "Epoch 100, train loss: 0.47928115725517273, val loss: 0.04686259478330612\n",
      "Epoch 110, train loss: 0.41350215673446655, val loss: 0.029454877600073814\n",
      "Epoch 120, train loss: 0.33746492862701416, val loss: 0.01386663131415844\n",
      "Epoch 130, train loss: 0.2512701749801636, val loss: 0.004035244230180979\n",
      "Epoch 140, train loss: 0.15983986854553223, val loss: 0.007976054213941097\n",
      "Epoch 150, train loss: 0.07777757942676544, val loss: 0.038148414343595505\n",
      "Epoch 160, train loss: 0.027162715792655945, val loss: 0.10109718888998032\n",
      "Epoch 170, train loss: 0.014542716555297375, val loss: 0.1710137575864792\n",
      "Epoch 180, train loss: 0.016154926270246506, val loss: 0.19934114813804626\n",
      "Epoch 190, train loss: 0.015297668986022472, val loss: 0.18669863045215607\n",
      "Training time: 15.906689167022705\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.5692111849784851, val loss: 0.003357230219990015\n",
      "Epoch 10, train loss: 0.018102074041962624, val loss: 0.2802736759185791\n",
      "Epoch 20, train loss: 0.023942193016409874, val loss: 0.13962626457214355\n",
      "Epoch 30, train loss: 0.019080689176917076, val loss: 0.25155186653137207\n",
      "Epoch 40, train loss: 0.017702868208289146, val loss: 0.17918750643730164\n",
      "Epoch 50, train loss: 0.016569335013628006, val loss: 0.20662784576416016\n",
      "Epoch 60, train loss: 0.014563487842679024, val loss: 0.16061244904994965\n",
      "Epoch 70, train loss: 0.0018557279836386442, val loss: 0.04210156202316284\n",
      "Epoch 80, train loss: 0.006909628864377737, val loss: 0.08160171657800674\n",
      "Epoch 90, train loss: 0.005644768942147493, val loss: 0.05850943177938461\n",
      "Training time: 13.333340883255005\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.823944628238678, val loss: 0.16142745316028595\n",
      "Epoch 10, train loss: 0.3337823748588562, val loss: 0.007892491295933723\n",
      "Epoch 20, train loss: 0.07367966324090958, val loss: 0.2723625600337982\n",
      "Epoch 30, train loss: 0.03492101654410362, val loss: 0.068998783826828\n",
      "Epoch 40, train loss: 0.015299846418201923, val loss: 0.19567276537418365\n",
      "Epoch 50, train loss: 0.012351314537227154, val loss: 0.13091516494750977\n",
      "Epoch 60, train loss: 0.011977867223322392, val loss: 0.13596569001674652\n",
      "Epoch 70, train loss: 0.011383475735783577, val loss: 0.1371050328016281\n",
      "Epoch 80, train loss: 0.010583437979221344, val loss: 0.11955131590366364\n",
      "Epoch 90, train loss: 0.009669655002653599, val loss: 0.11621222645044327\n",
      "Training time: 13.402709007263184\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5475691556930542, val loss: 0.06502847373485565\n",
      "Epoch 10, train loss: 0.510790228843689, val loss: 0.05343782901763916\n",
      "Epoch 20, train loss: 0.47455906867980957, val loss: 0.042854271829128265\n",
      "Epoch 30, train loss: 0.43801820278167725, val loss: 0.03310154005885124\n",
      "Epoch 40, train loss: 0.4001515805721283, val loss: 0.024108001962304115\n",
      "Epoch 50, train loss: 0.35971158742904663, val loss: 0.015946416184306145\n",
      "Epoch 60, train loss: 0.3151755630970001, val loss: 0.008994730189442635\n",
      "Epoch 70, train loss: 0.26473119854927063, val loss: 0.004297361709177494\n",
      "Epoch 80, train loss: 0.2063988894224167, val loss: 0.0044060079380869865\n",
      "Epoch 90, train loss: 0.13910499215126038, val loss: 0.01559606846421957\n",
      "Training time: 13.229124546051025\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5920754671096802, val loss: 0.0036702777724713087\n",
      "Epoch 10, train loss: 0.018546421080827713, val loss: 0.13134677708148956\n",
      "Epoch 20, train loss: 0.011841953732073307, val loss: 0.1578657031059265\n",
      "Epoch 30, train loss: 0.018170692026615143, val loss: 0.015838585793972015\n",
      "Epoch 40, train loss: 0.004080977290868759, val loss: 0.04015317186713219\n",
      "Epoch 50, train loss: 0.0005524742300622165, val loss: 0.013285051099956036\n",
      "Epoch 60, train loss: 0.0006872295634821057, val loss: 0.002514667809009552\n",
      "Epoch 70, train loss: 0.0003083647461608052, val loss: 0.00661642849445343\n",
      "Epoch 80, train loss: 0.00018229502893518656, val loss: 0.0013880361802875996\n",
      "Epoch 90, train loss: 0.00011768980766646564, val loss: 0.0026903238613158464\n",
      "Epoch 100, train loss: 7.541337981820107e-05, val loss: 0.0014001447707414627\n",
      "Epoch 110, train loss: 7.844837091397494e-05, val loss: 0.0017231990350410342\n",
      "Epoch 120, train loss: 7.262957660714164e-05, val loss: 0.0014458891237154603\n",
      "Epoch 130, train loss: 7.22877521184273e-05, val loss: 0.0014039617963135242\n",
      "Epoch 140, train loss: 7.175369682954624e-05, val loss: 0.0012913906248286366\n",
      "Epoch 150, train loss: 7.113743049558252e-05, val loss: 0.0012577221496030688\n",
      "Epoch 160, train loss: 7.058425399009138e-05, val loss: 0.0011802203953266144\n",
      "Epoch 170, train loss: 7.006744999671355e-05, val loss: 0.001141335698775947\n",
      "Epoch 180, train loss: 6.957096047699451e-05, val loss: 0.0010811153333634138\n",
      "Epoch 190, train loss: 6.907459464855492e-05, val loss: 0.0010394984856247902\n",
      "Training time: 26.590431213378906\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8416176438331604, val loss: 0.1639583259820938\n",
      "Epoch 10, train loss: 0.3442619740962982, val loss: 0.009642950259149075\n",
      "Epoch 20, train loss: 0.07578904181718826, val loss: 0.26205164194107056\n",
      "Epoch 30, train loss: 0.03518064692616463, val loss: 0.06367401778697968\n",
      "Epoch 40, train loss: 0.014955335296690464, val loss: 0.18988673388957977\n",
      "Epoch 50, train loss: 0.011879745870828629, val loss: 0.12452568113803864\n",
      "Epoch 60, train loss: 0.01144163217395544, val loss: 0.1318134218454361\n",
      "Epoch 70, train loss: 0.010845445096492767, val loss: 0.1311832219362259\n",
      "Epoch 80, train loss: 0.009994493797421455, val loss: 0.11485640704631805\n",
      "Epoch 90, train loss: 0.0089339604601264, val loss: 0.1091134250164032\n",
      "Epoch 100, train loss: 0.0074912472628057, val loss: 0.09283459186553955\n",
      "Epoch 110, train loss: 0.005329947918653488, val loss: 0.07092567533254623\n",
      "Epoch 120, train loss: 0.002178736962378025, val loss: 0.04045959189534187\n",
      "Epoch 130, train loss: 0.00045618860167451203, val loss: 0.014946972951292992\n",
      "Epoch 140, train loss: 0.0005556783871725202, val loss: 0.012260526418685913\n",
      "Epoch 150, train loss: 0.0003455200931057334, val loss: 0.016786135733127594\n",
      "Epoch 160, train loss: 0.0003282354155089706, val loss: 0.015790637582540512\n",
      "Epoch 170, train loss: 0.0002775048487819731, val loss: 0.012798559851944447\n",
      "Epoch 180, train loss: 0.000256659637670964, val loss: 0.011816274374723434\n",
      "Epoch 190, train loss: 0.00023318776220548898, val loss: 0.011728770099580288\n",
      "Training time: 26.779694080352783\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6065160632133484, val loss: 0.08630926162004471\n",
      "Epoch 10, train loss: 0.5690453052520752, val loss: 0.07350397855043411\n",
      "Epoch 20, train loss: 0.5329463481903076, val loss: 0.06173000857234001\n",
      "Epoch 30, train loss: 0.49659398198127747, val loss: 0.05056273937225342\n",
      "Epoch 40, train loss: 0.45813363790512085, val loss: 0.03966902568936348\n",
      "Epoch 50, train loss: 0.4153483211994171, val loss: 0.02884216420352459\n",
      "Epoch 60, train loss: 0.3653159439563751, val loss: 0.01816670410335064\n",
      "Epoch 70, train loss: 0.3038772642612457, val loss: 0.00850252527743578\n",
      "Epoch 80, train loss: 0.22530072927474976, val loss: 0.00328730046749115\n",
      "Epoch 90, train loss: 0.1254279464483261, val loss: 0.01533712912350893\n",
      "Epoch 100, train loss: 0.027801817283034325, val loss: 0.08839549124240875\n",
      "Epoch 110, train loss: 0.021573606878519058, val loss: 0.21160373091697693\n",
      "Epoch 120, train loss: 0.012628491036593914, val loss: 0.15012119710445404\n",
      "Epoch 130, train loss: 0.013263554312288761, val loss: 0.11894623935222626\n",
      "Epoch 140, train loss: 0.011894776485860348, val loss: 0.13398724794387817\n",
      "Epoch 150, train loss: 0.011757963337004185, val loss: 0.14490191638469696\n",
      "Epoch 160, train loss: 0.011415913701057434, val loss: 0.13579188287258148\n",
      "Epoch 170, train loss: 0.011246724054217339, val loss: 0.13022932410240173\n",
      "Epoch 180, train loss: 0.011025214567780495, val loss: 0.13111037015914917\n",
      "Epoch 190, train loss: 0.010824358090758324, val loss: 0.12961162626743317\n",
      "Training time: 26.710318326950073\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.4970349669456482, val loss: 0.009853667579591274\n",
      "Epoch 10, train loss: 0.03817519545555115, val loss: 0.3832840621471405\n",
      "Epoch 20, train loss: 0.02332957461476326, val loss: 0.18881820142269135\n",
      "Epoch 30, train loss: 0.01999726891517639, val loss: 0.24042053520679474\n",
      "Epoch 40, train loss: 0.019085854291915894, val loss: 0.22193877398967743\n",
      "Epoch 50, train loss: 0.01680620200932026, val loss: 0.19736060500144958\n",
      "Epoch 60, train loss: 0.006059231236577034, val loss: 0.03509531915187836\n",
      "Epoch 70, train loss: 0.0011931839399039745, val loss: 0.062123578041791916\n",
      "Epoch 80, train loss: 0.0008276919252239168, val loss: 0.00877766590565443\n",
      "Epoch 90, train loss: 0.0004022852226626128, val loss: 0.006201340351253748\n",
      "Training time: 21.137308359146118\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7611060738563538, val loss: 0.1335105001926422\n",
      "Epoch 10, train loss: 0.27343156933784485, val loss: 0.004564659204334021\n",
      "Epoch 20, train loss: 0.01998112164437771, val loss: 0.180373415350914\n",
      "Epoch 30, train loss: 0.028007714077830315, val loss: 0.13540811836719513\n",
      "Epoch 40, train loss: 0.022628990933299065, val loss: 0.25532227754592896\n",
      "Epoch 50, train loss: 0.018621422350406647, val loss: 0.15931235253810883\n",
      "Epoch 60, train loss: 0.01675146259367466, val loss: 0.21247892081737518\n",
      "Epoch 70, train loss: 0.016048822551965714, val loss: 0.1775907278060913\n",
      "Epoch 80, train loss: 0.015542718581855297, val loss: 0.18775953352451324\n",
      "Epoch 90, train loss: 0.0149921253323555, val loss: 0.17297689616680145\n",
      "Training time: 21.38390278816223\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5265462398529053, val loss: 0.05748322606086731\n",
      "Epoch 10, train loss: 0.48593276739120483, val loss: 0.045077238231897354\n",
      "Epoch 20, train loss: 0.4443584084510803, val loss: 0.03361396864056587\n",
      "Epoch 30, train loss: 0.4000852108001709, val loss: 0.023000214248895645\n",
      "Epoch 40, train loss: 0.3505231440067291, val loss: 0.013402302749454975\n",
      "Epoch 50, train loss: 0.29192841053009033, val loss: 0.005856993608176708\n",
      "Epoch 60, train loss: 0.2194487303495407, val loss: 0.004082945641130209\n",
      "Epoch 70, train loss: 0.12965701520442963, val loss: 0.020417071878910065\n",
      "Epoch 80, train loss: 0.038374170660972595, val loss: 0.09330956637859344\n",
      "Epoch 90, train loss: 0.019596699625253677, val loss: 0.23764179646968842\n",
      "Training time: 21.25218439102173\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6435865759849548, val loss: 0.004813650157302618\n",
      "Epoch 10, train loss: 0.052830297499895096, val loss: 0.40893101692199707\n",
      "Epoch 20, train loss: 0.025533601641654968, val loss: 0.17700915038585663\n",
      "Epoch 30, train loss: 0.020740360021591187, val loss: 0.2494322508573532\n",
      "Epoch 40, train loss: 0.01951078325510025, val loss: 0.21627719700336456\n",
      "Epoch 50, train loss: 0.018736058846116066, val loss: 0.21876685321331024\n",
      "Epoch 60, train loss: 0.008176259696483612, val loss: 0.0559292808175087\n",
      "Epoch 70, train loss: 0.004093588329851627, val loss: 0.09282994270324707\n",
      "Epoch 80, train loss: 0.0017018711660057306, val loss: 0.05226340889930725\n",
      "Epoch 90, train loss: 0.000703783705830574, val loss: 0.019038651138544083\n",
      "Epoch 100, train loss: 0.0003649451828096062, val loss: 0.0062699089758098125\n",
      "Epoch 110, train loss: 0.0002128398627974093, val loss: 0.0019429101375862956\n",
      "Epoch 120, train loss: 0.00014554764493368566, val loss: 0.0009298060904257\n",
      "Epoch 130, train loss: 0.0001285671314690262, val loss: 0.0009516549180261791\n",
      "Epoch 140, train loss: 0.00012228517152834684, val loss: 0.000643323000986129\n",
      "Epoch 150, train loss: 0.00011841158993775025, val loss: 0.0006403139559552073\n",
      "Epoch 160, train loss: 0.00019002571934834123, val loss: 0.0009231801377609372\n",
      "Epoch 170, train loss: 0.00018709270807448775, val loss: 0.0007385648787021637\n",
      "Epoch 180, train loss: 0.0001229599438374862, val loss: 0.0006175015005283058\n",
      "Epoch 190, train loss: 0.00014413578901439905, val loss: 0.0007416708394885063\n",
      "Training time: 42.18154525756836\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.4886557459831238, val loss: 0.03638526797294617\n",
      "Epoch 10, train loss: 0.08315835893154144, val loss: 0.09890351444482803\n",
      "Epoch 20, train loss: 0.02160472981631756, val loss: 0.136026069521904\n",
      "Epoch 30, train loss: 0.019228428602218628, val loss: 0.19147680699825287\n",
      "Epoch 40, train loss: 0.018480399623513222, val loss: 0.2218470424413681\n",
      "Epoch 50, train loss: 0.017487430945038795, val loss: 0.17523281276226044\n",
      "Epoch 60, train loss: 0.01601317524909973, val loss: 0.19826525449752808\n",
      "Epoch 70, train loss: 0.014222623780369759, val loss: 0.16014118492603302\n",
      "Epoch 80, train loss: 0.010995958931744099, val loss: 0.1326034516096115\n",
      "Epoch 90, train loss: 0.0034697474911808968, val loss: 0.05283573642373085\n",
      "Epoch 100, train loss: 0.0017712786793708801, val loss: 0.010242614895105362\n",
      "Epoch 110, train loss: 0.0007005747174844146, val loss: 0.025928445160388947\n",
      "Epoch 120, train loss: 0.0002534639206714928, val loss: 0.011527878232300282\n",
      "Epoch 130, train loss: 0.00021963840117678046, val loss: 0.008917593397200108\n",
      "Epoch 140, train loss: 0.0001912170700961724, val loss: 0.008615421131253242\n",
      "Epoch 150, train loss: 0.00016249679902102798, val loss: 0.0050509036518633366\n",
      "Epoch 160, train loss: 0.00014890922466292977, val loss: 0.005093094892799854\n",
      "Epoch 170, train loss: 0.00014406733680516481, val loss: 0.0038009565323591232\n",
      "Epoch 180, train loss: 0.00014251678658183664, val loss: 0.003697091480717063\n",
      "Epoch 190, train loss: 0.000141879019793123, val loss: 0.0033374750055372715\n",
      "Training time: 42.43697142601013\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.779547929763794, val loss: 0.15661658346652985\n",
      "Epoch 10, train loss: 0.7340667843818665, val loss: 0.13686078786849976\n",
      "Epoch 20, train loss: 0.687892496585846, val loss: 0.11756666004657745\n",
      "Epoch 30, train loss: 0.6379464864730835, val loss: 0.09770707786083221\n",
      "Epoch 40, train loss: 0.579801082611084, val loss: 0.07614634931087494\n",
      "Epoch 50, train loss: 0.5070165991783142, val loss: 0.05195366591215134\n",
      "Epoch 60, train loss: 0.4096447825431824, val loss: 0.02553744427859783\n",
      "Epoch 70, train loss: 0.27373582124710083, val loss: 0.004510392900556326\n",
      "Epoch 80, train loss: 0.10118091851472855, val loss: 0.032677456736564636\n",
      "Epoch 90, train loss: 0.015827808529138565, val loss: 0.22297635674476624\n",
      "Epoch 100, train loss: 0.024479763582348824, val loss: 0.24225454032421112\n",
      "Epoch 110, train loss: 0.015253696590662003, val loss: 0.14810506999492645\n",
      "Epoch 120, train loss: 0.01581486128270626, val loss: 0.14739403128623962\n",
      "Epoch 130, train loss: 0.014392401091754436, val loss: 0.17647963762283325\n",
      "Epoch 140, train loss: 0.014380366541445255, val loss: 0.17618316411972046\n",
      "Epoch 150, train loss: 0.014081751927733421, val loss: 0.1629413664340973\n",
      "Epoch 160, train loss: 0.013926442712545395, val loss: 0.16195020079612732\n",
      "Epoch 170, train loss: 0.013743814080953598, val loss: 0.16400504112243652\n",
      "Epoch 180, train loss: 0.01356056984513998, val loss: 0.16090025007724762\n",
      "Epoch 190, train loss: 0.01336861215531826, val loss: 0.15756964683532715\n",
      "Training time: 42.16653323173523\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7683740854263306, val loss: 0.023312026634812355\n",
      "Epoch 10, train loss: 0.2101145088672638, val loss: 0.010623902082443237\n",
      "Epoch 20, train loss: 0.02474621683359146, val loss: 0.1345045268535614\n",
      "Epoch 30, train loss: 0.024017980322241783, val loss: 0.29107728600502014\n",
      "Epoch 40, train loss: 0.01998300477862358, val loss: 0.1880267709493637\n",
      "Epoch 50, train loss: 0.017638489603996277, val loss: 0.20632246136665344\n",
      "Epoch 60, train loss: 0.01600959151983261, val loss: 0.2036263644695282\n",
      "Epoch 70, train loss: 0.0629843920469284, val loss: 0.05626276507973671\n",
      "Epoch 80, train loss: 0.034662049263715744, val loss: 0.3261842429637909\n",
      "Epoch 90, train loss: 0.023533886298537254, val loss: 0.16821597516536713\n",
      "Training time: 41.7757511138916\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5799933075904846, val loss: 0.05735452100634575\n",
      "Epoch 10, train loss: 0.10304175317287445, val loss: 0.386779248714447\n",
      "Epoch 20, train loss: 0.03401792421936989, val loss: 0.10761027783155441\n",
      "Epoch 30, train loss: 0.018781088292598724, val loss: 0.20319540798664093\n",
      "Epoch 40, train loss: 0.015326240099966526, val loss: 0.14733624458312988\n",
      "Epoch 50, train loss: 0.013031049631536007, val loss: 0.1565074324607849\n",
      "Epoch 60, train loss: 0.011532893404364586, val loss: 0.1418498456478119\n",
      "Epoch 70, train loss: 0.009914587251842022, val loss: 0.1143682450056076\n",
      "Epoch 80, train loss: 0.0073387413285672665, val loss: 0.09247904270887375\n",
      "Epoch 90, train loss: 0.002935998374596238, val loss: 0.04436078667640686\n",
      "Training time: 41.59478235244751\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.7440399527549744, val loss: 0.14041709899902344\n",
      "Epoch 10, train loss: 0.6695125102996826, val loss: 0.11159852892160416\n",
      "Epoch 20, train loss: 0.5942140817642212, val loss: 0.08442278951406479\n",
      "Epoch 30, train loss: 0.510546088218689, val loss: 0.05720686540007591\n",
      "Epoch 40, train loss: 0.4070243537425995, val loss: 0.02917064167559147\n",
      "Epoch 50, train loss: 0.265850305557251, val loss: 0.0054269153624773026\n",
      "Epoch 60, train loss: 0.07867656648159027, val loss: 0.030929096043109894\n",
      "Epoch 70, train loss: 0.034339744597673416, val loss: 0.22906756401062012\n",
      "Epoch 80, train loss: 0.010166346095502377, val loss: 0.10875462740659714\n",
      "Epoch 90, train loss: 0.013858591206371784, val loss: 0.09404251724481583\n",
      "Training time: 41.553815603256226\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5462337136268616, val loss: 0.062125544995069504\n",
      "Epoch 10, train loss: 0.1604831963777542, val loss: 0.02744683250784874\n",
      "Epoch 20, train loss: 0.028188619762659073, val loss: 0.13254159688949585\n",
      "Epoch 30, train loss: 0.023891927674412727, val loss: 0.285177618265152\n",
      "Epoch 40, train loss: 0.019786518067121506, val loss: 0.1983117312192917\n",
      "Epoch 50, train loss: 0.01805086061358452, val loss: 0.2076907753944397\n",
      "Epoch 60, train loss: 0.01731821335852146, val loss: 0.21700195968151093\n",
      "Epoch 70, train loss: 0.011086715385317802, val loss: 0.1355036199092865\n",
      "Epoch 80, train loss: 0.012947425246238708, val loss: 0.0905381441116333\n",
      "Epoch 90, train loss: 0.008141245692968369, val loss: 0.07576833665370941\n",
      "Epoch 100, train loss: 0.0021081131417304277, val loss: 0.05483526363968849\n",
      "Epoch 110, train loss: 0.0009950078092515469, val loss: 0.04057151451706886\n",
      "Epoch 120, train loss: 0.0008445191779173911, val loss: 0.02918280102312565\n",
      "Epoch 130, train loss: 0.0005989247001707554, val loss: 0.03118329867720604\n",
      "Epoch 140, train loss: 0.0004889577976427972, val loss: 0.02980559505522251\n",
      "Epoch 150, train loss: 0.00040064906352199614, val loss: 0.02857552282512188\n",
      "Epoch 160, train loss: 0.0003448709612712264, val loss: 0.025214172899723053\n",
      "Epoch 170, train loss: 0.0002907636226154864, val loss: 0.02223634533584118\n",
      "Epoch 180, train loss: 0.0002480179537087679, val loss: 0.01929052174091339\n",
      "Epoch 190, train loss: 0.00021313682373147458, val loss: 0.017142489552497864\n",
      "Training time: 83.491286277771\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6341637372970581, val loss: 0.07524052262306213\n",
      "Epoch 10, train loss: 0.11004738509654999, val loss: 0.3352580666542053\n",
      "Epoch 20, train loss: 0.031861837953329086, val loss: 0.09427980333566666\n",
      "Epoch 30, train loss: 0.015777815133333206, val loss: 0.17000636458396912\n",
      "Epoch 40, train loss: 0.013266940601170063, val loss: 0.13661696016788483\n",
      "Epoch 50, train loss: 0.011447477154433727, val loss: 0.1323818862438202\n",
      "Epoch 60, train loss: 0.010218638926744461, val loss: 0.12854552268981934\n",
      "Epoch 70, train loss: 0.008493481203913689, val loss: 0.09556121379137039\n",
      "Epoch 80, train loss: 0.005253301467746496, val loss: 0.06478303670883179\n",
      "Epoch 90, train loss: 0.0007505426765419543, val loss: 0.01920696720480919\n",
      "Epoch 100, train loss: 0.000810133817140013, val loss: 0.006071710493415594\n",
      "Epoch 110, train loss: 0.0003680429654195905, val loss: 0.014745105989277363\n",
      "Epoch 120, train loss: 0.0002912342897616327, val loss: 0.011156733147799969\n",
      "Epoch 130, train loss: 0.0002599266590550542, val loss: 0.007200114894658327\n",
      "Epoch 140, train loss: 0.00021834520157426596, val loss: 0.008152596652507782\n",
      "Epoch 150, train loss: 0.0002019215316977352, val loss: 0.007449726574122906\n",
      "Epoch 160, train loss: 0.0001839058386394754, val loss: 0.0059651220217347145\n",
      "Epoch 170, train loss: 0.00016826290811877698, val loss: 0.005561642348766327\n",
      "Epoch 180, train loss: 0.0001550830784253776, val loss: 0.004894388373941183\n",
      "Epoch 190, train loss: 0.00014336274762172252, val loss: 0.004131925757974386\n",
      "Training time: 83.14795112609863\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.5680687427520752, val loss: 0.07225525379180908\n",
      "Epoch 10, train loss: 0.5052250623703003, val loss: 0.052736327052116394\n",
      "Epoch 20, train loss: 0.44094958901405334, val loss: 0.035096216946840286\n",
      "Epoch 30, train loss: 0.3681931793689728, val loss: 0.018827836960554123\n",
      "Epoch 40, train loss: 0.27696770429611206, val loss: 0.005755890626460314\n",
      "Epoch 50, train loss: 0.1542598158121109, val loss: 0.008517419919371605\n",
      "Epoch 60, train loss: 0.021078404039144516, val loss: 0.0999697595834732\n",
      "Epoch 70, train loss: 0.026548217982053757, val loss: 0.20018672943115234\n",
      "Epoch 80, train loss: 0.012811555527150631, val loss: 0.1049218401312828\n",
      "Epoch 90, train loss: 0.011813810095191002, val loss: 0.11609791964292526\n",
      "Epoch 100, train loss: 0.01100606843829155, val loss: 0.1422167271375656\n",
      "Epoch 110, train loss: 0.010188793763518333, val loss: 0.12439034879207611\n",
      "Epoch 120, train loss: 0.010004756972193718, val loss: 0.11886169016361237\n",
      "Epoch 130, train loss: 0.009658082388341427, val loss: 0.12232706695795059\n",
      "Epoch 140, train loss: 0.009330473840236664, val loss: 0.11617467552423477\n",
      "Epoch 150, train loss: 0.009007107466459274, val loss: 0.11232606321573257\n",
      "Epoch 160, train loss: 0.00865892507135868, val loss: 0.10982674360275269\n",
      "Epoch 170, train loss: 0.008288074284791946, val loss: 0.10511209070682526\n",
      "Epoch 180, train loss: 0.007893947884440422, val loss: 0.1012362465262413\n",
      "Epoch 190, train loss: 0.007473459001630545, val loss: 0.09683134406805038\n",
      "Training time: 85.51606917381287\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6881755590438843, val loss: 0.13514691591262817\n",
      "Epoch 10, train loss: 0.08496543020009995, val loss: 0.17763331532478333\n",
      "Epoch 20, train loss: 0.023037657141685486, val loss: 0.24011582136154175\n",
      "Epoch 30, train loss: 0.019847873598337173, val loss: 0.2200154960155487\n",
      "Epoch 40, train loss: 0.019697731360793114, val loss: 0.2239447683095932\n",
      "Epoch 50, train loss: 0.019671469926834106, val loss: 0.22707222402095795\n",
      "Epoch 60, train loss: 0.019659794867038727, val loss: 0.2281065285205841\n",
      "Epoch 70, train loss: 0.019659489393234253, val loss: 0.22850686311721802\n",
      "Epoch 80, train loss: 0.019662536680698395, val loss: 0.22930127382278442\n",
      "Epoch 90, train loss: 0.01966145448386669, val loss: 0.23070523142814636\n",
      "Training time: 69.16243600845337\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5525386333465576, val loss: 0.046860989183187485\n",
      "Epoch 10, train loss: 0.03688986226916313, val loss: 0.2005767673254013\n",
      "Epoch 20, train loss: 0.018284020945429802, val loss: 0.19568587839603424\n",
      "Epoch 30, train loss: 0.016914645209908485, val loss: 0.16025292873382568\n",
      "Epoch 40, train loss: 0.01627679355442524, val loss: 0.21487179398536682\n",
      "Epoch 50, train loss: 0.015761040151119232, val loss: 0.16314436495304108\n",
      "Epoch 60, train loss: 0.014477564953267574, val loss: 0.17927905917167664\n",
      "Epoch 70, train loss: 0.012489959597587585, val loss: 0.1521630734205246\n",
      "Epoch 80, train loss: 0.008631852455437183, val loss: 0.1069238930940628\n",
      "Epoch 90, train loss: 0.00073554168920964, val loss: 0.018208704888820648\n",
      "Training time: 69.7803099155426\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.6606984734535217, val loss: 0.10442829132080078\n",
      "Epoch 10, train loss: 0.5818506479263306, val loss: 0.07554557174444199\n",
      "Epoch 20, train loss: 0.49702179431915283, val loss: 0.048109475523233414\n",
      "Epoch 30, train loss: 0.39051833748817444, val loss: 0.020974503830075264\n",
      "Epoch 40, train loss: 0.2347029447555542, val loss: 0.003396236337721348\n",
      "Epoch 50, train loss: 0.030217120423913002, val loss: 0.10243111848831177\n",
      "Epoch 60, train loss: 0.031189709901809692, val loss: 0.2240242063999176\n",
      "Epoch 70, train loss: 0.01816440559923649, val loss: 0.10312456637620926\n",
      "Epoch 80, train loss: 0.01255954708904028, val loss: 0.1411331743001938\n",
      "Epoch 90, train loss: 0.012952991761267185, val loss: 0.16285695135593414\n",
      "Training time: 69.35132455825806\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6950638294219971, val loss: 0.08703751862049103\n",
      "Epoch 10, train loss: 0.2126741111278534, val loss: 0.034230031073093414\n",
      "Epoch 20, train loss: 0.020171644166111946, val loss: 0.29674503207206726\n",
      "Epoch 30, train loss: 0.0198660921305418, val loss: 0.23876824975013733\n",
      "Epoch 40, train loss: 0.020286723971366882, val loss: 0.21394982933998108\n",
      "Epoch 50, train loss: 0.019731789827346802, val loss: 0.21998266875743866\n",
      "Epoch 60, train loss: 0.019668297842144966, val loss: 0.23057930171489716\n",
      "Epoch 70, train loss: 0.019682569429278374, val loss: 0.23480278253555298\n",
      "Epoch 80, train loss: 0.019668692722916603, val loss: 0.23484882712364197\n",
      "Epoch 90, train loss: 0.01966080069541931, val loss: 0.23392444849014282\n",
      "Epoch 100, train loss: 0.019658459350466728, val loss: 0.23318974673748016\n",
      "Epoch 110, train loss: 0.019657695665955544, val loss: 0.23275808990001678\n",
      "Epoch 120, train loss: 0.01965724304318428, val loss: 0.23251304030418396\n",
      "Epoch 130, train loss: 0.01965680904686451, val loss: 0.23235845565795898\n",
      "Epoch 140, train loss: 0.01965634524822235, val loss: 0.23224711418151855\n",
      "Epoch 150, train loss: 0.01965581625699997, val loss: 0.23216429352760315\n",
      "Epoch 160, train loss: 0.019655195996165276, val loss: 0.2321094125509262\n",
      "Epoch 170, train loss: 0.019654452800750732, val loss: 0.23208282887935638\n",
      "Epoch 180, train loss: 0.019653525203466415, val loss: 0.23207665979862213\n",
      "Epoch 190, train loss: 0.019652321934700012, val loss: 0.23207534849643707\n",
      "Training time: 138.70660781860352\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7796436548233032, val loss: 0.12363103032112122\n",
      "Epoch 10, train loss: 0.12532401084899902, val loss: 0.3667890131473541\n",
      "Epoch 20, train loss: 0.029627198353409767, val loss: 0.14349465072155\n",
      "Epoch 30, train loss: 0.016666674986481667, val loss: 0.1841021031141281\n",
      "Epoch 40, train loss: 0.015930794179439545, val loss: 0.19470813870429993\n",
      "Epoch 50, train loss: 0.01516635436564684, val loss: 0.16406892240047455\n",
      "Epoch 60, train loss: 0.014025390148162842, val loss: 0.17662844061851501\n",
      "Epoch 70, train loss: 0.011233803816139698, val loss: 0.1275772899389267\n",
      "Epoch 80, train loss: 0.0036713206209242344, val loss: 0.051242243498563766\n",
      "Epoch 90, train loss: 0.0017031661700457335, val loss: 0.00785231962800026\n",
      "Epoch 100, train loss: 0.0008050742908380926, val loss: 0.022976789623498917\n",
      "Epoch 110, train loss: 0.00026853225426748395, val loss: 0.008245466277003288\n",
      "Epoch 120, train loss: 0.00022728116891812533, val loss: 0.00745671521872282\n",
      "Epoch 130, train loss: 0.00020036443311255425, val loss: 0.006967911962419748\n",
      "Epoch 140, train loss: 0.00016959862841758877, val loss: 0.003814296331256628\n",
      "Epoch 150, train loss: 0.00015119997260626405, val loss: 0.0038141857367008924\n",
      "Epoch 160, train loss: 0.00014237346476875246, val loss: 0.0025066074449568987\n",
      "Epoch 170, train loss: 0.00013845838839188218, val loss: 0.002263748086988926\n",
      "Epoch 180, train loss: 0.0001367249060422182, val loss: 0.0018297294154763222\n",
      "Epoch 190, train loss: 0.00013593370385933667, val loss: 0.0016872200649231672\n",
      "Training time: 145.48554110527039\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6981221437454224, val loss: 0.1205485463142395\n",
      "Epoch 10, train loss: 0.6146945953369141, val loss: 0.08836279064416885\n",
      "Epoch 20, train loss: 0.5290200710296631, val loss: 0.058686722069978714\n",
      "Epoch 30, train loss: 0.4258534908294678, val loss: 0.029199322685599327\n",
      "Epoch 40, train loss: 0.28258880972862244, val loss: 0.005013218615204096\n",
      "Epoch 50, train loss: 0.08220323175191879, val loss: 0.046109192073345184\n",
      "Epoch 60, train loss: 0.043696269392967224, val loss: 0.3026455342769623\n",
      "Epoch 70, train loss: 0.014401235617697239, val loss: 0.13283176720142365\n",
      "Epoch 80, train loss: 0.01681017316877842, val loss: 0.1292707473039627\n",
      "Epoch 90, train loss: 0.013984700664877892, val loss: 0.18114669620990753\n",
      "Epoch 100, train loss: 0.013059376738965511, val loss: 0.15787087380886078\n",
      "Epoch 110, train loss: 0.012977253645658493, val loss: 0.14742296934127808\n",
      "Epoch 120, train loss: 0.012582473456859589, val loss: 0.1565464287996292\n",
      "Epoch 130, train loss: 0.012267844751477242, val loss: 0.14862848818302155\n",
      "Epoch 140, train loss: 0.011970994994044304, val loss: 0.14471523463726044\n",
      "Epoch 150, train loss: 0.011640758253633976, val loss: 0.1432366818189621\n",
      "Epoch 160, train loss: 0.011279735714197159, val loss: 0.1378529667854309\n",
      "Epoch 170, train loss: 0.010885690338909626, val loss: 0.13424843549728394\n",
      "Epoch 180, train loss: 0.010450880043208599, val loss: 0.12946940958499908\n",
      "Epoch 190, train loss: 0.009966366924345493, val loss: 0.12424228340387344\n",
      "Training time: 143.5083270072937\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.8164035081863403, val loss: 0.02970469370484352\n",
      "Epoch 10, train loss: 0.058326683938503265, val loss: 0.0656663253903389\n",
      "Epoch 20, train loss: 0.012603412382304668, val loss: 0.10504083335399628\n",
      "Epoch 30, train loss: 0.014949684962630272, val loss: 0.16546960175037384\n",
      "Epoch 40, train loss: 0.008988525718450546, val loss: 0.09370021522045135\n",
      "Epoch 50, train loss: 0.006094838492572308, val loss: 0.047230418771505356\n",
      "Epoch 60, train loss: 0.001675612642429769, val loss: 0.004738166928291321\n",
      "Epoch 70, train loss: 0.0007740998407825828, val loss: 0.01634625531733036\n",
      "Epoch 80, train loss: 0.0001466156099922955, val loss: 0.0006154669681563973\n",
      "Epoch 90, train loss: 8.233985863626003e-05, val loss: 0.0004237713001202792\n",
      "Training time: 10.453169345855713\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5310846567153931, val loss: 0.0428934320807457\n",
      "Epoch 10, train loss: 0.157095804810524, val loss: 0.017389677464962006\n",
      "Epoch 20, train loss: 0.02294488623738289, val loss: 0.289490669965744\n",
      "Epoch 30, train loss: 0.020061485469341278, val loss: 0.2139931470155716\n",
      "Epoch 40, train loss: 0.018641071394085884, val loss: 0.12101941555738449\n",
      "Epoch 50, train loss: 0.013880805112421513, val loss: 0.156753808259964\n",
      "Epoch 60, train loss: 0.013740872964262962, val loss: 0.16892032325267792\n",
      "Epoch 70, train loss: 0.012321142479777336, val loss: 0.13411065936088562\n",
      "Epoch 80, train loss: 0.011334062553942204, val loss: 0.1296948343515396\n",
      "Epoch 90, train loss: 0.010306558571755886, val loss: 0.11989106982946396\n",
      "Training time: 10.350775480270386\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.8429951071739197, val loss: 0.18199364840984344\n",
      "Epoch 10, train loss: 0.7911103963851929, val loss: 0.16155730187892914\n",
      "Epoch 20, train loss: 0.7417518496513367, val loss: 0.14263513684272766\n",
      "Epoch 30, train loss: 0.694572925567627, val loss: 0.12506888806819916\n",
      "Epoch 40, train loss: 0.6490016579627991, val loss: 0.1086922213435173\n",
      "Epoch 50, train loss: 0.6044899821281433, val loss: 0.09334122389554977\n",
      "Epoch 60, train loss: 0.5605624318122864, val loss: 0.0788901299238205\n",
      "Epoch 70, train loss: 0.5167927145957947, val loss: 0.06526383757591248\n",
      "Epoch 80, train loss: 0.47279301285743713, val loss: 0.052441731095314026\n",
      "Epoch 90, train loss: 0.4282369017601013, val loss: 0.04047306999564171\n",
      "Training time: 10.358344078063965\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.607987105846405, val loss: 0.003677716013044119\n",
      "Epoch 10, train loss: 0.027762455865740776, val loss: 0.13424476981163025\n",
      "Epoch 20, train loss: 0.020344886928796768, val loss: 0.07883655279874802\n",
      "Epoch 30, train loss: 0.012142345309257507, val loss: 0.12471569329500198\n",
      "Epoch 40, train loss: 0.007578710559755564, val loss: 0.09343614429235458\n",
      "Epoch 50, train loss: 0.0034832812380045652, val loss: 0.03699899837374687\n",
      "Epoch 60, train loss: 0.00047220298438332975, val loss: 0.0007279359269887209\n",
      "Epoch 70, train loss: 0.0005649356171488762, val loss: 0.006631346885114908\n",
      "Epoch 80, train loss: 0.00010691631905501708, val loss: 0.0003165525558870286\n",
      "Epoch 90, train loss: 9.257433703169227e-05, val loss: 0.0002741386415436864\n",
      "Epoch 100, train loss: 5.9158661315450445e-05, val loss: 0.001127334195189178\n",
      "Epoch 110, train loss: 5.2424500609049574e-05, val loss: 0.0005427167052403092\n",
      "Epoch 120, train loss: 5.004879494663328e-05, val loss: 0.0003500636084936559\n",
      "Epoch 130, train loss: 4.681079008150846e-05, val loss: 0.000503268267493695\n",
      "Epoch 140, train loss: 4.5486212911782786e-05, val loss: 0.0004714351089205593\n",
      "Epoch 150, train loss: 4.4317384890746325e-05, val loss: 0.0003883629397023469\n",
      "Epoch 160, train loss: 4.317314233048819e-05, val loss: 0.00039600240415893495\n",
      "Epoch 170, train loss: 4.218224421492778e-05, val loss: 0.00037980821798555553\n",
      "Epoch 180, train loss: 4.124324914300814e-05, val loss: 0.0003516759315971285\n",
      "Epoch 190, train loss: 4.035994061268866e-05, val loss: 0.0003409106866456568\n",
      "Training time: 21.12241792678833\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9838512539863586, val loss: 0.22863614559173584\n",
      "Epoch 10, train loss: 0.40458062291145325, val loss: 0.029420383274555206\n",
      "Epoch 20, train loss: 0.02599898725748062, val loss: 0.08171781152486801\n",
      "Epoch 30, train loss: 0.04765695333480835, val loss: 0.24493442475795746\n",
      "Epoch 40, train loss: 0.011043542996048927, val loss: 0.08999871462583542\n",
      "Epoch 50, train loss: 0.013832944445312023, val loss: 0.08257431536912918\n",
      "Epoch 60, train loss: 0.009774651378393173, val loss: 0.12244797497987747\n",
      "Epoch 70, train loss: 0.009125715121626854, val loss: 0.10780612379312515\n",
      "Epoch 80, train loss: 0.008568811230361462, val loss: 0.08831518143415451\n",
      "Epoch 90, train loss: 0.007728253956884146, val loss: 0.09037327021360397\n",
      "Epoch 100, train loss: 0.006975614000111818, val loss: 0.07865764200687408\n",
      "Epoch 110, train loss: 0.006103125866502523, val loss: 0.06748221814632416\n",
      "Epoch 120, train loss: 0.005051721353083849, val loss: 0.05558524280786514\n",
      "Epoch 130, train loss: 0.003773617558181286, val loss: 0.040272846817970276\n",
      "Epoch 140, train loss: 0.002261489862576127, val loss: 0.022752221673727036\n",
      "Epoch 150, train loss: 0.0007388726226054132, val loss: 0.006800173781812191\n",
      "Epoch 160, train loss: 6.866246258141473e-05, val loss: 0.00038082757964730263\n",
      "Epoch 170, train loss: 0.00016869472165126354, val loss: 0.0006321771652437747\n",
      "Epoch 180, train loss: 6.76131239742972e-05, val loss: 0.0004991897731088102\n",
      "Epoch 190, train loss: 7.844408537494019e-05, val loss: 0.0007486813701689243\n",
      "Training time: 20.977258443832397\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.45527350902557373, val loss: 0.03310840576887131\n",
      "Epoch 10, train loss: 0.4187729060649872, val loss: 0.024850761517882347\n",
      "Epoch 20, train loss: 0.3837932348251343, val loss: 0.017927346751093864\n",
      "Epoch 30, train loss: 0.3500480055809021, val loss: 0.01230006292462349\n",
      "Epoch 40, train loss: 0.3172304332256317, val loss: 0.007996565662324429\n",
      "Epoch 50, train loss: 0.285157173871994, val loss: 0.005099571775645018\n",
      "Epoch 60, train loss: 0.25374138355255127, val loss: 0.0037389197386801243\n",
      "Epoch 70, train loss: 0.22296157479286194, val loss: 0.004097988363355398\n",
      "Epoch 80, train loss: 0.19286862015724182, val loss: 0.006418384611606598\n",
      "Epoch 90, train loss: 0.16361993551254272, val loss: 0.011004914529621601\n",
      "Epoch 100, train loss: 0.1355171948671341, val loss: 0.01821901462972164\n",
      "Epoch 110, train loss: 0.10902733355760574, val loss: 0.028445156291127205\n",
      "Epoch 120, train loss: 0.08477731794118881, val loss: 0.04201405495405197\n",
      "Epoch 130, train loss: 0.06350814551115036, val loss: 0.05906425788998604\n",
      "Epoch 140, train loss: 0.04596110060811043, val loss: 0.07933280616998672\n",
      "Epoch 150, train loss: 0.0326789952814579, val loss: 0.10190460085868835\n",
      "Epoch 160, train loss: 0.023756185546517372, val loss: 0.12503932416439056\n",
      "Epoch 170, train loss: 0.018660999834537506, val loss: 0.14630840718746185\n",
      "Epoch 180, train loss: 0.016314079985022545, val loss: 0.1632513850927353\n",
      "Epoch 190, train loss: 0.015481114387512207, val loss: 0.17437797784805298\n",
      "Training time: 21.01529860496521\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7849542498588562, val loss: 0.0076058548875153065\n",
      "Epoch 10, train loss: 0.03676217794418335, val loss: 0.2466125786304474\n",
      "Epoch 20, train loss: 0.021076658740639687, val loss: 0.21424399316310883\n",
      "Epoch 30, train loss: 0.012379840016365051, val loss: 0.14795789122581482\n",
      "Epoch 40, train loss: 0.004584831651300192, val loss: 0.017046431079506874\n",
      "Epoch 50, train loss: 0.00036413170164451003, val loss: 0.006034356541931629\n",
      "Epoch 60, train loss: 0.000673913222271949, val loss: 0.0009443077142350376\n",
      "Epoch 70, train loss: 0.00030861955019645393, val loss: 0.006224467419087887\n",
      "Epoch 80, train loss: 0.00017763541836757213, val loss: 0.0005823984392918646\n",
      "Epoch 90, train loss: 0.00012273271568119526, val loss: 0.002783996518701315\n",
      "Training time: 15.79788088798523\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3312391936779022, val loss: 0.008137528784573078\n",
      "Epoch 10, train loss: 0.03449677303433418, val loss: 0.10775034129619598\n",
      "Epoch 20, train loss: 0.03276398777961731, val loss: 0.25429999828338623\n",
      "Epoch 30, train loss: 0.01796499453485012, val loss: 0.11302496492862701\n",
      "Epoch 40, train loss: 0.012791713699698448, val loss: 0.14600764214992523\n",
      "Epoch 50, train loss: 0.012142617255449295, val loss: 0.14551815390586853\n",
      "Epoch 60, train loss: 0.010073952376842499, val loss: 0.10071427375078201\n",
      "Epoch 70, train loss: 0.007690539583563805, val loss: 0.08861352503299713\n",
      "Epoch 80, train loss: 0.004990092013031244, val loss: 0.04775943607091904\n",
      "Epoch 90, train loss: 0.0020217716228216887, val loss: 0.016651267185807228\n",
      "Training time: 15.526228666305542\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5330631732940674, val loss: 0.05631958693265915\n",
      "Epoch 10, train loss: 0.46748265624046326, val loss: 0.03838600590825081\n",
      "Epoch 20, train loss: 0.4051315188407898, val loss: 0.023929359391331673\n",
      "Epoch 30, train loss: 0.34537243843078613, val loss: 0.013027569279074669\n",
      "Epoch 40, train loss: 0.2878637909889221, val loss: 0.006004821974784136\n",
      "Epoch 50, train loss: 0.23266851902008057, val loss: 0.0034556020982563496\n",
      "Epoch 60, train loss: 0.18035806715488434, val loss: 0.0062538632191717625\n",
      "Epoch 70, train loss: 0.13217006623744965, val loss: 0.015486284159123898\n",
      "Epoch 80, train loss: 0.09006255865097046, val loss: 0.03214661777019501\n",
      "Epoch 90, train loss: 0.0563887357711792, val loss: 0.056446902453899384\n",
      "Training time: 15.516666889190674\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7654999494552612, val loss: 0.0040472690016031265\n",
      "Epoch 10, train loss: 0.0172040406614542, val loss: 0.2946379482746124\n",
      "Epoch 20, train loss: 0.01961028203368187, val loss: 0.16444340348243713\n",
      "Epoch 30, train loss: 0.016161799430847168, val loss: 0.12644250690937042\n",
      "Epoch 40, train loss: 0.012276725843548775, val loss: 0.13585464656352997\n",
      "Epoch 50, train loss: 0.008168073371052742, val loss: 0.08279500901699066\n",
      "Epoch 60, train loss: 0.0006238745409063995, val loss: 0.0034400750882923603\n",
      "Epoch 70, train loss: 0.0001495473989052698, val loss: 0.0013329825596883893\n",
      "Epoch 80, train loss: 0.0004009817203041166, val loss: 0.0005126469186507165\n",
      "Epoch 90, train loss: 0.00017168269550893456, val loss: 0.005424864590167999\n",
      "Epoch 100, train loss: 8.877733489498496e-05, val loss: 0.0009111349936574697\n",
      "Epoch 110, train loss: 8.204768528230488e-05, val loss: 0.001140966429375112\n",
      "Epoch 120, train loss: 7.890112465247512e-05, val loss: 0.0017180816503241658\n",
      "Epoch 130, train loss: 7.284837920451537e-05, val loss: 0.000994510599412024\n",
      "Epoch 140, train loss: 6.849751662230119e-05, val loss: 0.0012624285882338881\n",
      "Epoch 150, train loss: 6.534005660796538e-05, val loss: 0.0010055425809696317\n",
      "Epoch 160, train loss: 6.263860268518329e-05, val loss: 0.0009536428842693567\n",
      "Epoch 170, train loss: 6.008529817336239e-05, val loss: 0.0008927462040446699\n",
      "Epoch 180, train loss: 5.767065522377379e-05, val loss: 0.0007947618141770363\n",
      "Epoch 190, train loss: 5.539766789297573e-05, val loss: 0.0007467737304978073\n",
      "Training time: 31.235556840896606\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.532608687877655, val loss: 0.03817924112081528\n",
      "Epoch 10, train loss: 0.04901454225182533, val loss: 0.11334027349948883\n",
      "Epoch 20, train loss: 0.0450279526412487, val loss: 0.31898924708366394\n",
      "Epoch 30, train loss: 0.02181386947631836, val loss: 0.1256437450647354\n",
      "Epoch 40, train loss: 0.015608277171850204, val loss: 0.16718065738677979\n",
      "Epoch 50, train loss: 0.015472671948373318, val loss: 0.18962326645851135\n",
      "Epoch 60, train loss: 0.01342600118368864, val loss: 0.14066772162914276\n",
      "Epoch 70, train loss: 0.011938576586544514, val loss: 0.14041829109191895\n",
      "Epoch 80, train loss: 0.01046953909099102, val loss: 0.11997062712907791\n",
      "Epoch 90, train loss: 0.008583921939134598, val loss: 0.09452059864997864\n",
      "Epoch 100, train loss: 0.0061520799063146114, val loss: 0.06528306752443314\n",
      "Epoch 110, train loss: 0.0031598282512277365, val loss: 0.030289430171251297\n",
      "Epoch 120, train loss: 0.0004944909596815705, val loss: 0.00295616895891726\n",
      "Epoch 130, train loss: 0.0002993865346070379, val loss: 0.0026127512101083994\n",
      "Epoch 140, train loss: 0.00014790847490075976, val loss: 0.0007720912108197808\n",
      "Epoch 150, train loss: 0.0001159703460871242, val loss: 0.0007251864299178123\n",
      "Epoch 160, train loss: 0.00010562271199887618, val loss: 0.0005739857442677021\n",
      "Epoch 170, train loss: 9.406539902556688e-05, val loss: 0.00048685618094168603\n",
      "Epoch 180, train loss: 9.404479351360351e-05, val loss: 0.00048184243496507406\n",
      "Epoch 190, train loss: 9.183628571918234e-05, val loss: 0.00048450540634803474\n",
      "Training time: 31.551695823669434\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.6596416234970093, val loss: 0.10292563587427139\n",
      "Epoch 10, train loss: 0.5817705988883972, val loss: 0.07603848725557327\n",
      "Epoch 20, train loss: 0.5072930455207825, val loss: 0.05284493789076805\n",
      "Epoch 30, train loss: 0.4356078505516052, val loss: 0.03345891833305359\n",
      "Epoch 40, train loss: 0.3663356602191925, val loss: 0.018248070031404495\n",
      "Epoch 50, train loss: 0.29954323172569275, val loss: 0.00790403876453638\n",
      "Epoch 60, train loss: 0.23586320877075195, val loss: 0.0034413703251630068\n",
      "Epoch 70, train loss: 0.17664848268032074, val loss: 0.0061456807889044285\n",
      "Epoch 80, train loss: 0.12398800998926163, val loss: 0.017325129359960556\n",
      "Epoch 90, train loss: 0.08041263371706009, val loss: 0.037723831832408905\n",
      "Epoch 100, train loss: 0.04814913496375084, val loss: 0.0665443167090416\n",
      "Epoch 110, train loss: 0.027959058061242104, val loss: 0.10038986057043076\n",
      "Epoch 120, train loss: 0.01815985143184662, val loss: 0.13317139446735382\n",
      "Epoch 130, train loss: 0.01497186254709959, val loss: 0.15825892984867096\n",
      "Epoch 140, train loss: 0.014491171576082706, val loss: 0.17216920852661133\n",
      "Epoch 150, train loss: 0.014499322511255741, val loss: 0.1762697547674179\n",
      "Epoch 160, train loss: 0.014426294714212418, val loss: 0.17471641302108765\n",
      "Epoch 170, train loss: 0.014318129979074001, val loss: 0.17135675251483917\n",
      "Epoch 180, train loss: 0.014230768196284771, val loss: 0.16833561658859253\n",
      "Epoch 190, train loss: 0.014158145524561405, val loss: 0.16629938781261444\n",
      "Training time: 30.50212788581848\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.49117451906204224, val loss: 0.03060847520828247\n",
      "Epoch 10, train loss: 0.013323495164513588, val loss: 0.2456142008304596\n",
      "Epoch 20, train loss: 0.018482811748981476, val loss: 0.10412940382957458\n",
      "Epoch 30, train loss: 0.010186159051954746, val loss: 0.07556939870119095\n",
      "Epoch 40, train loss: 0.006144695449620485, val loss: 0.059494391083717346\n",
      "Epoch 50, train loss: 0.0012225231621414423, val loss: 0.0029779679607599974\n",
      "Epoch 60, train loss: 0.0004973479663021863, val loss: 0.00753035256639123\n",
      "Epoch 70, train loss: 0.00021757619106210768, val loss: 0.0005172217497602105\n",
      "Epoch 80, train loss: 6.998779281275347e-05, val loss: 0.0005594301619566977\n",
      "Epoch 90, train loss: 9.326137660536915e-05, val loss: 0.0017818462802097201\n",
      "Training time: 17.246434450149536\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6431154012680054, val loss: 0.06855293363332748\n",
      "Epoch 10, train loss: 0.014206760562956333, val loss: 0.12576405704021454\n",
      "Epoch 20, train loss: 0.012418977916240692, val loss: 0.11843237280845642\n",
      "Epoch 30, train loss: 0.018754063174128532, val loss: 0.06446214765310287\n",
      "Epoch 40, train loss: 0.010255423374474049, val loss: 0.12249777466058731\n",
      "Epoch 50, train loss: 0.007469587028026581, val loss: 0.08049577474594116\n",
      "Epoch 60, train loss: 0.0065734293311834335, val loss: 0.07407689094543457\n",
      "Epoch 70, train loss: 0.005492698401212692, val loss: 0.06441452354192734\n",
      "Epoch 80, train loss: 0.004210032522678375, val loss: 0.0460873618721962\n",
      "Epoch 90, train loss: 0.0028008923400193453, val loss: 0.030812300741672516\n",
      "Training time: 17.130353212356567\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.45358461141586304, val loss: 0.036580149084329605\n",
      "Epoch 10, train loss: 0.38056161999702454, val loss: 0.02069588378071785\n",
      "Epoch 20, train loss: 0.3128153681755066, val loss: 0.009763652458786964\n",
      "Epoch 30, train loss: 0.24946410953998566, val loss: 0.0040078009478747845\n",
      "Epoch 40, train loss: 0.1896781176328659, val loss: 0.004182187374681234\n",
      "Epoch 50, train loss: 0.1338922083377838, val loss: 0.011919132433831692\n",
      "Epoch 60, train loss: 0.08436588943004608, val loss: 0.029604116454720497\n",
      "Epoch 70, train loss: 0.04528259113430977, val loss: 0.05928237363696098\n",
      "Epoch 80, train loss: 0.02127230539917946, val loss: 0.09939122200012207\n",
      "Epoch 90, train loss: 0.012885130941867828, val loss: 0.13932400941848755\n",
      "Training time: 17.20860242843628\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5991922616958618, val loss: 0.03405296429991722\n",
      "Epoch 10, train loss: 0.04344184324145317, val loss: 0.3155246675014496\n",
      "Epoch 20, train loss: 0.011621966026723385, val loss: 0.17123641073703766\n",
      "Epoch 30, train loss: 0.010758957825601101, val loss: 0.07307054847478867\n",
      "Epoch 40, train loss: 0.0051961736753582954, val loss: 0.021750984713435173\n",
      "Epoch 50, train loss: 0.0017884126864373684, val loss: 0.010966194793581963\n",
      "Epoch 60, train loss: 0.00029864866519346833, val loss: 0.0010784923797473311\n",
      "Epoch 70, train loss: 0.0001452820433769375, val loss: 0.0006905083428137004\n",
      "Epoch 80, train loss: 8.698519377503544e-05, val loss: 0.0014520930126309395\n",
      "Epoch 90, train loss: 8.64601242938079e-05, val loss: 0.00046055560233071446\n",
      "Epoch 100, train loss: 6.253657193155959e-05, val loss: 0.0010018345201388001\n",
      "Epoch 110, train loss: 5.855317067471333e-05, val loss: 0.0006234098691493273\n",
      "Epoch 120, train loss: 5.519496335182339e-05, val loss: 0.0005739105981774628\n",
      "Epoch 130, train loss: 5.361471266951412e-05, val loss: 0.0006178694311529398\n",
      "Epoch 140, train loss: 5.178653009352274e-05, val loss: 0.000509087520185858\n",
      "Epoch 150, train loss: 5.016570139559917e-05, val loss: 0.000509308825712651\n",
      "Epoch 160, train loss: 4.8734429583419114e-05, val loss: 0.0004526358097791672\n",
      "Epoch 170, train loss: 4.7425412049051374e-05, val loss: 0.00042759525240398943\n",
      "Epoch 180, train loss: 4.621269181370735e-05, val loss: 0.0003965390787925571\n",
      "Epoch 190, train loss: 4.508674828684889e-05, val loss: 0.00037121676723472774\n",
      "Training time: 34.520867586135864\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6941971778869629, val loss: 0.09088774025440216\n",
      "Epoch 10, train loss: 0.058493662625551224, val loss: 0.07815419882535934\n",
      "Epoch 20, train loss: 0.029522491618990898, val loss: 0.21742989122867584\n",
      "Epoch 30, train loss: 0.02430330030620098, val loss: 0.0903528481721878\n",
      "Epoch 40, train loss: 0.01266344916075468, val loss: 0.1676129847764969\n",
      "Epoch 50, train loss: 0.011619679629802704, val loss: 0.13826848566532135\n",
      "Epoch 60, train loss: 0.011136721819639206, val loss: 0.11700430512428284\n",
      "Epoch 70, train loss: 0.010039940476417542, val loss: 0.12391116470098495\n",
      "Epoch 80, train loss: 0.008930550888180733, val loss: 0.10031839460134506\n",
      "Epoch 90, train loss: 0.007796797901391983, val loss: 0.09371192753314972\n",
      "Epoch 100, train loss: 0.006532913073897362, val loss: 0.07476755976676941\n",
      "Epoch 110, train loss: 0.005061989184468985, val loss: 0.058954235166311264\n",
      "Epoch 120, train loss: 0.0033337692730128765, val loss: 0.03785991296172142\n",
      "Epoch 130, train loss: 0.0014600775903090835, val loss: 0.016089830547571182\n",
      "Epoch 140, train loss: 0.00015635701129212976, val loss: 0.0017712860135361552\n",
      "Epoch 150, train loss: 0.00020214120741002262, val loss: 0.0006165938684716821\n",
      "Epoch 160, train loss: 8.651658572489396e-05, val loss: 0.0004599763487931341\n",
      "Epoch 170, train loss: 8.037402585614473e-05, val loss: 0.0011380184441804886\n",
      "Epoch 180, train loss: 7.416261360049248e-05, val loss: 0.0008966313907876611\n",
      "Epoch 190, train loss: 6.840421701781452e-05, val loss: 0.0005777446203865111\n",
      "Training time: 34.50543808937073\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.8393900990486145, val loss: 0.17158567905426025\n",
      "Epoch 10, train loss: 0.7584010362625122, val loss: 0.14066913723945618\n",
      "Epoch 20, train loss: 0.681638777256012, val loss: 0.11276371777057648\n",
      "Epoch 30, train loss: 0.6062194108963013, val loss: 0.08703124523162842\n",
      "Epoch 40, train loss: 0.5298068523406982, val loss: 0.06309867650270462\n",
      "Epoch 50, train loss: 0.4505748152732849, val loss: 0.04116828367114067\n",
      "Epoch 60, train loss: 0.36737483739852905, val loss: 0.022166939452290535\n",
      "Epoch 70, train loss: 0.28046393394470215, val loss: 0.008163612335920334\n",
      "Epoch 80, train loss: 0.19270510971546173, val loss: 0.0029689494986087084\n",
      "Epoch 90, train loss: 0.111107237637043, val loss: 0.012452609837055206\n",
      "Epoch 100, train loss: 0.04781077429652214, val loss: 0.04280247911810875\n",
      "Epoch 110, train loss: 0.015398265793919563, val loss: 0.09180968254804611\n",
      "Epoch 120, train loss: 0.010592447593808174, val loss: 0.13474830985069275\n",
      "Epoch 130, train loss: 0.011719610542058945, val loss: 0.14362424612045288\n",
      "Epoch 140, train loss: 0.010549746453762054, val loss: 0.1302710622549057\n",
      "Epoch 150, train loss: 0.010047883726656437, val loss: 0.11811822652816772\n",
      "Epoch 160, train loss: 0.00998408067971468, val loss: 0.11364827305078506\n",
      "Epoch 170, train loss: 0.009833363816142082, val loss: 0.11386850476264954\n",
      "Epoch 180, train loss: 0.009686934761703014, val loss: 0.11463182419538498\n",
      "Epoch 190, train loss: 0.009560332633554935, val loss: 0.1139170229434967\n",
      "Training time: 34.20816254615784\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7023025751113892, val loss: 0.05921187251806259\n",
      "Epoch 10, train loss: 0.06535886973142624, val loss: 0.4895422160625458\n",
      "Epoch 20, train loss: 0.018278557807207108, val loss: 0.28357160091400146\n",
      "Epoch 30, train loss: 0.021170999854803085, val loss: 0.12901102006435394\n",
      "Epoch 40, train loss: 0.011779609136283398, val loss: 0.10300451517105103\n",
      "Epoch 50, train loss: 0.002493872307240963, val loss: 0.004286574199795723\n",
      "Epoch 60, train loss: 0.0011360729113221169, val loss: 0.0024441229179501534\n",
      "Epoch 70, train loss: 0.0003365906304679811, val loss: 0.013704844750463963\n",
      "Epoch 80, train loss: 0.0002659456222318113, val loss: 0.0007640489493496716\n",
      "Epoch 90, train loss: 0.00015261612134054303, val loss: 0.004544464871287346\n",
      "Training time: 24.480579137802124\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5312726497650146, val loss: 0.02988986112177372\n",
      "Epoch 10, train loss: 0.08403602242469788, val loss: 0.42159518599510193\n",
      "Epoch 20, train loss: 0.0331864096224308, val loss: 0.09129633009433746\n",
      "Epoch 30, train loss: 0.016301576048135757, val loss: 0.21569983661174774\n",
      "Epoch 40, train loss: 0.013493206351995468, val loss: 0.14170458912849426\n",
      "Epoch 50, train loss: 0.012148160487413406, val loss: 0.14413921535015106\n",
      "Epoch 60, train loss: 0.010567918419837952, val loss: 0.12027811259031296\n",
      "Epoch 70, train loss: 0.008527364581823349, val loss: 0.09855765104293823\n",
      "Epoch 80, train loss: 0.005847023334354162, val loss: 0.058440811932086945\n",
      "Epoch 90, train loss: 0.002321955980733037, val loss: 0.018405117094516754\n",
      "Training time: 24.489308834075928\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.6111290454864502, val loss: 0.08359608054161072\n",
      "Epoch 10, train loss: 0.4800480604171753, val loss: 0.04324617236852646\n",
      "Epoch 20, train loss: 0.3641442060470581, val loss: 0.016694514080882072\n",
      "Epoch 30, train loss: 0.25871896743774414, val loss: 0.004045001231133938\n",
      "Epoch 40, train loss: 0.1643955260515213, val loss: 0.008668056689202785\n",
      "Epoch 50, train loss: 0.0868128091096878, val loss: 0.03580842912197113\n",
      "Epoch 60, train loss: 0.03533795848488808, val loss: 0.08808070421218872\n",
      "Epoch 70, train loss: 0.015462467446923256, val loss: 0.15270757675170898\n",
      "Epoch 80, train loss: 0.015084072016179562, val loss: 0.1932603120803833\n",
      "Epoch 90, train loss: 0.015203159302473068, val loss: 0.19106929004192352\n",
      "Training time: 24.536673069000244\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6610462665557861, val loss: 0.18594320118427277\n",
      "Epoch 10, train loss: 0.03787072375416756, val loss: 0.10148660838603973\n",
      "Epoch 20, train loss: 0.020780472084879875, val loss: 0.11148033291101456\n",
      "Epoch 30, train loss: 0.009191961959004402, val loss: 0.06423027068376541\n",
      "Epoch 40, train loss: 0.0029239754658192396, val loss: 0.005045659840106964\n",
      "Epoch 50, train loss: 0.0011373067973181605, val loss: 0.0017155165551230311\n",
      "Epoch 60, train loss: 0.00043396296678110957, val loss: 0.00364029873162508\n",
      "Epoch 70, train loss: 0.00019414837879594415, val loss: 0.00035438223858363926\n",
      "Epoch 80, train loss: 0.00011236906721023843, val loss: 0.0011654718546196818\n",
      "Epoch 90, train loss: 6.669305730611086e-05, val loss: 0.0003331491316203028\n",
      "Epoch 100, train loss: 5.806742046843283e-05, val loss: 0.0005415618070401251\n",
      "Epoch 110, train loss: 5.2549963584169745e-05, val loss: 0.0003566483792383224\n",
      "Epoch 120, train loss: 5.003606929676607e-05, val loss: 0.0003891153319273144\n",
      "Epoch 130, train loss: 4.749000436277129e-05, val loss: 0.00032490381272509694\n",
      "Epoch 140, train loss: 4.54994551546406e-05, val loss: 0.0003177944745402783\n",
      "Epoch 150, train loss: 4.380594691610895e-05, val loss: 0.0002965714083984494\n",
      "Epoch 160, train loss: 4.2300369386794046e-05, val loss: 0.000284018024103716\n",
      "Epoch 170, train loss: 4.095270560355857e-05, val loss: 0.00027540550217963755\n",
      "Epoch 180, train loss: 3.9712867874186486e-05, val loss: 0.00026872672606259584\n",
      "Epoch 190, train loss: 3.856422335957177e-05, val loss: 0.0002647862711455673\n",
      "Training time: 49.120370626449585\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6278958916664124, val loss: 0.05759229511022568\n",
      "Epoch 10, train loss: 0.0569627545773983, val loss: 0.396347314119339\n",
      "Epoch 20, train loss: 0.025509675964713097, val loss: 0.09494444727897644\n",
      "Epoch 30, train loss: 0.013919837772846222, val loss: 0.17459695041179657\n",
      "Epoch 40, train loss: 0.013865393586456776, val loss: 0.16450951993465424\n",
      "Epoch 50, train loss: 0.013006462715566158, val loss: 0.13046172261238098\n",
      "Epoch 60, train loss: 0.011376955546438694, val loss: 0.13866333663463593\n",
      "Epoch 70, train loss: 0.009582117199897766, val loss: 0.10461052507162094\n",
      "Epoch 80, train loss: 0.0073563833720982075, val loss: 0.08399175852537155\n",
      "Epoch 90, train loss: 0.004344764165580273, val loss: 0.04598316177725792\n",
      "Epoch 100, train loss: 0.0007686797180213034, val loss: 0.004308465402573347\n",
      "Epoch 110, train loss: 0.0005808825953863561, val loss: 0.007964718155562878\n",
      "Epoch 120, train loss: 9.30761598283425e-05, val loss: 0.0004969282890670002\n",
      "Epoch 130, train loss: 0.00014992969227023423, val loss: 0.0007162927067838609\n",
      "Epoch 140, train loss: 8.94668119144626e-05, val loss: 0.0007536376942880452\n",
      "Epoch 150, train loss: 9.00539307622239e-05, val loss: 0.0006447212654165924\n",
      "Epoch 160, train loss: 8.679518941789865e-05, val loss: 0.000463359261630103\n",
      "Epoch 170, train loss: 8.41207365738228e-05, val loss: 0.0005035582580603659\n",
      "Epoch 180, train loss: 8.376823825528845e-05, val loss: 0.0005276263109408319\n",
      "Epoch 190, train loss: 8.281472400994971e-05, val loss: 0.000483606563648209\n",
      "Training time: 49.209739685058594\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.4822312593460083, val loss: 0.0415228009223938\n",
      "Epoch 10, train loss: 0.3764324188232422, val loss: 0.01775597408413887\n",
      "Epoch 20, train loss: 0.2799651324748993, val loss: 0.005070777144283056\n",
      "Epoch 30, train loss: 0.190810427069664, val loss: 0.005711451172828674\n",
      "Epoch 40, train loss: 0.11101753264665604, val loss: 0.024773864075541496\n",
      "Epoch 50, train loss: 0.04932539910078049, val loss: 0.06887975335121155\n",
      "Epoch 60, train loss: 0.01803642325103283, val loss: 0.13535650074481964\n",
      "Epoch 70, train loss: 0.014722584746778011, val loss: 0.1898418366909027\n",
      "Epoch 80, train loss: 0.01549444254487753, val loss: 0.19337275624275208\n",
      "Epoch 90, train loss: 0.01385769248008728, val loss: 0.1706179976463318\n",
      "Epoch 100, train loss: 0.013479569926857948, val loss: 0.15523332357406616\n",
      "Epoch 110, train loss: 0.013283402658998966, val loss: 0.15187427401542664\n",
      "Epoch 120, train loss: 0.012974703684449196, val loss: 0.15325826406478882\n",
      "Epoch 130, train loss: 0.012723695486783981, val loss: 0.15286272764205933\n",
      "Epoch 140, train loss: 0.012466580606997013, val loss: 0.14947058260440826\n",
      "Epoch 150, train loss: 0.012198779731988907, val loss: 0.14522431790828705\n",
      "Epoch 160, train loss: 0.011926054023206234, val loss: 0.14163362979888916\n",
      "Epoch 170, train loss: 0.011643970385193825, val loss: 0.138525128364563\n",
      "Epoch 180, train loss: 0.01135310623794794, val loss: 0.13524305820465088\n",
      "Epoch 190, train loss: 0.011052723042666912, val loss: 0.13160422444343567\n",
      "Training time: 48.75743055343628\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7520542144775391, val loss: 0.20014838874340057\n",
      "Epoch 10, train loss: 0.015390541404485703, val loss: 0.06146736443042755\n",
      "Epoch 20, train loss: 0.005201238207519054, val loss: 0.02484777942299843\n",
      "Epoch 30, train loss: 0.0027409959584474564, val loss: 0.0010871547274291515\n",
      "Epoch 40, train loss: 0.0009010583162307739, val loss: 0.0014436168130487204\n",
      "Epoch 50, train loss: 6.534630665555596e-05, val loss: 0.0004979274235665798\n",
      "Epoch 60, train loss: 4.371493560029194e-05, val loss: 0.0002018937811953947\n",
      "Epoch 70, train loss: 2.7442210921435617e-05, val loss: 0.0003727450384758413\n",
      "Epoch 80, train loss: 3.328147795400582e-05, val loss: 0.0003259673248976469\n",
      "Epoch 90, train loss: 2.555037644924596e-05, val loss: 0.000277332728728652\n",
      "Training time: 39.25621843338013\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6288678050041199, val loss: 0.04359949380159378\n",
      "Epoch 10, train loss: 0.06909996271133423, val loss: 0.2743103802204132\n",
      "Epoch 20, train loss: 0.028995607048273087, val loss: 0.0744088888168335\n",
      "Epoch 30, train loss: 0.015693441033363342, val loss: 0.17161233723163605\n",
      "Epoch 40, train loss: 0.010927684605121613, val loss: 0.09267022460699081\n",
      "Epoch 50, train loss: 0.008582254871726036, val loss: 0.10940149426460266\n",
      "Epoch 60, train loss: 0.006900419481098652, val loss: 0.07260698825120926\n",
      "Epoch 70, train loss: 0.004986447282135487, val loss: 0.05957832932472229\n",
      "Epoch 80, train loss: 0.002628294751048088, val loss: 0.028297748416662216\n",
      "Epoch 90, train loss: 0.00042323910747654736, val loss: 0.0024540144950151443\n",
      "Training time: 39.34146523475647\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.8341693878173828, val loss: 0.17972645163536072\n",
      "Epoch 10, train loss: 0.6724253296852112, val loss: 0.11921343952417374\n",
      "Epoch 20, train loss: 0.5192810297012329, val loss: 0.06858960539102554\n",
      "Epoch 30, train loss: 0.3648655414581299, val loss: 0.027530595660209656\n",
      "Epoch 40, train loss: 0.20778341591358185, val loss: 0.00357675994746387\n",
      "Epoch 50, train loss: 0.06856653839349747, val loss: 0.020191336050629616\n",
      "Epoch 60, train loss: 0.008218917064368725, val loss: 0.10526081919670105\n",
      "Epoch 70, train loss: 0.017064346000552177, val loss: 0.14640739560127258\n",
      "Epoch 80, train loss: 0.008242721669375896, val loss: 0.09957389533519745\n",
      "Epoch 90, train loss: 0.008784913457930088, val loss: 0.0811997726559639\n",
      "Training time: 39.166961669921875\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6735754013061523, val loss: 0.3204893171787262\n",
      "Epoch 10, train loss: 0.022289389744400978, val loss: 0.23258143663406372\n",
      "Epoch 20, train loss: 0.009676240384578705, val loss: 0.07072173058986664\n",
      "Epoch 30, train loss: 0.00507330521941185, val loss: 0.009738839231431484\n",
      "Epoch 40, train loss: 0.0020098292734473944, val loss: 0.0025734829250723124\n",
      "Epoch 50, train loss: 0.0007026835228316486, val loss: 0.0010726741747930646\n",
      "Epoch 60, train loss: 0.00018907769117504358, val loss: 0.00046105816727504134\n",
      "Epoch 70, train loss: 0.00011723981151590124, val loss: 0.0003626545367296785\n",
      "Epoch 80, train loss: 6.17148180026561e-05, val loss: 0.0002097864489769563\n",
      "Epoch 90, train loss: 3.186128742527217e-05, val loss: 0.0002434928173897788\n",
      "Epoch 100, train loss: 3.3275846362812445e-05, val loss: 0.00019294829689897597\n",
      "Epoch 110, train loss: 2.8600599762285128e-05, val loss: 0.00020337464229669422\n",
      "Epoch 120, train loss: 2.7942414817516692e-05, val loss: 0.00019129333668388426\n",
      "Epoch 130, train loss: 2.724831210798584e-05, val loss: 0.00018430377531331033\n",
      "Epoch 140, train loss: 2.6566191081656143e-05, val loss: 0.00018156057922169566\n",
      "Epoch 150, train loss: 2.5995772375608794e-05, val loss: 0.00017611149814911187\n",
      "Epoch 160, train loss: 2.5514120352454484e-05, val loss: 0.00017320213373750448\n",
      "Epoch 170, train loss: 2.5071083655348048e-05, val loss: 0.00016989890718832612\n",
      "Epoch 180, train loss: 2.4672161089256406e-05, val loss: 0.00016762808081693947\n",
      "Epoch 190, train loss: 2.430924905638676e-05, val loss: 0.00016554503235965967\n",
      "Training time: 78.38272404670715\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.4630754590034485, val loss: 0.011994012631475925\n",
      "Epoch 10, train loss: 0.02701096050441265, val loss: 0.17120075225830078\n",
      "Epoch 20, train loss: 0.019359182566404343, val loss: 0.09627597033977509\n",
      "Epoch 30, train loss: 0.013380058109760284, val loss: 0.14398910105228424\n",
      "Epoch 40, train loss: 0.01030735857784748, val loss: 0.09027057886123657\n",
      "Epoch 50, train loss: 0.00786439049988985, val loss: 0.09422576427459717\n",
      "Epoch 60, train loss: 0.005702968221157789, val loss: 0.06444277614355087\n",
      "Epoch 70, train loss: 0.0035535115748643875, val loss: 0.03721443563699722\n",
      "Epoch 80, train loss: 0.0012321016984060407, val loss: 0.011503930203616619\n",
      "Epoch 90, train loss: 6.673818279523402e-05, val loss: 0.0010483479127287865\n",
      "Epoch 100, train loss: 0.0001918825291795656, val loss: 0.0015983037883415818\n",
      "Epoch 110, train loss: 7.244994048960507e-05, val loss: 0.0005170489894226193\n",
      "Epoch 120, train loss: 7.4177653004881e-05, val loss: 0.0004350296803750098\n",
      "Epoch 130, train loss: 5.650723687722348e-05, val loss: 0.0003762420092243701\n",
      "Epoch 140, train loss: 5.673782652593218e-05, val loss: 0.0003662852104753256\n",
      "Epoch 150, train loss: 5.468533709063195e-05, val loss: 0.0003258625802118331\n",
      "Epoch 160, train loss: 5.4002633987693116e-05, val loss: 0.0003248570137657225\n",
      "Epoch 170, train loss: 5.3440704505192116e-05, val loss: 0.0003313978959340602\n",
      "Epoch 180, train loss: 5.2898627473041415e-05, val loss: 0.0003249082656111568\n",
      "Epoch 190, train loss: 5.245074498816393e-05, val loss: 0.0003211331204511225\n",
      "Training time: 77.75731158256531\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.651779055595398, val loss: 0.10630100965499878\n",
      "Epoch 10, train loss: 0.4859393537044525, val loss: 0.053845133632421494\n",
      "Epoch 20, train loss: 0.33502712845802307, val loss: 0.01788569986820221\n",
      "Epoch 30, train loss: 0.19541239738464355, val loss: 0.0027926720213145018\n",
      "Epoch 40, train loss: 0.07651298493146896, val loss: 0.023472709581255913\n",
      "Epoch 50, train loss: 0.013074669986963272, val loss: 0.09868664294481277\n",
      "Epoch 60, train loss: 0.015582849271595478, val loss: 0.16498823463916779\n",
      "Epoch 70, train loss: 0.011214825324714184, val loss: 0.1336279660463333\n",
      "Epoch 80, train loss: 0.009607433341443539, val loss: 0.10264304280281067\n",
      "Epoch 90, train loss: 0.009533737786114216, val loss: 0.10008472204208374\n",
      "Epoch 100, train loss: 0.008903815411031246, val loss: 0.1072113960981369\n",
      "Epoch 110, train loss: 0.008702895604074001, val loss: 0.1071920171380043\n",
      "Epoch 120, train loss: 0.008395347744226456, val loss: 0.10089763253927231\n",
      "Epoch 130, train loss: 0.008131843991577625, val loss: 0.0962560698390007\n",
      "Epoch 140, train loss: 0.007850912399590015, val loss: 0.09397205710411072\n",
      "Epoch 150, train loss: 0.007568048313260078, val loss: 0.0911598801612854\n",
      "Epoch 160, train loss: 0.007276416290551424, val loss: 0.08733309060335159\n",
      "Epoch 170, train loss: 0.006977806333452463, val loss: 0.08360902965068817\n",
      "Epoch 180, train loss: 0.006671140436083078, val loss: 0.08010684698820114\n",
      "Epoch 190, train loss: 0.006356561556458473, val loss: 0.0763910710811615\n",
      "Training time: 78.40167284011841\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7666105031967163, val loss: 0.7770100235939026\n",
      "Epoch 10, train loss: 0.08247467130422592, val loss: 0.18447721004486084\n",
      "Epoch 20, train loss: 0.016578886657953262, val loss: 0.2733757197856903\n",
      "Epoch 30, train loss: 0.01445415336638689, val loss: 0.16439659893512726\n",
      "Epoch 40, train loss: 0.007833090610802174, val loss: 0.0668782889842987\n",
      "Epoch 50, train loss: 0.0017783980583772063, val loss: 0.05301857367157936\n",
      "Epoch 60, train loss: 0.0007779988809488714, val loss: 0.004984482191503048\n",
      "Epoch 70, train loss: 0.00038105581188574433, val loss: 0.002153214532881975\n",
      "Epoch 80, train loss: 0.00016504213272128254, val loss: 0.0010098610073328018\n",
      "Epoch 90, train loss: 9.477842831984162e-05, val loss: 0.0004089712747372687\n",
      "Training time: 62.53124260902405\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7854414582252502, val loss: 0.07217031717300415\n",
      "Epoch 10, train loss: 0.014468224719166756, val loss: 0.11985959857702255\n",
      "Epoch 20, train loss: 0.013456424698233604, val loss: 0.19659103453159332\n",
      "Epoch 30, train loss: 0.013615399599075317, val loss: 0.10430572181940079\n",
      "Epoch 40, train loss: 0.010683368891477585, val loss: 0.13319505751132965\n",
      "Epoch 50, train loss: 0.00692566717043519, val loss: 0.05412784591317177\n",
      "Epoch 60, train loss: 0.0013056333409622312, val loss: 0.005827399902045727\n",
      "Epoch 70, train loss: 0.0009831993374973536, val loss: 0.01120933797210455\n",
      "Epoch 80, train loss: 0.0001324963814113289, val loss: 0.0010657564271241426\n",
      "Epoch 90, train loss: 0.00016132228483911604, val loss: 0.0006682446110062301\n",
      "Training time: 62.32569122314453\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.0001\n",
      "Epoch 0, train loss: 0.5409153699874878, val loss: 0.058851998299360275\n",
      "Epoch 10, train loss: 0.3311508893966675, val loss: 0.010953301563858986\n",
      "Epoch 20, train loss: 0.1551993042230606, val loss: 0.009937314316630363\n",
      "Epoch 30, train loss: 0.03229992836713791, val loss: 0.08849703520536423\n",
      "Epoch 40, train loss: 0.019528668373823166, val loss: 0.2203901708126068\n",
      "Epoch 50, train loss: 0.016219453886151314, val loss: 0.18529167771339417\n",
      "Epoch 60, train loss: 0.012964574620127678, val loss: 0.13111995160579681\n",
      "Epoch 70, train loss: 0.012764099054038525, val loss: 0.13063457608222961\n",
      "Epoch 80, train loss: 0.011780987493693829, val loss: 0.1453229784965515\n",
      "Epoch 90, train loss: 0.011479619890451431, val loss: 0.1414729207754135\n",
      "Training time: 62.761154890060425\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5668568015098572, val loss: 0.7853283882141113\n",
      "Epoch 10, train loss: 0.013461727648973465, val loss: 0.10534924268722534\n",
      "Epoch 20, train loss: 0.02234167605638504, val loss: 0.08758571743965149\n",
      "Epoch 30, train loss: 0.006935004610568285, val loss: 0.04936971142888069\n",
      "Epoch 40, train loss: 0.0009154910803772509, val loss: 0.006385753396898508\n",
      "Epoch 50, train loss: 0.0002636022982187569, val loss: 0.001269316067919135\n",
      "Epoch 60, train loss: 0.0001443854416720569, val loss: 0.0018654388841241598\n",
      "Epoch 70, train loss: 9.886198677122593e-05, val loss: 0.00038969155866652727\n",
      "Epoch 80, train loss: 8.375178731512278e-05, val loss: 0.0003625268873292953\n",
      "Epoch 90, train loss: 8.059001265792176e-05, val loss: 0.00037933920975774527\n",
      "Epoch 100, train loss: 7.93680883361958e-05, val loss: 0.00033731621806509793\n",
      "Epoch 110, train loss: 7.86044547567144e-05, val loss: 0.00037366567994467914\n",
      "Epoch 120, train loss: 7.785424531903118e-05, val loss: 0.00034995886380784214\n",
      "Epoch 130, train loss: 7.711743819527328e-05, val loss: 0.000362334685632959\n",
      "Epoch 140, train loss: 7.639234536327422e-05, val loss: 0.0003511392860673368\n",
      "Epoch 150, train loss: 7.566350541310385e-05, val loss: 0.00035851902794092894\n",
      "Epoch 160, train loss: 7.493189332308248e-05, val loss: 0.0003537906159181148\n",
      "Epoch 170, train loss: 7.419810572173446e-05, val loss: 0.00035793049028143287\n",
      "Epoch 180, train loss: 7.346340862568468e-05, val loss: 0.0003562801284715533\n",
      "Epoch 190, train loss: 7.272883522091433e-05, val loss: 0.0003579762123990804\n",
      "Training time: 124.44060945510864\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6738358736038208, val loss: 0.04873698949813843\n",
      "Epoch 10, train loss: 0.022352492436766624, val loss: 0.0846029594540596\n",
      "Epoch 20, train loss: 0.023879051208496094, val loss: 0.2683234214782715\n",
      "Epoch 30, train loss: 0.018972229212522507, val loss: 0.10113415122032166\n",
      "Epoch 40, train loss: 0.01192133966833353, val loss: 0.13330285251140594\n",
      "Epoch 50, train loss: 0.0069934953935444355, val loss: 0.08443398028612137\n",
      "Epoch 60, train loss: 0.0028467236552387476, val loss: 0.020483743399381638\n",
      "Epoch 70, train loss: 0.00037991564022377133, val loss: 0.0069308653473854065\n",
      "Epoch 80, train loss: 0.00013850905816070735, val loss: 0.0009642568184062839\n",
      "Epoch 90, train loss: 0.00020193110685795546, val loss: 0.0010416923323646188\n",
      "Epoch 100, train loss: 9.315359056927264e-05, val loss: 0.0009646676480770111\n",
      "Epoch 110, train loss: 9.496873099124059e-05, val loss: 0.0009692377643659711\n",
      "Epoch 120, train loss: 8.78012870089151e-05, val loss: 0.0005002033431082964\n",
      "Epoch 130, train loss: 8.250345854321495e-05, val loss: 0.0006611034041270614\n",
      "Epoch 140, train loss: 8.217683352995664e-05, val loss: 0.0006945810164324939\n",
      "Epoch 150, train loss: 8.129298657877371e-05, val loss: 0.0005797311314381659\n",
      "Epoch 160, train loss: 8.054005593294278e-05, val loss: 0.0006212933803908527\n",
      "Epoch 170, train loss: 7.994996849447489e-05, val loss: 0.0006114939460530877\n",
      "Epoch 180, train loss: 7.935863686725497e-05, val loss: 0.0005850205197930336\n",
      "Epoch 190, train loss: 7.876557356212288e-05, val loss: 0.0005885316641069949\n",
      "Training time: 125.60134959220886\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.0001\n",
      "Epoch 0, train loss: 0.550071656703949, val loss: 0.06341861188411713\n",
      "Epoch 10, train loss: 0.3761979639530182, val loss: 0.018244300037622452\n",
      "Epoch 20, train loss: 0.2158220410346985, val loss: 0.0038150157779455185\n",
      "Epoch 30, train loss: 0.07444190233945847, val loss: 0.046672992408275604\n",
      "Epoch 40, train loss: 0.013805834576487541, val loss: 0.18572309613227844\n",
      "Epoch 50, train loss: 0.02248377539217472, val loss: 0.2275514006614685\n",
      "Epoch 60, train loss: 0.013191834092140198, val loss: 0.15112796425819397\n",
      "Epoch 70, train loss: 0.01393983792513609, val loss: 0.13247790932655334\n",
      "Epoch 80, train loss: 0.01237533986568451, val loss: 0.14738868176937103\n",
      "Epoch 90, train loss: 0.012037737295031548, val loss: 0.14977072179317474\n",
      "Epoch 100, train loss: 0.011391492560505867, val loss: 0.13591262698173523\n",
      "Epoch 110, train loss: 0.010882632806897163, val loss: 0.1278437077999115\n",
      "Epoch 120, train loss: 0.01030662190169096, val loss: 0.12422997504472733\n",
      "Epoch 130, train loss: 0.009718247689306736, val loss: 0.11701741814613342\n",
      "Epoch 140, train loss: 0.009093841537833214, val loss: 0.10856444388628006\n",
      "Epoch 150, train loss: 0.008433188311755657, val loss: 0.10110478103160858\n",
      "Epoch 160, train loss: 0.007734376005828381, val loss: 0.09277674555778503\n",
      "Epoch 170, train loss: 0.006994875147938728, val loss: 0.08367245644330978\n",
      "Epoch 180, train loss: 0.00621468760073185, val loss: 0.07437675446271896\n",
      "Epoch 190, train loss: 0.005396245047450066, val loss: 0.06449343264102936\n",
      "Training time: 125.63579678535461\n",
      "Tuning time: 3429.021542072296\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "tune_start_time = time.time()\n",
    "results = []\n",
    "best_model = None\n",
    "for model in models:\n",
    "    for h_dim in hidden_dim:\n",
    "        for n_layers in num_layers:\n",
    "            for n_epoch in num_epochs:\n",
    "                for lr in learning_rate:\n",
    "                    y_train_tune = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "                    y_val_tune = torch.from_numpy(y_val).type(torch.Tensor)\n",
    "                    print(f'Training {model.__name__} with hidden_dim={h_dim}, num_layers={n_layers}, num_epochs={n_epoch}, lr={lr}')\n",
    "                    model_instance = model(input_dim=input_dim, hidden_dim=h_dim, num_layers=n_layers, output_dim=output_dim).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimiser = torch.optim.Adam(model_instance.parameters(), lr=lr)\n",
    "                    train_loss, val_loss = train_model(model_instance, \n",
    "                                                       criterion, \n",
    "                                                       optimiser, \n",
    "                                                       x_train, \n",
    "                                                       y_train_tune, \n",
    "                                                       x_val = x_val, \n",
    "                                                       y_val = y_val_tune, \n",
    "                                                       num_epochs=n_epoch)\n",
    "                        \n",
    "                    results.append({\n",
    "                        'model': model.__name__,\n",
    "                        'hidden_dim': h_dim,\n",
    "                        'num_layers': n_layers,\n",
    "                        'num_epochs': n_epoch,\n",
    "                        'lr': lr,\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss\n",
    "                    })\n",
    "tune_time = time.time() - tune_start_time\n",
    "print(f'Tuning time: {tune_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model              GRU\n",
      "hidden_dim         128\n",
      "num_layers           2\n",
      "num_epochs         200\n",
      "lr                0.01\n",
      "mse           0.016456\n",
      "Name: 99, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check best model\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['mse'] = results_df['val_loss'].apply(np.mean)\n",
    "clean_df = results_df.drop(columns=['train_loss', 'val_loss'])\n",
    "clean_df.sort_values(by='mse', ascending=True).head()\n",
    "\n",
    "# Best model should be saved on the loop, but we forgot to do that\n",
    "# Well just train it again\n",
    "best_params = clean_df.sort_values(by='mse', ascending=True).iloc[0]\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.5482916831970215, val loss: 0.28616562485694885\n",
      "Epoch 10, train loss: 0.011488945223391056, val loss: 0.21613411605358124\n",
      "Epoch 20, train loss: 0.01593106985092163, val loss: 0.0930241197347641\n",
      "Epoch 30, train loss: 0.004130747634917498, val loss: 0.0004102076927665621\n",
      "Epoch 40, train loss: 9.787076123757288e-05, val loss: 0.00033667549723759294\n",
      "Epoch 50, train loss: 0.0006327501614578068, val loss: 0.0008724224753677845\n",
      "Epoch 60, train loss: 6.646248220931739e-05, val loss: 0.0003766904410440475\n",
      "Epoch 70, train loss: 0.00012982802581973374, val loss: 0.00023985990264918655\n",
      "Epoch 80, train loss: 7.031921268207952e-05, val loss: 0.00038487190613523126\n",
      "Epoch 90, train loss: 4.706636173068546e-05, val loss: 0.00021537365682888776\n",
      "Epoch 100, train loss: 4.0514885768061504e-05, val loss: 0.00023834257444832474\n",
      "Epoch 110, train loss: 3.712442412506789e-05, val loss: 0.00020494752970989794\n",
      "Epoch 120, train loss: 3.451237716944888e-05, val loss: 0.0002021667460212484\n",
      "Epoch 130, train loss: 3.34429059876129e-05, val loss: 0.00019766682817135006\n",
      "Epoch 140, train loss: 3.2444095268147066e-05, val loss: 0.00019375814008526504\n",
      "Epoch 150, train loss: 3.1626394047634676e-05, val loss: 0.00019239507673773915\n",
      "Epoch 160, train loss: 3.0866271117702127e-05, val loss: 0.0001903823867905885\n",
      "Epoch 170, train loss: 3.0191713449312374e-05, val loss: 0.00018929076031781733\n",
      "Epoch 180, train loss: 2.957630749733653e-05, val loss: 0.00018835403898265213\n",
      "Epoch 190, train loss: 2.9009650461375713e-05, val loss: 0.00018737080972641706\n",
      "Training time: 83.70378160476685\n"
     ]
    }
   ],
   "source": [
    "y_train_best = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "y_val_best = torch.from_numpy(y_val).type(torch.Tensor)\n",
    "y_test_best = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "best_model = GRU(input_dim=input_dim, \n",
    "                 hidden_dim=int(best_params['hidden_dim']), \n",
    "                 num_layers=int(best_params['num_layers']), \n",
    "                 output_dim=output_dim).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "best_train_loss, best_val_loss = train_model(best_model, \n",
    "                                   criterion, \n",
    "                                   optimiser, \n",
    "                                   x_train, \n",
    "                                   y_train_best, \n",
    "                                   x_val = x_val, \n",
    "                                   y_val = y_val_best, \n",
    "                                   num_epochs=best_params['num_epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the best model\n",
    "plt.plot(best_train_loss, label='train loss')\n",
    "plt.plot(best_val_loss, label='val loss')\n",
    "plt.title('Best model loss graph')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model MSE: 0.0065247248858213425\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(x_test)\n",
    "    test_loss = criterion(test_outputs, y_test_best)\n",
    "    print(f'Best model MSE: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descale the data\n",
    "y_test_descaled = scaler.inverse_transform(y_test_best)\n",
    "y_test_pred = scaler.inverse_transform(test_outputs)\n",
    "\n",
    "plt.plot(y_test_descaled, label='True price')\n",
    "plt.plot(y_test_pred, label='Predicted price')\n",
    "plt.title('Best model prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
