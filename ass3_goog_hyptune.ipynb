{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19-08-2004</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>2.490664</td>\n",
       "      <td>897427216</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20-08-2004</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>2.515820</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-08-2004</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>2.758411</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-08-2004</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>2.770615</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-08-2004</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>2.614201</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.640104</td>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Low      Open     Volume      High     Close  \\\n",
       "0  19-08-2004  2.390042  2.490664  897427216  2.591785  2.499133   \n",
       "1  20-08-2004  2.503118  2.515820  458857488  2.716817  2.697639   \n",
       "2  23-08-2004  2.716070  2.758411  366857939  2.826406  2.724787   \n",
       "3  24-08-2004  2.579581  2.770615  306396159  2.779581  2.611960   \n",
       "4  25-08-2004  2.587302  2.614201  184645512  2.689918  2.640104   \n",
       "\n",
       "   Adjusted Close  \n",
       "0        2.499133  \n",
       "1        2.697639  \n",
       "2        2.724787  \n",
       "3        2.611960  \n",
       "4        2.640104  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GOOG stock data\n",
    "\n",
    "path = 'data/stock_market_data/GOOG.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = data[['Close']].copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (3215, 19, 1)\n",
      "y_train.shape = (3215, 1)\n",
      "x_val.shape = (459, 19, 1)\n",
      "y_val.shape = (459, 1)\n",
      "x_test.shape = (918, 19, 1)\n",
      "y_test.shape = (918, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set with sliding window method\n",
    "\n",
    "def split_data(stock, lookback, val_size=0.1):\n",
    "    data_raw = stock.to_numpy()  # Convert to numpy array\n",
    "    data = []\n",
    "\n",
    "    # Create sequences of length `lookback`\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    test_set_size = int(np.round(0.2 * data.shape[0]))\n",
    "    val_set_size = int(np.round(val_size * data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (test_set_size + val_set_size)\n",
    "\n",
    "    # Split into training, validation, and test sets\n",
    "    x_train = data[:train_set_size, :-1, :]\n",
    "    y_train = data[:train_set_size, -1, :]\n",
    "    \n",
    "    x_val = data[train_set_size:train_set_size+val_set_size, :-1, :]\n",
    "    y_val = data[train_set_size:train_set_size+val_set_size, -1, :]\n",
    "    \n",
    "    x_test = data[train_set_size+val_set_size:, :-1, :]\n",
    "    y_test = data[train_set_size+val_set_size:, -1, :]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "lookback = 20 # choose sequence length\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_data(price, lookback)\n",
    "\n",
    "# Check shapes\n",
    "print('x_train.shape =', x_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('x_val.shape =', x_val.shape)\n",
    "print('y_val.shape =', y_val.shape)\n",
    "print('x_test.shape =', x_test.shape)\n",
    "print('y_test.shape =', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "def train_model(model, \n",
    "                criterion, \n",
    "                optimiser, \n",
    "                x_train, y_train,\n",
    "                x_val=None, y_val=None, \n",
    "                num_epochs = 100):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        if x_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_val)\n",
    "                val_epoch_loss = criterion(val_outputs, y_val)\n",
    "                val_loss.append(val_epoch_loss.item())\n",
    "        else:\n",
    "            val_loss.append(None)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            if x_val is not None:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}, val loss: {val_epoch_loss.item()}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time: {training_time}')\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "x_val = torch.from_numpy(x_val).type(torch.Tensor)\n",
    "\n",
    "# Hyperparameters defining\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "# Tunable hyperparameters\n",
    "models = [RNN, LSTM, GRU]\n",
    "hidden_dim = [32, 64, 128]\n",
    "num_layers = [2, 3]\n",
    "num_epochs = [100, 200]\n",
    "learning_rate = [0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.843235194683075, val loss: 0.022998029366135597\n",
      "Epoch 10, train loss: 0.037770964205265045, val loss: 0.3678644001483917\n",
      "Epoch 20, train loss: 0.01745638996362686, val loss: 0.22473447024822235\n",
      "Epoch 30, train loss: 0.016726475208997726, val loss: 0.151933491230011\n",
      "Epoch 40, train loss: 0.010581498965620995, val loss: 0.10869301855564117\n",
      "Epoch 50, train loss: 0.017108513042330742, val loss: 0.07912709563970566\n",
      "Epoch 60, train loss: 0.0058102477341890335, val loss: 0.017235904932022095\n",
      "Epoch 70, train loss: 0.0005928181344643235, val loss: 0.012719173915684223\n",
      "Epoch 80, train loss: 0.000642922124825418, val loss: 0.005425732582807541\n",
      "Epoch 90, train loss: 0.00031758748809807, val loss: 0.007941589690744877\n",
      "Training time: 6.202880144119263\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.2955033779144287, val loss: 0.003509025787934661\n",
      "Epoch 10, train loss: 0.0495731495320797, val loss: 0.3758220672607422\n",
      "Epoch 20, train loss: 0.01850910484790802, val loss: 0.1281009465456009\n",
      "Epoch 30, train loss: 0.015731757506728172, val loss: 0.16668902337551117\n",
      "Epoch 40, train loss: 0.015676358714699745, val loss: 0.18916699290275574\n",
      "Epoch 50, train loss: 0.014134960249066353, val loss: 0.13840442895889282\n",
      "Epoch 60, train loss: 0.01266617514193058, val loss: 0.15189193189144135\n",
      "Epoch 70, train loss: 0.011287248693406582, val loss: 0.12104924768209457\n",
      "Epoch 80, train loss: 0.009662045165896416, val loss: 0.1083369255065918\n",
      "Epoch 90, train loss: 0.007558696437627077, val loss: 0.07661101967096329\n",
      "Training time: 4.49944281578064\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 1.0351872444152832, val loss: 0.003150567412376404\n",
      "Epoch 10, train loss: 0.0179433673620224, val loss: 0.29840824007987976\n",
      "Epoch 20, train loss: 0.015609078109264374, val loss: 0.21289527416229248\n",
      "Epoch 30, train loss: 0.012480170466005802, val loss: 0.14133839309215546\n",
      "Epoch 40, train loss: 0.0008181131561286747, val loss: 0.06195937097072601\n",
      "Epoch 50, train loss: 0.0071064867079257965, val loss: 0.03392602875828743\n",
      "Epoch 60, train loss: 0.0019584917463362217, val loss: 0.004128343425691128\n",
      "Epoch 70, train loss: 0.0002560963621363044, val loss: 0.0010223686695098877\n",
      "Epoch 80, train loss: 0.00032505838316865265, val loss: 0.0008059308747760952\n",
      "Epoch 90, train loss: 0.00015471200458705425, val loss: 0.0038622641004621983\n",
      "Epoch 100, train loss: 0.00014615533291362226, val loss: 0.001696342253126204\n",
      "Epoch 110, train loss: 0.00011404554243199527, val loss: 0.003630775725468993\n",
      "Epoch 120, train loss: 9.86435916274786e-05, val loss: 0.002272249897941947\n",
      "Epoch 130, train loss: 8.960488776210696e-05, val loss: 0.0024480591528117657\n",
      "Epoch 140, train loss: 8.467079169349745e-05, val loss: 0.002070701215416193\n",
      "Epoch 150, train loss: 8.009418525034562e-05, val loss: 0.001985697541385889\n",
      "Epoch 160, train loss: 7.610718603245914e-05, val loss: 0.0018126103095710278\n",
      "Epoch 170, train loss: 7.257444667629898e-05, val loss: 0.0016546192346140742\n",
      "Epoch 180, train loss: 6.939855666132644e-05, val loss: 0.0015533989062532783\n",
      "Epoch 190, train loss: 6.648046110058203e-05, val loss: 0.0014213577378541231\n",
      "Training time: 9.337689876556396\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.4871601164340973, val loss: 0.02989615499973297\n",
      "Epoch 10, train loss: 0.021279212087392807, val loss: 0.18963804841041565\n",
      "Epoch 20, train loss: 0.03318730741739273, val loss: 0.27906370162963867\n",
      "Epoch 30, train loss: 0.021551992744207382, val loss: 0.12080489844083786\n",
      "Epoch 40, train loss: 0.014935249462723732, val loss: 0.1728806495666504\n",
      "Epoch 50, train loss: 0.015129570849239826, val loss: 0.18578192591667175\n",
      "Epoch 60, train loss: 0.013672135770320892, val loss: 0.14174701273441315\n",
      "Epoch 70, train loss: 0.012381434440612793, val loss: 0.14719904959201813\n",
      "Epoch 80, train loss: 0.011088402010500431, val loss: 0.12525002658367157\n",
      "Epoch 90, train loss: 0.009377414360642433, val loss: 0.10462816804647446\n",
      "Epoch 100, train loss: 0.006959465332329273, val loss: 0.07155732810497284\n",
      "Epoch 110, train loss: 0.0034799242857843637, val loss: 0.02936689369380474\n",
      "Epoch 120, train loss: 0.0001236219541169703, val loss: 0.001213600393384695\n",
      "Epoch 130, train loss: 0.0003512005787342787, val loss: 0.005145874340087175\n",
      "Epoch 140, train loss: 0.00017545277660246938, val loss: 0.00032815412851050496\n",
      "Epoch 150, train loss: 9.200710337609053e-05, val loss: 0.0006569698452949524\n",
      "Epoch 160, train loss: 9.635626338422298e-05, val loss: 0.001978222280740738\n",
      "Epoch 170, train loss: 7.807205838616937e-05, val loss: 0.0008069302421063185\n",
      "Epoch 180, train loss: 7.82723946031183e-05, val loss: 0.0008102542487904429\n",
      "Epoch 190, train loss: 7.665620069019496e-05, val loss: 0.0011149131460115314\n",
      "Training time: 6.498689889907837\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.9802570939064026, val loss: 0.2076566070318222\n",
      "Epoch 10, train loss: 0.0340745784342289, val loss: 0.29342231154441833\n",
      "Epoch 20, train loss: 0.025759445503354073, val loss: 0.29938599467277527\n",
      "Epoch 30, train loss: 0.020237213000655174, val loss: 0.2702573239803314\n",
      "Epoch 40, train loss: 0.019131558015942574, val loss: 0.24178539216518402\n",
      "Epoch 50, train loss: 0.018847858533263206, val loss: 0.22734805941581726\n",
      "Epoch 60, train loss: 0.018241887912154198, val loss: 0.21813595294952393\n",
      "Epoch 70, train loss: 0.016537442803382874, val loss: 0.19373494386672974\n",
      "Epoch 80, train loss: 0.0010949356947094202, val loss: 0.05233268067240715\n",
      "Epoch 90, train loss: 0.027228865772485733, val loss: 0.383012980222702\n",
      "Training time: 4.57608699798584\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3168622553348541, val loss: 0.0034715773072093725\n",
      "Epoch 10, train loss: 0.055029116570949554, val loss: 0.38586747646331787\n",
      "Epoch 20, train loss: 0.023383816704154015, val loss: 0.12528374791145325\n",
      "Epoch 30, train loss: 0.01595623977482319, val loss: 0.2027258723974228\n",
      "Epoch 40, train loss: 0.015234951861202717, val loss: 0.17831546068191528\n",
      "Epoch 50, train loss: 0.01386998314410448, val loss: 0.1452634483575821\n",
      "Epoch 60, train loss: 0.011059651151299477, val loss: 0.1209937334060669\n",
      "Epoch 70, train loss: 0.00536020752042532, val loss: 0.04615246132016182\n",
      "Epoch 80, train loss: 0.001968222903087735, val loss: 0.012362104840576649\n",
      "Epoch 90, train loss: 0.0005713585997000337, val loss: 0.0033303461968898773\n",
      "Training time: 4.535910606384277\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6040160059928894, val loss: 0.12398409843444824\n",
      "Epoch 10, train loss: 0.021275145933032036, val loss: 0.20481650531291962\n",
      "Epoch 20, train loss: 0.019283892586827278, val loss: 0.20687952637672424\n",
      "Epoch 30, train loss: 0.01777675934135914, val loss: 0.18801236152648926\n",
      "Epoch 40, train loss: 0.015292659401893616, val loss: 0.1438542753458023\n",
      "Epoch 50, train loss: 0.0796370878815651, val loss: 0.0037580900825560093\n",
      "Epoch 60, train loss: 0.01979636400938034, val loss: 0.16363182663917542\n",
      "Epoch 70, train loss: 0.015984920784831047, val loss: 0.16234947741031647\n",
      "Epoch 80, train loss: 0.010277030989527702, val loss: 0.1087447926402092\n",
      "Epoch 90, train loss: 0.0007054861634969711, val loss: 0.005413643084466457\n",
      "Epoch 100, train loss: 0.007888219319283962, val loss: 0.02398598939180374\n",
      "Epoch 110, train loss: 0.0023827252443879843, val loss: 0.005104790907353163\n",
      "Epoch 120, train loss: 0.0007985298871062696, val loss: 0.0004508925194386393\n",
      "Epoch 130, train loss: 0.0005152239464223385, val loss: 0.003195373807102442\n",
      "Epoch 140, train loss: 0.00021872392971999943, val loss: 0.0004694448143709451\n",
      "Epoch 150, train loss: 9.322651021648198e-05, val loss: 0.002393830567598343\n",
      "Epoch 160, train loss: 6.98588410159573e-05, val loss: 0.0011801945511251688\n",
      "Epoch 170, train loss: 6.0760339692933485e-05, val loss: 0.001976208295673132\n",
      "Epoch 180, train loss: 5.1867209549527615e-05, val loss: 0.0015827735187485814\n",
      "Epoch 190, train loss: 4.8885063733905554e-05, val loss: 0.002009779214859009\n",
      "Training time: 9.017334222793579\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8076836466789246, val loss: 0.09696891158819199\n",
      "Epoch 10, train loss: 0.05304420739412308, val loss: 0.47978776693344116\n",
      "Epoch 20, train loss: 0.018025489524006844, val loss: 0.18977029621601105\n",
      "Epoch 30, train loss: 0.026555486023426056, val loss: 0.1374712884426117\n",
      "Epoch 40, train loss: 0.019081922248005867, val loss: 0.24228979647159576\n",
      "Epoch 50, train loss: 0.016002921387553215, val loss: 0.17736512422561646\n",
      "Epoch 60, train loss: 0.015285360626876354, val loss: 0.1637902855873108\n",
      "Epoch 70, train loss: 0.013753088191151619, val loss: 0.16030216217041016\n",
      "Epoch 80, train loss: 0.01111368928104639, val loss: 0.10951850563287735\n",
      "Epoch 90, train loss: 0.005307367071509361, val loss: 0.03641191124916077\n",
      "Epoch 100, train loss: 0.002085252897813916, val loss: 0.015553954988718033\n",
      "Epoch 110, train loss: 0.0006997092277742922, val loss: 0.0027710217982530594\n",
      "Epoch 120, train loss: 0.00019795015396084636, val loss: 0.001413620077073574\n",
      "Epoch 130, train loss: 0.00018463538435753435, val loss: 0.0005801041261292994\n",
      "Epoch 140, train loss: 0.0001870294363470748, val loss: 0.00043060342431999743\n",
      "Epoch 150, train loss: 0.00017204567848239094, val loss: 0.000713591929525137\n",
      "Epoch 160, train loss: 0.0001586557482369244, val loss: 0.0004137172654736787\n",
      "Epoch 170, train loss: 0.000148817416629754, val loss: 0.0004970252048224211\n",
      "Epoch 180, train loss: 0.0001413755671819672, val loss: 0.00041199394036084414\n",
      "Epoch 190, train loss: 0.00013507115363609046, val loss: 0.00041635631350800395\n",
      "Training time: 9.145306587219238\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7778453826904297, val loss: 0.1995227336883545\n",
      "Epoch 10, train loss: 0.018185770139098167, val loss: 0.10418212413787842\n",
      "Epoch 20, train loss: 0.1558256447315216, val loss: 0.24099941551685333\n",
      "Epoch 30, train loss: 0.029612736776471138, val loss: 0.2479867935180664\n",
      "Epoch 40, train loss: 0.020537804812192917, val loss: 0.21418209373950958\n",
      "Epoch 50, train loss: 0.017659317702054977, val loss: 0.17105384171009064\n",
      "Epoch 60, train loss: 0.01293172687292099, val loss: 0.1337335854768753\n",
      "Epoch 70, train loss: 0.004951067268848419, val loss: 0.04597938805818558\n",
      "Epoch 80, train loss: 0.024957852438092232, val loss: 0.3827865719795227\n",
      "Epoch 90, train loss: 0.027155112475156784, val loss: 0.3153547942638397\n",
      "Training time: 5.502933502197266\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7569235563278198, val loss: 0.07756049185991287\n",
      "Epoch 10, train loss: 0.09565172344446182, val loss: 0.4325343370437622\n",
      "Epoch 20, train loss: 0.0356346033513546, val loss: 0.08132397383451462\n",
      "Epoch 30, train loss: 0.02027921937406063, val loss: 0.2263232320547104\n",
      "Epoch 40, train loss: 0.014453633688390255, val loss: 0.11152300238609314\n",
      "Epoch 50, train loss: 0.011317500844597816, val loss: 0.13720080256462097\n",
      "Epoch 60, train loss: 0.008048927411437035, val loss: 0.06705180555582047\n",
      "Epoch 70, train loss: 0.00173875130712986, val loss: 0.0032523430418223143\n",
      "Epoch 80, train loss: 0.0006081590545363724, val loss: 0.004832722712308168\n",
      "Epoch 90, train loss: 0.0005055502406321466, val loss: 0.0007130552548915148\n",
      "Training time: 5.486148834228516\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.8660638332366943, val loss: 0.08657649904489517\n",
      "Epoch 10, train loss: 0.02139650285243988, val loss: 0.483027845621109\n",
      "Epoch 20, train loss: 0.32441744208335876, val loss: 0.1342671662569046\n",
      "Epoch 30, train loss: 0.04897818714380264, val loss: 0.2995888888835907\n",
      "Epoch 40, train loss: 0.028740370646119118, val loss: 0.3546258807182312\n",
      "Epoch 50, train loss: 0.023946624249219894, val loss: 0.16850651800632477\n",
      "Epoch 60, train loss: 0.019073491916060448, val loss: 0.2552136778831482\n",
      "Epoch 70, train loss: 0.01905645802617073, val loss: 0.22303564846515656\n",
      "Epoch 80, train loss: 0.017074808478355408, val loss: 0.21217837929725647\n",
      "Epoch 90, train loss: 0.015079853124916553, val loss: 0.18513086438179016\n",
      "Epoch 100, train loss: 0.01769254170358181, val loss: 0.32286134362220764\n",
      "Epoch 110, train loss: 0.016816258430480957, val loss: 0.2589528262615204\n",
      "Epoch 120, train loss: 0.016332505270838737, val loss: 0.17072373628616333\n",
      "Epoch 130, train loss: 0.013506881892681122, val loss: 0.17239363491535187\n",
      "Epoch 140, train loss: 0.06324858963489532, val loss: 0.14914575219154358\n",
      "Epoch 150, train loss: 0.02627183124423027, val loss: 0.02332368865609169\n",
      "Epoch 160, train loss: 0.2279827743768692, val loss: 0.46457937359809875\n",
      "Epoch 170, train loss: 0.026141004636883736, val loss: 0.14391455054283142\n",
      "Epoch 180, train loss: 0.02020183391869068, val loss: 0.18675827980041504\n",
      "Epoch 190, train loss: 0.019692113623023033, val loss: 0.21539969742298126\n",
      "Training time: 11.022291660308838\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8588230609893799, val loss: 0.09816024452447891\n",
      "Epoch 10, train loss: 0.10060906410217285, val loss: 0.4689924120903015\n",
      "Epoch 20, train loss: 0.040409255772829056, val loss: 0.09671460837125778\n",
      "Epoch 30, train loss: 0.023183561861515045, val loss: 0.2625173330307007\n",
      "Epoch 40, train loss: 0.017324727028608322, val loss: 0.14339092373847961\n",
      "Epoch 50, train loss: 0.015038197860121727, val loss: 0.19137679040431976\n",
      "Epoch 60, train loss: 0.013905789703130722, val loss: 0.14851577579975128\n",
      "Epoch 70, train loss: 0.012819867581129074, val loss: 0.15519337356090546\n",
      "Epoch 80, train loss: 0.011533945798873901, val loss: 0.12697209417819977\n",
      "Epoch 90, train loss: 0.009803535416722298, val loss: 0.1106673926115036\n",
      "Epoch 100, train loss: 0.007263114210218191, val loss: 0.0775725468993187\n",
      "Epoch 110, train loss: 0.002803757321089506, val loss: 0.019215527921915054\n",
      "Epoch 120, train loss: 0.0012307718861848116, val loss: 0.0190249215811491\n",
      "Epoch 130, train loss: 0.00037235597847029567, val loss: 0.0013593833427876234\n",
      "Epoch 140, train loss: 8.718622848391533e-05, val loss: 0.0018378527602180839\n",
      "Epoch 150, train loss: 9.545293869450688e-05, val loss: 0.0015493982937186956\n",
      "Epoch 160, train loss: 9.862918523140252e-05, val loss: 0.0006341810803860426\n",
      "Epoch 170, train loss: 8.822062227409333e-05, val loss: 0.0016788769280537963\n",
      "Epoch 180, train loss: 8.272261766251177e-05, val loss: 0.0009065641788765788\n",
      "Epoch 190, train loss: 8.088073082035407e-05, val loss: 0.0011575075332075357\n",
      "Training time: 15.600922584533691\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.9376111626625061, val loss: 0.1269005984067917\n",
      "Epoch 10, train loss: 0.9892992973327637, val loss: 0.47503721714019775\n",
      "Epoch 20, train loss: 0.09049805253744125, val loss: 0.11219180375337601\n",
      "Epoch 30, train loss: 0.019428381696343422, val loss: 0.24609124660491943\n",
      "Epoch 40, train loss: 0.020061202347278595, val loss: 0.2410697638988495\n",
      "Epoch 50, train loss: 0.0196515005081892, val loss: 0.24452723562717438\n",
      "Epoch 60, train loss: 0.018722599372267723, val loss: 0.21300722658634186\n",
      "Epoch 70, train loss: 0.01665009930729866, val loss: 0.20082268118858337\n",
      "Epoch 80, train loss: 0.02750682644546032, val loss: 0.28319594264030457\n",
      "Epoch 90, train loss: 0.022609099745750427, val loss: 0.2528247833251953\n",
      "Training time: 8.561525821685791\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3593968451023102, val loss: 0.010322940535843372\n",
      "Epoch 10, train loss: 0.024437911808490753, val loss: 0.12151593714952469\n",
      "Epoch 20, train loss: 0.022155005484819412, val loss: 0.28418663144111633\n",
      "Epoch 30, train loss: 0.018952002748847008, val loss: 0.15894639492034912\n",
      "Epoch 40, train loss: 0.016469353809952736, val loss: 0.2065032720565796\n",
      "Epoch 50, train loss: 0.01323540136218071, val loss: 0.13308514654636383\n",
      "Epoch 60, train loss: 0.00564114935696125, val loss: 0.03269599378108978\n",
      "Epoch 70, train loss: 0.00021298331557773054, val loss: 0.0019280918641015887\n",
      "Epoch 80, train loss: 0.0004961900995112956, val loss: 0.0004540139634627849\n",
      "Epoch 90, train loss: 0.00021292314340826124, val loss: 0.00472675496712327\n",
      "Training time: 8.336411952972412\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6613442301750183, val loss: 0.33367031812667847\n",
      "Epoch 10, train loss: 0.046545904129743576, val loss: 0.6166057586669922\n",
      "Epoch 20, train loss: 0.021164514124393463, val loss: 0.18276748061180115\n",
      "Epoch 30, train loss: 0.0192494485527277, val loss: 0.26477140188217163\n",
      "Epoch 40, train loss: 0.00829588808119297, val loss: 0.07780902832746506\n",
      "Epoch 50, train loss: 0.052851200103759766, val loss: 0.17298346757888794\n",
      "Epoch 60, train loss: 0.019890306517481804, val loss: 0.34867194294929504\n",
      "Epoch 70, train loss: 0.020704589784145355, val loss: 0.2552776038646698\n",
      "Epoch 80, train loss: 0.019924214109778404, val loss: 0.23149371147155762\n",
      "Epoch 90, train loss: 0.019481198862195015, val loss: 0.22884027659893036\n",
      "Epoch 100, train loss: 0.01891331374645233, val loss: 0.22923940420150757\n",
      "Epoch 110, train loss: 0.022637147456407547, val loss: 0.22746685147285461\n",
      "Epoch 120, train loss: 0.019616473466157913, val loss: 0.17624139785766602\n",
      "Epoch 130, train loss: 0.020563703030347824, val loss: 0.2714124321937561\n",
      "Epoch 140, train loss: 0.019830066710710526, val loss: 0.2091958373785019\n",
      "Epoch 150, train loss: 0.019534219056367874, val loss: 0.24170370399951935\n",
      "Epoch 160, train loss: 0.01951506733894348, val loss: 0.22730399668216705\n",
      "Epoch 170, train loss: 0.019442882388830185, val loss: 0.2307596653699875\n",
      "Epoch 180, train loss: 0.019309839233756065, val loss: 0.2250409573316574\n",
      "Epoch 190, train loss: 0.019087975844740868, val loss: 0.22385618090629578\n",
      "Training time: 16.389635801315308\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6129209399223328, val loss: 0.0226752869784832\n",
      "Epoch 10, train loss: 0.016015075147151947, val loss: 0.1144806370139122\n",
      "Epoch 20, train loss: 0.016598286107182503, val loss: 0.23407284915447235\n",
      "Epoch 30, train loss: 0.01575746200978756, val loss: 0.1309482902288437\n",
      "Epoch 40, train loss: 0.01363979559391737, val loss: 0.17305967211723328\n",
      "Epoch 50, train loss: 0.011485273949801922, val loss: 0.10695315897464752\n",
      "Epoch 60, train loss: 0.007367302663624287, val loss: 0.07330216467380524\n",
      "Epoch 70, train loss: 0.0008430094458162785, val loss: 0.0004217168898321688\n",
      "Epoch 80, train loss: 0.0003552063135430217, val loss: 0.0032811700366437435\n",
      "Epoch 90, train loss: 0.0004546898999251425, val loss: 0.0006188294501043856\n",
      "Epoch 100, train loss: 0.0001979554072022438, val loss: 0.005761719308793545\n",
      "Epoch 110, train loss: 0.00011989623453700915, val loss: 0.0009397694375365973\n",
      "Epoch 120, train loss: 0.00011248428199905902, val loss: 0.001836656592786312\n",
      "Epoch 130, train loss: 0.0001105741030187346, val loss: 0.002097404794767499\n",
      "Epoch 140, train loss: 0.00010786326311063021, val loss: 0.00144692393951118\n",
      "Epoch 150, train loss: 0.00010491155262570828, val loss: 0.0018656100146472454\n",
      "Epoch 160, train loss: 0.00010256950918119401, val loss: 0.0014997102553024888\n",
      "Epoch 170, train loss: 0.00010063505760626867, val loss: 0.0016146558336913586\n",
      "Epoch 180, train loss: 9.88704850897193e-05, val loss: 0.001491460483521223\n",
      "Epoch 190, train loss: 9.718299406813458e-05, val loss: 0.001475707977078855\n",
      "Training time: 16.908020973205566\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.42520320415496826, val loss: 2.7919483184814453\n",
      "Epoch 10, train loss: 0.5435065627098083, val loss: 4.760836124420166\n",
      "Epoch 20, train loss: 0.23598507046699524, val loss: 0.06023241579532623\n",
      "Epoch 30, train loss: 1.0002022981643677, val loss: 2.180654525756836\n",
      "Epoch 40, train loss: 0.3576711118221283, val loss: 0.006555963307619095\n",
      "Epoch 50, train loss: 0.13122649490833282, val loss: 0.5590997338294983\n",
      "Epoch 60, train loss: 0.03686189651489258, val loss: 0.17561426758766174\n",
      "Epoch 70, train loss: 0.01970924250781536, val loss: 0.1985575407743454\n",
      "Epoch 80, train loss: 0.02196996472775936, val loss: 0.29503652453422546\n",
      "Epoch 90, train loss: 0.02149425446987152, val loss: 0.19924630224704742\n",
      "Training time: 13.619348049163818\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.61153644323349, val loss: 0.011046900413930416\n",
      "Epoch 10, train loss: 0.04696539416909218, val loss: 0.05981769785284996\n",
      "Epoch 20, train loss: 0.016752803698182106, val loss: 0.163007453083992\n",
      "Epoch 30, train loss: 0.01029165182262659, val loss: 0.13078482449054718\n",
      "Epoch 40, train loss: 0.008667496033012867, val loss: 0.0669558048248291\n",
      "Epoch 50, train loss: 0.0025714016519486904, val loss: 0.014552980661392212\n",
      "Epoch 60, train loss: 0.0009997687302529812, val loss: 0.003822770668193698\n",
      "Epoch 70, train loss: 0.00048232710105367005, val loss: 0.0014122026041150093\n",
      "Epoch 80, train loss: 0.0001838395546656102, val loss: 0.0031708190217614174\n",
      "Epoch 90, train loss: 7.362123142229393e-05, val loss: 0.0002825864066835493\n",
      "Training time: 13.546663522720337\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.542538583278656, val loss: 4.5013957023620605\n",
      "Epoch 10, train loss: 0.11649038642644882, val loss: 0.20422352850437164\n",
      "Epoch 20, train loss: 0.12632179260253906, val loss: 0.01533588208258152\n",
      "Epoch 30, train loss: 0.031688421964645386, val loss: 0.22923995554447174\n",
      "Epoch 40, train loss: 0.04034523665904999, val loss: 0.37121087312698364\n",
      "Epoch 50, train loss: 0.02131224423646927, val loss: 0.3058035969734192\n",
      "Epoch 60, train loss: 0.019765693694353104, val loss: 0.24951398372650146\n",
      "Epoch 70, train loss: 0.02003379724919796, val loss: 0.23000217974185944\n",
      "Epoch 80, train loss: 0.01987011544406414, val loss: 0.22712180018424988\n",
      "Epoch 90, train loss: 0.019726641476154327, val loss: 0.2289305329322815\n",
      "Epoch 100, train loss: 0.019661808386445045, val loss: 0.23122148215770721\n",
      "Epoch 110, train loss: 0.019639264792203903, val loss: 0.2328205704689026\n",
      "Epoch 120, train loss: 0.01963386870920658, val loss: 0.23344175517559052\n",
      "Epoch 130, train loss: 0.01963205821812153, val loss: 0.23315417766571045\n",
      "Epoch 140, train loss: 0.01962866447865963, val loss: 0.23235541582107544\n",
      "Epoch 150, train loss: 0.019624562934041023, val loss: 0.23163734376430511\n",
      "Epoch 160, train loss: 0.01962079294025898, val loss: 0.23140108585357666\n",
      "Epoch 170, train loss: 0.01961667835712433, val loss: 0.23154401779174805\n",
      "Epoch 180, train loss: 0.01961207017302513, val loss: 0.23165345191955566\n",
      "Epoch 190, train loss: 0.019606929272413254, val loss: 0.23154689371585846\n",
      "Training time: 24.95039701461792\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.46735942363739014, val loss: 0.003295404138043523\n",
      "Epoch 10, train loss: 0.04039879888296127, val loss: 0.06798781454563141\n",
      "Epoch 20, train loss: 0.013397755101323128, val loss: 0.1391267478466034\n",
      "Epoch 30, train loss: 0.010186506435275078, val loss: 0.13552676141262054\n",
      "Epoch 40, train loss: 0.007114645559340715, val loss: 0.0592159628868103\n",
      "Epoch 50, train loss: 0.0006437732954509556, val loss: 0.005001620855182409\n",
      "Epoch 60, train loss: 0.0004894016310572624, val loss: 0.005649859085679054\n",
      "Epoch 70, train loss: 0.00014439839287661016, val loss: 0.0024288196582347155\n",
      "Epoch 80, train loss: 6.379677506629378e-05, val loss: 0.000267011666437611\n",
      "Epoch 90, train loss: 6.761809345334768e-05, val loss: 0.0006865538889542222\n",
      "Epoch 100, train loss: 5.283022619551048e-05, val loss: 0.00031907152151688933\n",
      "Epoch 110, train loss: 4.807812729268335e-05, val loss: 0.0005155311082489789\n",
      "Epoch 120, train loss: 4.773501859745011e-05, val loss: 0.00038334893179126084\n",
      "Epoch 130, train loss: 4.752956738229841e-05, val loss: 0.00048112915828824043\n",
      "Epoch 140, train loss: 4.738469942822121e-05, val loss: 0.0004117059288546443\n",
      "Epoch 150, train loss: 4.718946365755983e-05, val loss: 0.0004590954340528697\n",
      "Epoch 160, train loss: 4.69851256639231e-05, val loss: 0.00042727592517621815\n",
      "Epoch 170, train loss: 4.682308281189762e-05, val loss: 0.00044018772314302623\n",
      "Epoch 180, train loss: 4.666427776101045e-05, val loss: 0.0004283216258045286\n",
      "Epoch 190, train loss: 4.6508186642313376e-05, val loss: 0.000426972983404994\n",
      "Training time: 27.220027208328247\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.48049697279930115, val loss: 6.32347297668457\n",
      "Epoch 10, train loss: 0.984054446220398, val loss: 0.10492324084043503\n",
      "Epoch 20, train loss: 0.026568055152893066, val loss: 0.1309097856283188\n",
      "Epoch 30, train loss: 0.10127405822277069, val loss: 0.47469794750213623\n",
      "Epoch 40, train loss: 0.039255306124687195, val loss: 0.3726058006286621\n",
      "Epoch 50, train loss: 0.019721126183867455, val loss: 0.26969069242477417\n",
      "Epoch 60, train loss: 0.021309932693839073, val loss: 0.20484620332717896\n",
      "Epoch 70, train loss: 0.019815992563962936, val loss: 0.21258588135242462\n",
      "Epoch 80, train loss: 0.01972358115017414, val loss: 0.23324339091777802\n",
      "Epoch 90, train loss: 0.019730327650904655, val loss: 0.23932600021362305\n",
      "Training time: 21.054072380065918\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3397417366504669, val loss: 0.10010570287704468\n",
      "Epoch 10, train loss: 0.0178145170211792, val loss: 0.283579021692276\n",
      "Epoch 20, train loss: 0.02078191377222538, val loss: 0.1529218554496765\n",
      "Epoch 30, train loss: 0.01389314979314804, val loss: 0.11897613108158112\n",
      "Epoch 40, train loss: 0.00515402015298605, val loss: 0.021670427173376083\n",
      "Epoch 50, train loss: 0.0010534980101510882, val loss: 0.005582265090197325\n",
      "Epoch 60, train loss: 7.074323366396129e-05, val loss: 0.003327789017930627\n",
      "Epoch 70, train loss: 0.00012977755977772176, val loss: 0.0004449942207429558\n",
      "Epoch 80, train loss: 8.547514880774543e-05, val loss: 0.002211357932537794\n",
      "Epoch 90, train loss: 8.149534551193938e-05, val loss: 0.0003358932735864073\n",
      "Training time: 20.679784536361694\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.552649736404419, val loss: 1.672986626625061\n",
      "Epoch 10, train loss: 0.10578332096338272, val loss: 0.16734500229358673\n",
      "Epoch 20, train loss: 0.04256102442741394, val loss: 0.21111921966075897\n",
      "Epoch 30, train loss: 0.024530626833438873, val loss: 0.24884794652462006\n",
      "Epoch 40, train loss: 0.019659817218780518, val loss: 0.28321734070777893\n",
      "Epoch 50, train loss: 0.021162066608667374, val loss: 0.28044793009757996\n",
      "Epoch 60, train loss: 0.02035670354962349, val loss: 0.23962676525115967\n",
      "Epoch 70, train loss: 0.01974434033036232, val loss: 0.21615755558013916\n",
      "Epoch 80, train loss: 0.019717905670404434, val loss: 0.2329302877187729\n",
      "Epoch 90, train loss: 0.019703075289726257, val loss: 0.23691914975643158\n",
      "Epoch 100, train loss: 0.01966569386422634, val loss: 0.22852380573749542\n",
      "Epoch 110, train loss: 0.01965971849858761, val loss: 0.2341110110282898\n",
      "Epoch 120, train loss: 0.01965952292084694, val loss: 0.23116527497768402\n",
      "Epoch 130, train loss: 0.019659511744976044, val loss: 0.23275861144065857\n",
      "Epoch 140, train loss: 0.01965950056910515, val loss: 0.23175248503684998\n",
      "Epoch 150, train loss: 0.019659506157040596, val loss: 0.23244205117225647\n",
      "Epoch 160, train loss: 0.019659502431750298, val loss: 0.23201556503772736\n",
      "Epoch 170, train loss: 0.01965947635471821, val loss: 0.2321908175945282\n",
      "Epoch 180, train loss: 0.019659454002976418, val loss: 0.23220020532608032\n",
      "Epoch 190, train loss: 0.01965944841504097, val loss: 0.23212791979312897\n",
      "Training time: 39.52774524688721\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.657234251499176, val loss: 0.009897744283080101\n",
      "Epoch 10, train loss: 0.050506770610809326, val loss: 0.0991421714425087\n",
      "Epoch 20, train loss: 0.016153773292899132, val loss: 0.12092799693346024\n",
      "Epoch 30, train loss: 0.014788909815251827, val loss: 0.15758100152015686\n",
      "Epoch 40, train loss: 0.007766765542328358, val loss: 0.07526793330907822\n",
      "Epoch 50, train loss: 0.0018003255827352405, val loss: 0.0087244538590312\n",
      "Epoch 60, train loss: 0.0007205253932625055, val loss: 0.002253922400996089\n",
      "Epoch 70, train loss: 0.0003443191817495972, val loss: 0.004122699610888958\n",
      "Epoch 80, train loss: 0.0001427569950465113, val loss: 0.0004122846876271069\n",
      "Epoch 90, train loss: 7.615137292305008e-05, val loss: 0.0013163480907678604\n",
      "Epoch 100, train loss: 6.72482856316492e-05, val loss: 0.00035456105251796544\n",
      "Epoch 110, train loss: 6.157336611067876e-05, val loss: 0.0008386840927414596\n",
      "Epoch 120, train loss: 5.8567984524415806e-05, val loss: 0.0004907495458610356\n",
      "Epoch 130, train loss: 5.714178769267164e-05, val loss: 0.0006614840240217745\n",
      "Epoch 140, train loss: 5.641596362693235e-05, val loss: 0.0005384047399275005\n",
      "Epoch 150, train loss: 5.593923560809344e-05, val loss: 0.0005942101124674082\n",
      "Epoch 160, train loss: 5.544965824810788e-05, val loss: 0.0005559882847592235\n",
      "Epoch 170, train loss: 5.501001942320727e-05, val loss: 0.0005588378990069032\n",
      "Epoch 180, train loss: 5.457902807393111e-05, val loss: 0.0005444723065011203\n",
      "Epoch 190, train loss: 5.415809937403537e-05, val loss: 0.0005343961529433727\n",
      "Training time: 39.16911220550537\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.716059684753418, val loss: 0.04525423049926758\n",
      "Epoch 10, train loss: 0.04655345901846886, val loss: 0.09308105707168579\n",
      "Epoch 20, train loss: 0.024916423484683037, val loss: 0.27600643038749695\n",
      "Epoch 30, train loss: 0.020340466871857643, val loss: 0.17922291159629822\n",
      "Epoch 40, train loss: 0.018368255347013474, val loss: 0.22428882122039795\n",
      "Epoch 50, train loss: 0.017070217058062553, val loss: 0.19164928793907166\n",
      "Epoch 60, train loss: 0.01539185643196106, val loss: 0.17240241169929504\n",
      "Epoch 70, train loss: 0.006874899845570326, val loss: 0.05704346299171448\n",
      "Epoch 80, train loss: 0.003921095747500658, val loss: 0.047217804938554764\n",
      "Epoch 90, train loss: 0.0015602491330355406, val loss: 0.006778803654015064\n",
      "Training time: 5.665086269378662\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6099509000778198, val loss: 0.08004381507635117\n",
      "Epoch 10, train loss: 0.4091615378856659, val loss: 0.021854352205991745\n",
      "Epoch 20, train loss: 0.17217540740966797, val loss: 0.012883136048913002\n",
      "Epoch 30, train loss: 0.04734648019075394, val loss: 0.3597256541252136\n",
      "Epoch 40, train loss: 0.01635150983929634, val loss: 0.141076922416687\n",
      "Epoch 50, train loss: 0.019003625959157944, val loss: 0.1411205679178238\n",
      "Epoch 60, train loss: 0.016242418438196182, val loss: 0.20559637248516083\n",
      "Epoch 70, train loss: 0.014418551698327065, val loss: 0.16355231404304504\n",
      "Epoch 80, train loss: 0.01403480488806963, val loss: 0.1584731489419937\n",
      "Epoch 90, train loss: 0.01341209840029478, val loss: 0.1635272055864334\n",
      "Training time: 5.536373853683472\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 1.0665256977081299, val loss: 0.16754166781902313\n",
      "Epoch 10, train loss: 0.02942723035812378, val loss: 0.08074130117893219\n",
      "Epoch 20, train loss: 0.026069147512316704, val loss: 0.314714252948761\n",
      "Epoch 30, train loss: 0.021005384624004364, val loss: 0.16815820336341858\n",
      "Epoch 40, train loss: 0.018991436809301376, val loss: 0.24519075453281403\n",
      "Epoch 50, train loss: 0.01830187626183033, val loss: 0.1939246505498886\n",
      "Epoch 60, train loss: 0.017729461193084717, val loss: 0.21806700527668\n",
      "Epoch 70, train loss: 0.016905169934034348, val loss: 0.19127102196216583\n",
      "Epoch 80, train loss: 0.013702571392059326, val loss: 0.16815534234046936\n",
      "Epoch 90, train loss: 0.003932088613510132, val loss: 0.023573122918605804\n",
      "Epoch 100, train loss: 0.004124800208956003, val loss: 0.018032649531960487\n",
      "Epoch 110, train loss: 0.0010125660337507725, val loss: 0.041291579604148865\n",
      "Epoch 120, train loss: 0.0011846923734992743, val loss: 0.038047175854444504\n",
      "Epoch 130, train loss: 0.0007277065888047218, val loss: 0.03057648241519928\n",
      "Epoch 140, train loss: 0.0005485651199705899, val loss: 0.026512499898672104\n",
      "Epoch 150, train loss: 0.0004601786786224693, val loss: 0.021774951368570328\n",
      "Epoch 160, train loss: 0.0003511410905048251, val loss: 0.018474169075489044\n",
      "Epoch 170, train loss: 0.0002878422965295613, val loss: 0.014484534971415997\n",
      "Epoch 180, train loss: 0.00023490629973821342, val loss: 0.01121414452791214\n",
      "Epoch 190, train loss: 0.00019483474898152053, val loss: 0.008407188579440117\n",
      "Training time: 10.889508962631226\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9185299277305603, val loss: 0.2133857160806656\n",
      "Epoch 10, train loss: 0.6105278730392456, val loss: 0.08598975837230682\n",
      "Epoch 20, train loss: 0.24462135136127472, val loss: 0.0030394699424505234\n",
      "Epoch 30, train loss: 0.046237051486968994, val loss: 0.33522823452949524\n",
      "Epoch 40, train loss: 0.01318227406591177, val loss: 0.13415606319904327\n",
      "Epoch 50, train loss: 0.0208682119846344, val loss: 0.10349295288324356\n",
      "Epoch 60, train loss: 0.013888636603951454, val loss: 0.17592334747314453\n",
      "Epoch 70, train loss: 0.013040968216955662, val loss: 0.1527991145849228\n",
      "Epoch 80, train loss: 0.01278869342058897, val loss: 0.13081659376621246\n",
      "Epoch 90, train loss: 0.011996463872492313, val loss: 0.14227281510829926\n",
      "Epoch 100, train loss: 0.01140050683170557, val loss: 0.12985485792160034\n",
      "Epoch 110, train loss: 0.010787693783640862, val loss: 0.12170970439910889\n",
      "Epoch 120, train loss: 0.01007428951561451, val loss: 0.11618126183748245\n",
      "Epoch 130, train loss: 0.009252211079001427, val loss: 0.10495977103710175\n",
      "Epoch 140, train loss: 0.008296962827444077, val loss: 0.09560582786798477\n",
      "Epoch 150, train loss: 0.007161557208746672, val loss: 0.08278994262218475\n",
      "Epoch 160, train loss: 0.005775938741862774, val loss: 0.06874002516269684\n",
      "Epoch 170, train loss: 0.004049814771860838, val loss: 0.05134769529104233\n",
      "Epoch 180, train loss: 0.002019221195951104, val loss: 0.03157301992177963\n",
      "Epoch 190, train loss: 0.0004107272543478757, val loss: 0.013994859531521797\n",
      "Training time: 10.538684368133545\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.3315967321395874, val loss: 0.011388028040528297\n",
      "Epoch 10, train loss: 0.03786856681108475, val loss: 0.14947763085365295\n",
      "Epoch 20, train loss: 0.022121001034975052, val loss: 0.25811174511909485\n",
      "Epoch 30, train loss: 0.020812418311834335, val loss: 0.20541682839393616\n",
      "Epoch 40, train loss: 0.019897084683179855, val loss: 0.2450270652770996\n",
      "Epoch 50, train loss: 0.019245179370045662, val loss: 0.21542569994926453\n",
      "Epoch 60, train loss: 0.01519432757049799, val loss: 0.19346420466899872\n",
      "Epoch 70, train loss: 0.005753966048359871, val loss: 0.0495283342897892\n",
      "Epoch 80, train loss: 0.0037466345820575953, val loss: 0.07142159342765808\n",
      "Epoch 90, train loss: 0.002916756086051464, val loss: 0.08910682052373886\n",
      "Training time: 7.854466676712036\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.33004501461982727, val loss: 0.00745747284963727\n",
      "Epoch 10, train loss: 0.10970287770032883, val loss: 0.049736250191926956\n",
      "Epoch 20, train loss: 0.04707803577184677, val loss: 0.3794955015182495\n",
      "Epoch 30, train loss: 0.022198405116796494, val loss: 0.16174109280109406\n",
      "Epoch 40, train loss: 0.019240567460656166, val loss: 0.20284195244312286\n",
      "Epoch 50, train loss: 0.019329175353050232, val loss: 0.2430027574300766\n",
      "Epoch 60, train loss: 0.018162501975893974, val loss: 0.19821074604988098\n",
      "Epoch 70, train loss: 0.017533062025904655, val loss: 0.21071292459964752\n",
      "Epoch 80, train loss: 0.016952255740761757, val loss: 0.2030615508556366\n",
      "Epoch 90, train loss: 0.015941526740789413, val loss: 0.18867561221122742\n",
      "Training time: 7.385022401809692\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6020282506942749, val loss: 0.019549814984202385\n",
      "Epoch 10, train loss: 0.02231353148818016, val loss: 0.22713741660118103\n",
      "Epoch 20, train loss: 0.0212663933634758, val loss: 0.1802087277173996\n",
      "Epoch 30, train loss: 0.020704442635178566, val loss: 0.26211339235305786\n",
      "Epoch 40, train loss: 0.01954393833875656, val loss: 0.21572346985340118\n",
      "Epoch 50, train loss: 0.018911410123109818, val loss: 0.21794694662094116\n",
      "Epoch 60, train loss: 0.01731223054230213, val loss: 0.20933277904987335\n",
      "Epoch 70, train loss: 0.0047515761107206345, val loss: 0.040299125015735626\n",
      "Epoch 80, train loss: 0.0026463638059794903, val loss: 0.0853906124830246\n",
      "Epoch 90, train loss: 0.001285073347389698, val loss: 0.04010460153222084\n",
      "Epoch 100, train loss: 0.0004657526151277125, val loss: 0.025284936651587486\n",
      "Epoch 110, train loss: 0.00033782259561121464, val loss: 0.01535503938794136\n",
      "Epoch 120, train loss: 0.00017358909826725721, val loss: 0.008433789014816284\n",
      "Epoch 130, train loss: 0.00015917611017357558, val loss: 0.006120646372437477\n",
      "Epoch 140, train loss: 0.00014907759032212198, val loss: 0.0050138672813773155\n",
      "Epoch 150, train loss: 0.00012969961971975863, val loss: 0.0037827580235898495\n",
      "Epoch 160, train loss: 0.0001580403040861711, val loss: 0.002812949474900961\n",
      "Epoch 170, train loss: 0.00023037430946715176, val loss: 0.0029686016496270895\n",
      "Epoch 180, train loss: 0.0001530123845441267, val loss: 0.00346323917619884\n",
      "Epoch 190, train loss: 0.00012479862198233604, val loss: 0.0028739215340465307\n",
      "Training time: 15.684171438217163\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.40022408962249756, val loss: 0.019664205610752106\n",
      "Epoch 10, train loss: 0.21675434708595276, val loss: 0.005927571095526218\n",
      "Epoch 20, train loss: 0.02274433523416519, val loss: 0.3294854164123535\n",
      "Epoch 30, train loss: 0.01735972799360752, val loss: 0.15805281698703766\n",
      "Epoch 40, train loss: 0.01994650810956955, val loss: 0.1623431295156479\n",
      "Epoch 50, train loss: 0.018280141055583954, val loss: 0.22985605895519257\n",
      "Epoch 60, train loss: 0.016457175835967064, val loss: 0.17741532623767853\n",
      "Epoch 70, train loss: 0.015853440389037132, val loss: 0.18986213207244873\n",
      "Epoch 80, train loss: 0.015416005626320839, val loss: 0.18495629727840424\n",
      "Epoch 90, train loss: 0.01479637622833252, val loss: 0.1733318418264389\n",
      "Epoch 100, train loss: 0.013943412341177464, val loss: 0.16835498809814453\n",
      "Epoch 110, train loss: 0.012648056261241436, val loss: 0.1523139476776123\n",
      "Epoch 120, train loss: 0.010410992428660393, val loss: 0.12941831350326538\n",
      "Epoch 130, train loss: 0.00610447209328413, val loss: 0.08730726689100266\n",
      "Epoch 140, train loss: 0.0007866154774092138, val loss: 0.03730589151382446\n",
      "Epoch 150, train loss: 0.0010348688811063766, val loss: 0.02376534976065159\n",
      "Epoch 160, train loss: 0.0004456646856851876, val loss: 0.029749305918812752\n",
      "Epoch 170, train loss: 0.0003761347907129675, val loss: 0.025941401720046997\n",
      "Epoch 180, train loss: 0.00034628852154128253, val loss: 0.021853605285286903\n",
      "Epoch 190, train loss: 0.0003034437831956893, val loss: 0.02196134254336357\n",
      "Training time: 15.395719289779663\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.4618109464645386, val loss: 0.005867967382073402\n",
      "Epoch 10, train loss: 0.018183724954724312, val loss: 0.17061154544353485\n",
      "Epoch 20, train loss: 0.017633797600865364, val loss: 0.19146455824375153\n",
      "Epoch 30, train loss: 0.01175537146627903, val loss: 0.12647683918476105\n",
      "Epoch 40, train loss: 0.004681100603193045, val loss: 0.026055850088596344\n",
      "Epoch 50, train loss: 0.005642732139676809, val loss: 0.05451655015349388\n",
      "Epoch 60, train loss: 0.0007208831375464797, val loss: 0.011011374182999134\n",
      "Epoch 70, train loss: 0.0008298757602460682, val loss: 0.0041933683678507805\n",
      "Epoch 80, train loss: 0.00046856558765284717, val loss: 0.008716721087694168\n",
      "Epoch 90, train loss: 0.00019563989189919084, val loss: 0.0019836300052702427\n",
      "Training time: 12.573648452758789\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5373694896697998, val loss: 0.04953964054584503\n",
      "Epoch 10, train loss: 0.19997262954711914, val loss: 0.011575034819543362\n",
      "Epoch 20, train loss: 0.04606189951300621, val loss: 0.29161328077316284\n",
      "Epoch 30, train loss: 0.030400099232792854, val loss: 0.11186675727367401\n",
      "Epoch 40, train loss: 0.01813996210694313, val loss: 0.23390451073646545\n",
      "Epoch 50, train loss: 0.015224621631205082, val loss: 0.15901418030261993\n",
      "Epoch 60, train loss: 0.0139622762799263, val loss: 0.16977807879447937\n",
      "Epoch 70, train loss: 0.012502742931246758, val loss: 0.14710046350955963\n",
      "Epoch 80, train loss: 0.010048240423202515, val loss: 0.12698334455490112\n",
      "Epoch 90, train loss: 0.005429442040622234, val loss: 0.07556412369012833\n",
      "Training time: 12.292693614959717\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.8285655379295349, val loss: 0.03094891645014286\n",
      "Epoch 10, train loss: 0.15548495948314667, val loss: 0.022944141179323196\n",
      "Epoch 20, train loss: 0.03400903195142746, val loss: 0.31133630871772766\n",
      "Epoch 30, train loss: 0.023573193699121475, val loss: 0.1670004278421402\n",
      "Epoch 40, train loss: 0.019490541890263557, val loss: 0.2517455220222473\n",
      "Epoch 50, train loss: 0.018700670450925827, val loss: 0.21433259546756744\n",
      "Epoch 60, train loss: 0.01851774752140045, val loss: 0.21418745815753937\n",
      "Epoch 70, train loss: 0.018183156847953796, val loss: 0.2186623215675354\n",
      "Epoch 80, train loss: 0.01766924187541008, val loss: 0.20479370653629303\n",
      "Epoch 90, train loss: 0.016855968162417412, val loss: 0.2019188106060028\n",
      "Epoch 100, train loss: 0.014960198663175106, val loss: 0.18076930940151215\n",
      "Epoch 110, train loss: 0.003839259734377265, val loss: 0.0778159573674202\n",
      "Epoch 120, train loss: 0.02157922089099884, val loss: 0.06987990438938141\n",
      "Epoch 130, train loss: 0.007857519201934338, val loss: 0.09197832643985748\n",
      "Epoch 140, train loss: 0.003264568978920579, val loss: 0.07669597864151001\n",
      "Epoch 150, train loss: 0.001418471452780068, val loss: 0.04238566383719444\n",
      "Epoch 160, train loss: 0.0012426982866600156, val loss: 0.03237568587064743\n",
      "Epoch 170, train loss: 0.001028616912662983, val loss: 0.03214794024825096\n",
      "Epoch 180, train loss: 0.0007157099316827953, val loss: 0.03191414102911949\n",
      "Epoch 190, train loss: 0.0006228351849131286, val loss: 0.03326769173145294\n",
      "Training time: 25.040783882141113\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7176199555397034, val loss: 0.11664385348558426\n",
      "Epoch 10, train loss: 0.1655007302761078, val loss: 0.01405264064669609\n",
      "Epoch 20, train loss: 0.015856338664889336, val loss: 0.1398577094078064\n",
      "Epoch 30, train loss: 0.02605302259325981, val loss: 0.08905929327011108\n",
      "Epoch 40, train loss: 0.018201712518930435, val loss: 0.19688266515731812\n",
      "Epoch 50, train loss: 0.013514168560504913, val loss: 0.1156742125749588\n",
      "Epoch 60, train loss: 0.01176440343260765, val loss: 0.14830957353115082\n",
      "Epoch 70, train loss: 0.011056281626224518, val loss: 0.12454919517040253\n",
      "Epoch 80, train loss: 0.010383852757513523, val loss: 0.1247260719537735\n",
      "Epoch 90, train loss: 0.00962549913674593, val loss: 0.11188184469938278\n",
      "Epoch 100, train loss: 0.00874408707022667, val loss: 0.1050991490483284\n",
      "Epoch 110, train loss: 0.007700570393353701, val loss: 0.09103086590766907\n",
      "Epoch 120, train loss: 0.006419614423066378, val loss: 0.07840512692928314\n",
      "Epoch 130, train loss: 0.004749324172735214, val loss: 0.06048903614282608\n",
      "Epoch 140, train loss: 0.0024237411562353373, val loss: 0.03603139892220497\n",
      "Epoch 150, train loss: 0.0002661647740751505, val loss: 0.009584947489202023\n",
      "Epoch 160, train loss: 0.0004330175288487226, val loss: 0.00543412659317255\n",
      "Epoch 170, train loss: 0.0002379224606556818, val loss: 0.010533838532865047\n",
      "Epoch 180, train loss: 0.00021735619520768523, val loss: 0.009082002565264702\n",
      "Epoch 190, train loss: 0.00019263381545897573, val loss: 0.00658034672960639\n",
      "Training time: 25.39393901824951\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7005417346954346, val loss: 0.009550463408231735\n",
      "Epoch 10, train loss: 0.04267950728535652, val loss: 0.34196799993515015\n",
      "Epoch 20, train loss: 0.02138173207640648, val loss: 0.2148074209690094\n",
      "Epoch 30, train loss: 0.019491078332066536, val loss: 0.21426717936992645\n",
      "Epoch 40, train loss: 0.019650213420391083, val loss: 0.250090092420578\n",
      "Epoch 50, train loss: 0.01964646205306053, val loss: 0.21384933590888977\n",
      "Epoch 60, train loss: 0.019335731863975525, val loss: 0.23252347111701965\n",
      "Epoch 70, train loss: 0.018926214426755905, val loss: 0.22604160010814667\n",
      "Epoch 80, train loss: 0.009432612918317318, val loss: 0.14799152314662933\n",
      "Epoch 90, train loss: 0.0031208437867462635, val loss: 0.07506921887397766\n",
      "Training time: 19.779858350753784\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5466000437736511, val loss: 0.05356048792600632\n",
      "Epoch 10, train loss: 0.0959414690732956, val loss: 0.07297829538583755\n",
      "Epoch 20, train loss: 0.019751714542508125, val loss: 0.1205231100320816\n",
      "Epoch 30, train loss: 0.01797768846154213, val loss: 0.18135176599025726\n",
      "Epoch 40, train loss: 0.017354954034090042, val loss: 0.20609717071056366\n",
      "Epoch 50, train loss: 0.01691330038011074, val loss: 0.1709006428718567\n",
      "Epoch 60, train loss: 0.01593845523893833, val loss: 0.19659458100795746\n",
      "Epoch 70, train loss: 0.015059120953083038, val loss: 0.16991089284420013\n",
      "Epoch 80, train loss: 0.014035080559551716, val loss: 0.17011681199073792\n",
      "Epoch 90, train loss: 0.01234994176775217, val loss: 0.1506337970495224\n",
      "Training time: 20.88368511199951\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.4156274199485779, val loss: 0.0468631312251091\n",
      "Epoch 10, train loss: 0.06218886002898216, val loss: 0.13691279292106628\n",
      "Epoch 20, train loss: 0.023872211575508118, val loss: 0.1658429056406021\n",
      "Epoch 30, train loss: 0.019979946315288544, val loss: 0.2618609070777893\n",
      "Epoch 40, train loss: 0.01945348270237446, val loss: 0.21659992635250092\n",
      "Epoch 50, train loss: 0.01928192935883999, val loss: 0.23084387183189392\n",
      "Epoch 60, train loss: 0.019095350056886673, val loss: 0.22511304914951324\n",
      "Epoch 70, train loss: 0.018772440031170845, val loss: 0.2222774773836136\n",
      "Epoch 80, train loss: 0.018029484897851944, val loss: 0.2130117267370224\n",
      "Epoch 90, train loss: 0.007447251118719578, val loss: 0.12864752113819122\n",
      "Epoch 100, train loss: 0.010178289376199245, val loss: 0.07411913573741913\n",
      "Epoch 110, train loss: 0.001708346651867032, val loss: 0.06538226455450058\n",
      "Epoch 120, train loss: 0.0012204545782878995, val loss: 0.012496273964643478\n",
      "Epoch 130, train loss: 0.0005259836325421929, val loss: 0.015820972621440887\n",
      "Epoch 140, train loss: 0.0002534602244850248, val loss: 0.006826111115515232\n",
      "Epoch 150, train loss: 0.00017568709154147655, val loss: 0.001867865677922964\n",
      "Epoch 160, train loss: 0.00013586162822321057, val loss: 0.0015955570852383971\n",
      "Epoch 170, train loss: 0.00013562534877564758, val loss: 0.0013523598900064826\n",
      "Epoch 180, train loss: 0.0001286165352212265, val loss: 0.0019228709861636162\n",
      "Epoch 190, train loss: 0.00012703886022791266, val loss: 0.002101582707837224\n",
      "Training time: 41.696943283081055\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7177809476852417, val loss: 0.11097969859838486\n",
      "Epoch 10, train loss: 0.1671561449766159, val loss: 0.03166509047150612\n",
      "Epoch 20, train loss: 0.017033740878105164, val loss: 0.15810203552246094\n",
      "Epoch 30, train loss: 0.024247556924819946, val loss: 0.1613355427980423\n",
      "Epoch 40, train loss: 0.021420810371637344, val loss: 0.25002220273017883\n",
      "Epoch 50, train loss: 0.018831241875886917, val loss: 0.17017114162445068\n",
      "Epoch 60, train loss: 0.01722787693142891, val loss: 0.21751731634140015\n",
      "Epoch 70, train loss: 0.01637892983853817, val loss: 0.1809765249490738\n",
      "Epoch 80, train loss: 0.01564808376133442, val loss: 0.1907377690076828\n",
      "Epoch 90, train loss: 0.014724346809089184, val loss: 0.1704493910074234\n",
      "Epoch 100, train loss: 0.013278004713356495, val loss: 0.16047683358192444\n",
      "Epoch 110, train loss: 0.01050035934895277, val loss: 0.1290893256664276\n",
      "Epoch 120, train loss: 0.00479945819824934, val loss: 0.07200727611780167\n",
      "Epoch 130, train loss: 0.000681794248521328, val loss: 0.018183594569563866\n",
      "Epoch 140, train loss: 0.0004603037377819419, val loss: 0.017833033576607704\n",
      "Epoch 150, train loss: 0.0004591578326653689, val loss: 0.022232206538319588\n",
      "Epoch 160, train loss: 0.0002549327036831528, val loss: 0.01550967339426279\n",
      "Epoch 170, train loss: 0.00025564394309185445, val loss: 0.013996945694088936\n",
      "Epoch 180, train loss: 0.0002413296897429973, val loss: 0.0145031176507473\n",
      "Epoch 190, train loss: 0.00022887595696374774, val loss: 0.012884946539998055\n",
      "Training time: 42.43516516685486\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6311360001564026, val loss: 0.0429520457983017\n",
      "Epoch 10, train loss: 0.19972287118434906, val loss: 0.01244738046079874\n",
      "Epoch 20, train loss: 0.025884190574288368, val loss: 0.1317431479692459\n",
      "Epoch 30, train loss: 0.023913688957691193, val loss: 0.2948991656303406\n",
      "Epoch 40, train loss: 0.020502159371972084, val loss: 0.1847664713859558\n",
      "Epoch 50, train loss: 0.01824665255844593, val loss: 0.21965153515338898\n",
      "Epoch 60, train loss: 0.016719022765755653, val loss: 0.20667919516563416\n",
      "Epoch 70, train loss: 0.037772905081510544, val loss: 0.08116915076971054\n",
      "Epoch 80, train loss: 0.017480527982115746, val loss: 0.18101726472377777\n",
      "Epoch 90, train loss: 0.01615414209663868, val loss: 0.1631922721862793\n",
      "Training time: 42.491453886032104\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7235201001167297, val loss: 0.10683989524841309\n",
      "Epoch 10, train loss: 0.13807819783687592, val loss: 0.40362975001335144\n",
      "Epoch 20, train loss: 0.04293644428253174, val loss: 0.07384686917066574\n",
      "Epoch 30, train loss: 0.020640909671783447, val loss: 0.19590164721012115\n",
      "Epoch 40, train loss: 0.015187106095254421, val loss: 0.12254565954208374\n",
      "Epoch 50, train loss: 0.012321584858000278, val loss: 0.1484214961528778\n",
      "Epoch 60, train loss: 0.010784846730530262, val loss: 0.12652193009853363\n",
      "Epoch 70, train loss: 0.009469536133110523, val loss: 0.10978938639163971\n",
      "Epoch 80, train loss: 0.007789629511535168, val loss: 0.09779801964759827\n",
      "Epoch 90, train loss: 0.004710061475634575, val loss: 0.06269615888595581\n",
      "Training time: 44.5712103843689\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6198354959487915, val loss: 0.011439553461968899\n",
      "Epoch 10, train loss: 0.11186477541923523, val loss: 0.07478776574134827\n",
      "Epoch 20, train loss: 0.03216349333524704, val loss: 0.1310546100139618\n",
      "Epoch 30, train loss: 0.023582005873322487, val loss: 0.2765255570411682\n",
      "Epoch 40, train loss: 0.01958189718425274, val loss: 0.20133136212825775\n",
      "Epoch 50, train loss: 0.018118014559149742, val loss: 0.2100052833557129\n",
      "Epoch 60, train loss: 0.01750234141945839, val loss: 0.21751052141189575\n",
      "Epoch 70, train loss: 0.015974050387740135, val loss: 0.18797053396701813\n",
      "Epoch 80, train loss: 0.00909555982798338, val loss: 0.1002436950802803\n",
      "Epoch 90, train loss: 0.026784183457493782, val loss: 0.2245112955570221\n",
      "Epoch 100, train loss: 0.01691281795501709, val loss: 0.1960410624742508\n",
      "Epoch 110, train loss: 0.016076959669589996, val loss: 0.1753675937652588\n",
      "Epoch 120, train loss: 0.014578410424292088, val loss: 0.18469153344631195\n",
      "Epoch 130, train loss: 0.009782304987311363, val loss: 0.1263732761144638\n",
      "Epoch 140, train loss: 0.001031877938657999, val loss: 0.04185129702091217\n",
      "Epoch 150, train loss: 0.0007911589345894754, val loss: 0.034492723643779755\n",
      "Epoch 160, train loss: 0.0007072506123222411, val loss: 0.03376536816358566\n",
      "Epoch 170, train loss: 0.0006287400610744953, val loss: 0.033401645720005035\n",
      "Epoch 180, train loss: 0.0005362702650018036, val loss: 0.033975839614868164\n",
      "Epoch 190, train loss: 0.00045392458559945226, val loss: 0.028604434803128242\n",
      "Training time: 85.98992037773132\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5838149189949036, val loss: 0.055272024124860764\n",
      "Epoch 10, train loss: 0.05061986297369003, val loss: 0.3729855418205261\n",
      "Epoch 20, train loss: 0.03504379093647003, val loss: 0.08561946451663971\n",
      "Epoch 30, train loss: 0.020017217844724655, val loss: 0.20377297699451447\n",
      "Epoch 40, train loss: 0.014780987054109573, val loss: 0.1256095916032791\n",
      "Epoch 50, train loss: 0.011897156946361065, val loss: 0.14487560093402863\n",
      "Epoch 60, train loss: 0.00958710815757513, val loss: 0.11713612824678421\n",
      "Epoch 70, train loss: 0.006627555936574936, val loss: 0.0760735273361206\n",
      "Epoch 80, train loss: 0.0009623991791158915, val loss: 0.0204171109944582\n",
      "Epoch 90, train loss: 0.0007187037845142186, val loss: 0.0055181654170155525\n",
      "Epoch 100, train loss: 0.0004674895608332008, val loss: 0.014653784222900867\n",
      "Epoch 110, train loss: 0.00020201446022838354, val loss: 0.005672752857208252\n",
      "Epoch 120, train loss: 0.00018184955115430057, val loss: 0.0048203538171947\n",
      "Epoch 130, train loss: 0.00015920470468699932, val loss: 0.005193538498133421\n",
      "Epoch 140, train loss: 0.00013399447198025882, val loss: 0.002841488691046834\n",
      "Epoch 150, train loss: 0.00011905025894520804, val loss: 0.002659283112734556\n",
      "Epoch 160, train loss: 0.00010979911894537508, val loss: 0.0018888985505327582\n",
      "Epoch 170, train loss: 0.00010353152174502611, val loss: 0.0014160481514409184\n",
      "Epoch 180, train loss: 9.929595398716629e-05, val loss: 0.0011512498604133725\n",
      "Epoch 190, train loss: 9.655439498601481e-05, val loss: 0.0008908548043109477\n",
      "Training time: 86.68402338027954\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.691596269607544, val loss: 0.027935046702623367\n",
      "Epoch 10, train loss: 0.17836548388004303, val loss: 0.06264665722846985\n",
      "Epoch 20, train loss: 0.04745742678642273, val loss: 0.3789319694042206\n",
      "Epoch 30, train loss: 0.019712232053279877, val loss: 0.27623122930526733\n",
      "Epoch 40, train loss: 0.022500095888972282, val loss: 0.18536046147346497\n",
      "Epoch 50, train loss: 0.020012449473142624, val loss: 0.2373487651348114\n",
      "Epoch 60, train loss: 0.01981700398027897, val loss: 0.2486349195241928\n",
      "Epoch 70, train loss: 0.019758816808462143, val loss: 0.2266252338886261\n",
      "Epoch 80, train loss: 0.019673140719532967, val loss: 0.22668534517288208\n",
      "Epoch 90, train loss: 0.019669411703944206, val loss: 0.2335393875837326\n",
      "Training time: 66.95634031295776\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6138036251068115, val loss: 0.06526359915733337\n",
      "Epoch 10, train loss: 0.038592416793107986, val loss: 0.20486368238925934\n",
      "Epoch 20, train loss: 0.01964757777750492, val loss: 0.19858521223068237\n",
      "Epoch 30, train loss: 0.01696457900106907, val loss: 0.16693900525569916\n",
      "Epoch 40, train loss: 0.016208969056606293, val loss: 0.21252624690532684\n",
      "Epoch 50, train loss: 0.015543501824140549, val loss: 0.16256876289844513\n",
      "Epoch 60, train loss: 0.013846536166965961, val loss: 0.1741981953382492\n",
      "Epoch 70, train loss: 0.00959200132638216, val loss: 0.12309613078832626\n",
      "Epoch 80, train loss: 0.001352620660327375, val loss: 0.03353374823927879\n",
      "Epoch 90, train loss: 0.0009698602952994406, val loss: 0.01785908080637455\n",
      "Training time: 66.13893461227417\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7613136768341064, val loss: 0.11445402354001999\n",
      "Epoch 10, train loss: 0.18775971233844757, val loss: 0.006665835622698069\n",
      "Epoch 20, train loss: 0.021809225901961327, val loss: 0.13276134431362152\n",
      "Epoch 30, train loss: 0.019948923960328102, val loss: 0.22257016599178314\n",
      "Epoch 40, train loss: 0.0271166805177927, val loss: 0.1609889566898346\n",
      "Epoch 50, train loss: 0.019779948517680168, val loss: 0.18289066851139069\n",
      "Epoch 60, train loss: 0.014384638518095016, val loss: 0.15721723437309265\n",
      "Epoch 70, train loss: 0.0010235396912321448, val loss: 0.17258167266845703\n",
      "Epoch 80, train loss: 0.0012596967862918973, val loss: 0.02745651826262474\n",
      "Epoch 90, train loss: 0.0005182513850741088, val loss: 0.020951824262738228\n",
      "Epoch 100, train loss: 0.0003612118889577687, val loss: 0.022046325728297234\n",
      "Epoch 110, train loss: 0.00025105566601268947, val loss: 0.017239883542060852\n",
      "Epoch 120, train loss: 0.00022311411157716066, val loss: 0.016894878819584846\n",
      "Epoch 130, train loss: 0.00019839855667669326, val loss: 0.01403397973626852\n",
      "Epoch 140, train loss: 0.00018920924048870802, val loss: 0.013213835656642914\n",
      "Epoch 150, train loss: 0.0001851431152317673, val loss: 0.012029258534312248\n",
      "Epoch 160, train loss: 0.0013047127285972238, val loss: 0.014166034758090973\n",
      "Epoch 170, train loss: 0.0011255507124587893, val loss: 0.011443686671555042\n",
      "Epoch 180, train loss: 0.000732158834580332, val loss: 0.010390828363597393\n",
      "Epoch 190, train loss: 0.00042790966108441353, val loss: 0.011767420917749405\n",
      "Training time: 136.15262055397034\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5843214988708496, val loss: 0.055234555155038834\n",
      "Epoch 10, train loss: 0.061005834490060806, val loss: 0.2787455916404724\n",
      "Epoch 20, train loss: 0.022959835827350616, val loss: 0.18775229156017303\n",
      "Epoch 30, train loss: 0.01730155572295189, val loss: 0.17933389544487\n",
      "Epoch 40, train loss: 0.016520678997039795, val loss: 0.21540072560310364\n",
      "Epoch 50, train loss: 0.015545789152383804, val loss: 0.161187082529068\n",
      "Epoch 60, train loss: 0.012427492067217827, val loss: 0.15235890448093414\n",
      "Epoch 70, train loss: 0.0027159936726093292, val loss: 0.03960094600915909\n",
      "Epoch 80, train loss: 0.0006121120532043278, val loss: 0.016842840239405632\n",
      "Epoch 90, train loss: 0.0008564580930396914, val loss: 0.02119932696223259\n",
      "Epoch 100, train loss: 0.00039235458825714886, val loss: 0.0041728210635483265\n",
      "Epoch 110, train loss: 0.0002214236737927422, val loss: 0.00752051780000329\n",
      "Epoch 120, train loss: 0.00015007154433988035, val loss: 0.0026604312006384134\n",
      "Epoch 130, train loss: 0.00013494440645445138, val loss: 0.0022845666389912367\n",
      "Epoch 140, train loss: 0.0001269057102035731, val loss: 0.0013366957427933812\n",
      "Epoch 150, train loss: 0.0001250171335414052, val loss: 0.0011309466790407896\n",
      "Epoch 160, train loss: 0.0001243447040906176, val loss: 0.000929623784031719\n",
      "Epoch 170, train loss: 0.00012407134636305273, val loss: 0.0009065004414878786\n",
      "Epoch 180, train loss: 0.00012389247422106564, val loss: 0.0008562253788113594\n",
      "Epoch 190, train loss: 0.000123696998343803, val loss: 0.0008744709193706512\n",
      "Training time: 129.90190291404724\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7263984680175781, val loss: 0.007959004491567612\n",
      "Epoch 10, train loss: 0.037353843450546265, val loss: 0.09361527115106583\n",
      "Epoch 20, train loss: 0.012972235679626465, val loss: 0.10262982547283173\n",
      "Epoch 30, train loss: 0.01327748503535986, val loss: 0.14083142578601837\n",
      "Epoch 40, train loss: 0.006705315317958593, val loss: 0.07638077437877655\n",
      "Epoch 50, train loss: 0.0029952486511319876, val loss: 0.02272365242242813\n",
      "Epoch 60, train loss: 0.0006597538013011217, val loss: 0.004141306038945913\n",
      "Epoch 70, train loss: 5.453833000501618e-05, val loss: 0.00030807132134214044\n",
      "Epoch 80, train loss: 0.00014768107212148607, val loss: 0.0005601804004982114\n",
      "Epoch 90, train loss: 6.32079245406203e-05, val loss: 0.0006411643698811531\n",
      "Training time: 10.014298915863037\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.33475810289382935, val loss: 0.005513766314834356\n",
      "Epoch 10, train loss: 0.04116564989089966, val loss: 0.10080330073833466\n",
      "Epoch 20, train loss: 0.03655523061752319, val loss: 0.28351089358329773\n",
      "Epoch 30, train loss: 0.014409872703254223, val loss: 0.13794496655464172\n",
      "Epoch 40, train loss: 0.015591828152537346, val loss: 0.1323813796043396\n",
      "Epoch 50, train loss: 0.013484823517501354, val loss: 0.17151972651481628\n",
      "Epoch 60, train loss: 0.012503301724791527, val loss: 0.1482335478067398\n",
      "Epoch 70, train loss: 0.012080155313014984, val loss: 0.1357978880405426\n",
      "Epoch 80, train loss: 0.011397410184144974, val loss: 0.1383889764547348\n",
      "Epoch 90, train loss: 0.01065702736377716, val loss: 0.12415212392807007\n",
      "Training time: 9.965008735656738\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6453160643577576, val loss: 0.0033442662097513676\n",
      "Epoch 10, train loss: 0.03616354241967201, val loss: 0.12696418166160583\n",
      "Epoch 20, train loss: 0.01679900661110878, val loss: 0.10553672164678574\n",
      "Epoch 30, train loss: 0.014397261664271355, val loss: 0.1520930677652359\n",
      "Epoch 40, train loss: 0.007351874373853207, val loss: 0.09204648435115814\n",
      "Epoch 50, train loss: 0.002564013469964266, val loss: 0.023672591894865036\n",
      "Epoch 60, train loss: 0.0008110590861178935, val loss: 0.02121424674987793\n",
      "Epoch 70, train loss: 0.00030846873414702713, val loss: 0.001289232517592609\n",
      "Epoch 80, train loss: 0.00012576767767313868, val loss: 0.0003015189722646028\n",
      "Epoch 90, train loss: 7.92000864748843e-05, val loss: 0.0016933277947828174\n",
      "Epoch 100, train loss: 5.538942423299886e-05, val loss: 0.0002556073304731399\n",
      "Epoch 110, train loss: 4.14746355090756e-05, val loss: 0.0004321433953009546\n",
      "Epoch 120, train loss: 4.145022467128001e-05, val loss: 0.0004876712046097964\n",
      "Epoch 130, train loss: 3.8977952499408275e-05, val loss: 0.0003193463198840618\n",
      "Epoch 140, train loss: 3.702117464854382e-05, val loss: 0.00037289928877726197\n",
      "Epoch 150, train loss: 3.5762976040132344e-05, val loss: 0.0003149969852529466\n",
      "Epoch 160, train loss: 3.4710417821770534e-05, val loss: 0.00028971213032491505\n",
      "Epoch 170, train loss: 3.3742468076525256e-05, val loss: 0.0002784593962132931\n",
      "Epoch 180, train loss: 3.2858519261935726e-05, val loss: 0.00025317544350400567\n",
      "Epoch 190, train loss: 3.2056294003268704e-05, val loss: 0.0002416931965854019\n",
      "Training time: 20.206751108169556\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9253618717193604, val loss: 0.18977956473827362\n",
      "Epoch 10, train loss: 0.3803461492061615, val loss: 0.022486865520477295\n",
      "Epoch 20, train loss: 0.037467170506715775, val loss: 0.05954108014702797\n",
      "Epoch 30, train loss: 0.04380914196372032, val loss: 0.24068915843963623\n",
      "Epoch 40, train loss: 0.009884249418973923, val loss: 0.10501525551080704\n",
      "Epoch 50, train loss: 0.014155215583741665, val loss: 0.08046156167984009\n",
      "Epoch 60, train loss: 0.009532000869512558, val loss: 0.11559315025806427\n",
      "Epoch 70, train loss: 0.009747087024152279, val loss: 0.1172398254275322\n",
      "Epoch 80, train loss: 0.009025589562952518, val loss: 0.09727755934000015\n",
      "Epoch 90, train loss: 0.008629001677036285, val loss: 0.09737720340490341\n",
      "Epoch 100, train loss: 0.008279518224298954, val loss: 0.09681817144155502\n",
      "Epoch 110, train loss: 0.007870413362979889, val loss: 0.08872822672128677\n",
      "Epoch 120, train loss: 0.007456460501998663, val loss: 0.08446330577135086\n",
      "Epoch 130, train loss: 0.007013217080384493, val loss: 0.07967764884233475\n",
      "Epoch 140, train loss: 0.006533126346766949, val loss: 0.073175810277462\n",
      "Epoch 150, train loss: 0.006012619473040104, val loss: 0.06725816428661346\n",
      "Epoch 160, train loss: 0.005445004440844059, val loss: 0.06029869243502617\n",
      "Epoch 170, train loss: 0.004822620190680027, val loss: 0.05285453051328659\n",
      "Epoch 180, train loss: 0.004137981217354536, val loss: 0.044725898653268814\n",
      "Epoch 190, train loss: 0.0033862271811813116, val loss: 0.03587302938103676\n",
      "Training time: 19.727036952972412\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6211552619934082, val loss: 0.012776517309248447\n",
      "Epoch 10, train loss: 0.023455901071429253, val loss: 0.3009481430053711\n",
      "Epoch 20, train loss: 0.013686033897101879, val loss: 0.15396659076213837\n",
      "Epoch 30, train loss: 0.007871577516198158, val loss: 0.05010426416993141\n",
      "Epoch 40, train loss: 0.001944176503457129, val loss: 0.027289526537060738\n",
      "Epoch 50, train loss: 0.0006639778730459511, val loss: 0.002581316977739334\n",
      "Epoch 60, train loss: 0.00020079145906493068, val loss: 0.007419680245220661\n",
      "Epoch 70, train loss: 0.00014776179159525782, val loss: 0.0004332824028097093\n",
      "Epoch 80, train loss: 0.00011638538853731006, val loss: 0.001865822123363614\n",
      "Epoch 90, train loss: 7.686288154218346e-05, val loss: 0.0009739000233821571\n",
      "Training time: 14.626538515090942\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.46805813908576965, val loss: 0.030287493020296097\n",
      "Epoch 10, train loss: 0.06696755439043045, val loss: 0.06686536967754364\n",
      "Epoch 20, train loss: 0.04424823448061943, val loss: 0.3011028468608856\n",
      "Epoch 30, train loss: 0.016887666657567024, val loss: 0.131449893116951\n",
      "Epoch 40, train loss: 0.016034772619605064, val loss: 0.14321406185626984\n",
      "Epoch 50, train loss: 0.014469398185610771, val loss: 0.18066662549972534\n",
      "Epoch 60, train loss: 0.012413342483341694, val loss: 0.13873925805091858\n",
      "Epoch 70, train loss: 0.011184426955878735, val loss: 0.12856246531009674\n",
      "Epoch 80, train loss: 0.009625391103327274, val loss: 0.11339815706014633\n",
      "Epoch 90, train loss: 0.007522763218730688, val loss: 0.08385828137397766\n",
      "Training time: 14.570168256759644\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.46106261014938354, val loss: 0.02627250924706459\n",
      "Epoch 10, train loss: 0.02598917856812477, val loss: 0.2851395905017853\n",
      "Epoch 20, train loss: 0.014697864651679993, val loss: 0.2016870528459549\n",
      "Epoch 30, train loss: 0.009823701344430447, val loss: 0.11461891233921051\n",
      "Epoch 40, train loss: 0.0007596906507387757, val loss: 0.03735695779323578\n",
      "Epoch 50, train loss: 0.000666746636852622, val loss: 0.0016636939253658056\n",
      "Epoch 60, train loss: 0.00031969023984856904, val loss: 0.005244183819741011\n",
      "Epoch 70, train loss: 0.0001657317770877853, val loss: 0.000701019715052098\n",
      "Epoch 80, train loss: 0.00011704291682690382, val loss: 0.0015998074086382985\n",
      "Epoch 90, train loss: 8.839376096148044e-05, val loss: 0.0011742600472643971\n",
      "Epoch 100, train loss: 7.949788414407521e-05, val loss: 0.0010720026912167668\n",
      "Epoch 110, train loss: 7.451912097167224e-05, val loss: 0.0008828931604512036\n",
      "Epoch 120, train loss: 6.892374221934006e-05, val loss: 0.0007622100529260933\n",
      "Epoch 130, train loss: 6.423550075851381e-05, val loss: 0.0006729750311933458\n",
      "Epoch 140, train loss: 5.950576451141387e-05, val loss: 0.0005845091654919088\n",
      "Epoch 150, train loss: 5.372688974603079e-05, val loss: 0.00044644795707426965\n",
      "Epoch 160, train loss: 5.060502007836476e-05, val loss: 0.0003787245077546686\n",
      "Epoch 170, train loss: 4.82376926811412e-05, val loss: 0.00034077948657795787\n",
      "Epoch 180, train loss: 4.60388146166224e-05, val loss: 0.0003222486120648682\n",
      "Epoch 190, train loss: 4.407868254929781e-05, val loss: 0.0003043751639779657\n",
      "Training time: 29.17952799797058\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8319099545478821, val loss: 0.1550966054201126\n",
      "Epoch 10, train loss: 0.16406764090061188, val loss: 0.010341336950659752\n",
      "Epoch 20, train loss: 0.043668799102306366, val loss: 0.31539592146873474\n",
      "Epoch 30, train loss: 0.013409730046987534, val loss: 0.15053889155387878\n",
      "Epoch 40, train loss: 0.019284240901470184, val loss: 0.10597244650125504\n",
      "Epoch 50, train loss: 0.012683581560850143, val loss: 0.1623905748128891\n",
      "Epoch 60, train loss: 0.01218945998698473, val loss: 0.1472979038953781\n",
      "Epoch 70, train loss: 0.011510899290442467, val loss: 0.12126453965902328\n",
      "Epoch 80, train loss: 0.010502866469323635, val loss: 0.12610219419002533\n",
      "Epoch 90, train loss: 0.009635655209422112, val loss: 0.11153411865234375\n",
      "Epoch 100, train loss: 0.008655397221446037, val loss: 0.09821721911430359\n",
      "Epoch 110, train loss: 0.0074682412669062614, val loss: 0.08524803817272186\n",
      "Epoch 120, train loss: 0.006007572636008263, val loss: 0.06572835892438889\n",
      "Epoch 130, train loss: 0.00418820371851325, val loss: 0.04403043910861015\n",
      "Epoch 140, train loss: 0.002038165694102645, val loss: 0.018863486126065254\n",
      "Epoch 150, train loss: 0.0002751820138655603, val loss: 0.0013934571761637926\n",
      "Epoch 160, train loss: 0.000238962602452375, val loss: 0.002316534984856844\n",
      "Epoch 170, train loss: 0.00010331998782930896, val loss: 0.0006054576369933784\n",
      "Epoch 180, train loss: 9.468862117500976e-05, val loss: 0.0005437342915683985\n",
      "Epoch 190, train loss: 8.468744636047632e-05, val loss: 0.00044046054244972765\n",
      "Training time: 29.177500247955322\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.45067447423934937, val loss: 0.09319101274013519\n",
      "Epoch 10, train loss: 0.01286376267671585, val loss: 0.23164768517017365\n",
      "Epoch 20, train loss: 0.02596220001578331, val loss: 0.08712123334407806\n",
      "Epoch 30, train loss: 0.009102228097617626, val loss: 0.09028874337673187\n",
      "Epoch 40, train loss: 0.008198840543627739, val loss: 0.08096450567245483\n",
      "Epoch 50, train loss: 0.0023720869794487953, val loss: 0.014703534543514252\n",
      "Epoch 60, train loss: 0.001040955656208098, val loss: 0.014486078172922134\n",
      "Epoch 70, train loss: 0.0003311285108793527, val loss: 0.0015324951382353902\n",
      "Epoch 80, train loss: 8.057086233748123e-05, val loss: 0.0007114647305570543\n",
      "Epoch 90, train loss: 9.186835086438805e-05, val loss: 0.001059367205016315\n",
      "Training time: 16.900153636932373\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.881351888179779, val loss: 0.15470337867736816\n",
      "Epoch 10, train loss: 0.047076739370822906, val loss: 0.10882677882909775\n",
      "Epoch 20, train loss: 0.0160228218883276, val loss: 0.15039895474910736\n",
      "Epoch 30, train loss: 0.027021408081054688, val loss: 0.07893539220094681\n",
      "Epoch 40, train loss: 0.015556985512375832, val loss: 0.18137088418006897\n",
      "Epoch 50, train loss: 0.011107921600341797, val loss: 0.11170928180217743\n",
      "Epoch 60, train loss: 0.01024953369051218, val loss: 0.12000787258148193\n",
      "Epoch 70, train loss: 0.009690588340163231, val loss: 0.1165330559015274\n",
      "Epoch 80, train loss: 0.008987139910459518, val loss: 0.10098269581794739\n",
      "Epoch 90, train loss: 0.008238270878791809, val loss: 0.09866108745336533\n",
      "Training time: 16.914608478546143\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.88779616355896, val loss: 0.003132807556539774\n",
      "Epoch 10, train loss: 0.042946022003889084, val loss: 0.17918793857097626\n",
      "Epoch 20, train loss: 0.018427951261401176, val loss: 0.1485578566789627\n",
      "Epoch 30, train loss: 0.007371153682470322, val loss: 0.09172406792640686\n",
      "Epoch 40, train loss: 0.003584860824048519, val loss: 0.03941381722688675\n",
      "Epoch 50, train loss: 0.00027009102632291615, val loss: 0.0003416916588321328\n",
      "Epoch 60, train loss: 0.0005704259965568781, val loss: 0.0021274148020893335\n",
      "Epoch 70, train loss: 9.398002293892205e-05, val loss: 0.0007607879815623164\n",
      "Epoch 80, train loss: 7.739376451354474e-05, val loss: 0.000269431242486462\n",
      "Epoch 90, train loss: 5.7589382777223364e-05, val loss: 0.0006653215968981385\n",
      "Epoch 100, train loss: 4.876676030107774e-05, val loss: 0.0002886149741243571\n",
      "Epoch 110, train loss: 4.4227945181773975e-05, val loss: 0.00025809623184613883\n",
      "Epoch 120, train loss: 4.161314063821919e-05, val loss: 0.0003171961579937488\n",
      "Epoch 130, train loss: 4.0570048440713435e-05, val loss: 0.00028741505229845643\n",
      "Epoch 140, train loss: 3.980079782195389e-05, val loss: 0.00026854051975533366\n",
      "Epoch 150, train loss: 3.899205330526456e-05, val loss: 0.0002763803640846163\n",
      "Epoch 160, train loss: 3.825137173407711e-05, val loss: 0.00026609000633470714\n",
      "Epoch 170, train loss: 3.754188946913928e-05, val loss: 0.00025748589541763067\n",
      "Epoch 180, train loss: 3.686786658363417e-05, val loss: 0.00025357765844091773\n",
      "Epoch 190, train loss: 3.622676376835443e-05, val loss: 0.00024585003848187625\n",
      "Training time: 33.70123219490051\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7875856757164001, val loss: 0.1272062212228775\n",
      "Epoch 10, train loss: 0.11433234810829163, val loss: 0.029144588857889175\n",
      "Epoch 20, train loss: 0.0441737100481987, val loss: 0.23532778024673462\n",
      "Epoch 30, train loss: 0.025070693343877792, val loss: 0.0679713785648346\n",
      "Epoch 40, train loss: 0.010706344619393349, val loss: 0.14277072250843048\n",
      "Epoch 50, train loss: 0.009736593812704086, val loss: 0.11405453085899353\n",
      "Epoch 60, train loss: 0.009114318527281284, val loss: 0.09105584025382996\n",
      "Epoch 70, train loss: 0.007808166556060314, val loss: 0.09478968381881714\n",
      "Epoch 80, train loss: 0.006498577073216438, val loss: 0.06985756754875183\n",
      "Epoch 90, train loss: 0.0051931217312812805, val loss: 0.060931991785764694\n",
      "Epoch 100, train loss: 0.0038207953330129385, val loss: 0.04144951328635216\n",
      "Epoch 110, train loss: 0.002392331138253212, val loss: 0.025783712044358253\n",
      "Epoch 120, train loss: 0.0010663227876648307, val loss: 0.010677139274775982\n",
      "Epoch 130, train loss: 0.0002069678739644587, val loss: 0.0015404296573251486\n",
      "Epoch 140, train loss: 8.284729847218841e-05, val loss: 0.0006093980045989156\n",
      "Epoch 150, train loss: 0.00010769941582111642, val loss: 0.0007151198224164546\n",
      "Epoch 160, train loss: 6.551298429258168e-05, val loss: 0.00036941212601959705\n",
      "Epoch 170, train loss: 6.641193613177165e-05, val loss: 0.00041565843275748193\n",
      "Epoch 180, train loss: 6.491612293757498e-05, val loss: 0.00039079823181964457\n",
      "Epoch 190, train loss: 6.290355668170378e-05, val loss: 0.00036529460339806974\n",
      "Training time: 33.804187059402466\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7703887224197388, val loss: 0.13990101218223572\n",
      "Epoch 10, train loss: 0.0456853061914444, val loss: 0.44131606817245483\n",
      "Epoch 20, train loss: 0.029298067092895508, val loss: 0.31219881772994995\n",
      "Epoch 30, train loss: 0.015755733475089073, val loss: 0.19242273271083832\n",
      "Epoch 40, train loss: 0.015096203424036503, val loss: 0.12877051532268524\n",
      "Epoch 50, train loss: 0.009629478678107262, val loss: 0.08081387728452682\n",
      "Epoch 60, train loss: 0.0003046802303288132, val loss: 0.025595135986804962\n",
      "Epoch 70, train loss: 0.00082877412205562, val loss: 0.004101287107914686\n",
      "Epoch 80, train loss: 0.00019321950094308704, val loss: 0.005584742408245802\n",
      "Epoch 90, train loss: 0.00011846738925669342, val loss: 0.00043284305138513446\n",
      "Training time: 25.10231065750122\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.47716858983039856, val loss: 0.02391560934484005\n",
      "Epoch 10, train loss: 0.07719770073890686, val loss: 0.36510711908340454\n",
      "Epoch 20, train loss: 0.030942626297473907, val loss: 0.08424073457717896\n",
      "Epoch 30, train loss: 0.015427527949213982, val loss: 0.19734296202659607\n",
      "Epoch 40, train loss: 0.012255973182618618, val loss: 0.12189159542322159\n",
      "Epoch 50, train loss: 0.010407086461782455, val loss: 0.1287705898284912\n",
      "Epoch 60, train loss: 0.008516104891896248, val loss: 0.0922621339559555\n",
      "Epoch 70, train loss: 0.00597033416852355, val loss: 0.06844307482242584\n",
      "Epoch 80, train loss: 0.0024413717910647392, val loss: 0.02133416198194027\n",
      "Epoch 90, train loss: 9.23292973311618e-05, val loss: 0.0019374802941456437\n",
      "Training time: 25.424776792526245\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6425154209136963, val loss: 0.14378906786441803\n",
      "Epoch 10, train loss: 0.048818785697221756, val loss: 0.4188036620616913\n",
      "Epoch 20, train loss: 0.024816012009978294, val loss: 0.2946591377258301\n",
      "Epoch 30, train loss: 0.015409828163683414, val loss: 0.16247376799583435\n",
      "Epoch 40, train loss: 0.011851401068270206, val loss: 0.08893688023090363\n",
      "Epoch 50, train loss: 0.0002441646938677877, val loss: 0.042789582163095474\n",
      "Epoch 60, train loss: 0.0011789757991209626, val loss: 0.006963582709431648\n",
      "Epoch 70, train loss: 0.0004166924918536097, val loss: 0.012180529534816742\n",
      "Epoch 80, train loss: 0.00019234162755310535, val loss: 0.00046521989861503243\n",
      "Epoch 90, train loss: 0.00010930768621619791, val loss: 0.0025169900618493557\n",
      "Epoch 100, train loss: 8.898544911062345e-05, val loss: 0.000496549648232758\n",
      "Epoch 110, train loss: 7.941845251480117e-05, val loss: 0.0010191549081355333\n",
      "Epoch 120, train loss: 7.5014031608589e-05, val loss: 0.000522006128448993\n",
      "Epoch 130, train loss: 7.108074350981042e-05, val loss: 0.0006204336532391608\n",
      "Epoch 140, train loss: 6.813587970100343e-05, val loss: 0.00046783764264546335\n",
      "Epoch 150, train loss: 6.55958938295953e-05, val loss: 0.00047850579721853137\n",
      "Epoch 160, train loss: 6.339225365081802e-05, val loss: 0.0004175332433078438\n",
      "Epoch 170, train loss: 6.140754703665152e-05, val loss: 0.0004041203937958926\n",
      "Epoch 180, train loss: 5.9614027122734115e-05, val loss: 0.00038004000089131296\n",
      "Epoch 190, train loss: 5.7982662838185206e-05, val loss: 0.0003673383907880634\n",
      "Training time: 50.245662212371826\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5470215678215027, val loss: 0.040105655789375305\n",
      "Epoch 10, train loss: 0.04712223634123802, val loss: 0.3924110233783722\n",
      "Epoch 20, train loss: 0.025536587461829185, val loss: 0.10143687576055527\n",
      "Epoch 30, train loss: 0.013928528875112534, val loss: 0.18129241466522217\n",
      "Epoch 40, train loss: 0.012766901403665543, val loss: 0.14824305474758148\n",
      "Epoch 50, train loss: 0.01107406709343195, val loss: 0.12208253145217896\n",
      "Epoch 60, train loss: 0.008303046226501465, val loss: 0.0943952426314354\n",
      "Epoch 70, train loss: 0.004595423582941294, val loss: 0.051179371774196625\n",
      "Epoch 80, train loss: 0.00030738397617824376, val loss: 0.0012774162460118532\n",
      "Epoch 90, train loss: 0.000498360488563776, val loss: 0.0037448389921337366\n",
      "Epoch 100, train loss: 0.00019487985991872847, val loss: 0.0017060815589502454\n",
      "Epoch 110, train loss: 0.0001047151381499134, val loss: 0.0005988955381326377\n",
      "Epoch 120, train loss: 0.00010425670188851655, val loss: 0.0006561076152138412\n",
      "Epoch 130, train loss: 8.254400745499879e-05, val loss: 0.0004722938174381852\n",
      "Epoch 140, train loss: 8.305659866891801e-05, val loss: 0.00046653588651679456\n",
      "Epoch 150, train loss: 8.14653467386961e-05, val loss: 0.00044594937935471535\n",
      "Epoch 160, train loss: 7.998719956958666e-05, val loss: 0.0004456960887182504\n",
      "Epoch 170, train loss: 7.946763071231544e-05, val loss: 0.0004443657526280731\n",
      "Epoch 180, train loss: 7.88554098107852e-05, val loss: 0.0004387212684378028\n",
      "Epoch 190, train loss: 7.824165368219838e-05, val loss: 0.00044002404320053756\n",
      "Training time: 50.30681657791138\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6436176300048828, val loss: 0.5026065111160278\n",
      "Epoch 10, train loss: 0.06197480484843254, val loss: 0.05005054548382759\n",
      "Epoch 20, train loss: 0.02286515384912491, val loss: 0.06487387418746948\n",
      "Epoch 30, train loss: 0.009972224943339825, val loss: 0.039651963859796524\n",
      "Epoch 40, train loss: 0.0006711697787977755, val loss: 0.008758393116295338\n",
      "Epoch 50, train loss: 0.0011708661913871765, val loss: 0.004969223868101835\n",
      "Epoch 60, train loss: 0.0003848352935165167, val loss: 0.0010147126158699393\n",
      "Epoch 70, train loss: 9.632498404243961e-05, val loss: 0.0004848228127229959\n",
      "Epoch 80, train loss: 9.538089216221124e-05, val loss: 0.0005115100066177547\n",
      "Epoch 90, train loss: 4.932130832457915e-05, val loss: 0.0002647323126439005\n",
      "Training time: 39.4725296497345\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5526145100593567, val loss: 0.02922411449253559\n",
      "Epoch 10, train loss: 0.048134852200746536, val loss: 0.21775253117084503\n",
      "Epoch 20, train loss: 0.025126220658421516, val loss: 0.08219333738088608\n",
      "Epoch 30, train loss: 0.015442493371665478, val loss: 0.1565321981906891\n",
      "Epoch 40, train loss: 0.01085616834461689, val loss: 0.08499877899885178\n",
      "Epoch 50, train loss: 0.007994432002305984, val loss: 0.0951816663146019\n",
      "Epoch 60, train loss: 0.0054696956649422646, val loss: 0.0584394596517086\n",
      "Epoch 70, train loss: 0.002936943434178829, val loss: 0.028765268623828888\n",
      "Epoch 80, train loss: 0.0004584391135722399, val loss: 0.002595428377389908\n",
      "Epoch 90, train loss: 0.0003700458037201315, val loss: 0.004087462555617094\n",
      "Training time: 40.34808850288391\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5313113331794739, val loss: 0.4653064012527466\n",
      "Epoch 10, train loss: 0.04767022654414177, val loss: 0.08873018622398376\n",
      "Epoch 20, train loss: 0.015833990648388863, val loss: 0.09384404122829437\n",
      "Epoch 30, train loss: 0.005604106932878494, val loss: 0.05369231104850769\n",
      "Epoch 40, train loss: 0.001914793043397367, val loss: 0.0004251872596796602\n",
      "Epoch 50, train loss: 0.0009843982988968492, val loss: 0.0005752724828198552\n",
      "Epoch 60, train loss: 0.00022220227401703596, val loss: 0.0016120047075673938\n",
      "Epoch 70, train loss: 0.00016743653395678848, val loss: 0.000634022697340697\n",
      "Epoch 80, train loss: 4.887067916570231e-05, val loss: 0.0002701783087104559\n",
      "Epoch 90, train loss: 4.2660092731239274e-05, val loss: 0.00023759753094054759\n",
      "Epoch 100, train loss: 4.1394581785425544e-05, val loss: 0.00022988105774857104\n",
      "Epoch 110, train loss: 3.869123611366376e-05, val loss: 0.00024786009453237057\n",
      "Epoch 120, train loss: 3.702191315824166e-05, val loss: 0.00022403558250516653\n",
      "Epoch 130, train loss: 3.624394958023913e-05, val loss: 0.0002333727607037872\n",
      "Epoch 140, train loss: 3.541731712175533e-05, val loss: 0.00022813100076746196\n",
      "Epoch 150, train loss: 3.464331166469492e-05, val loss: 0.0002299911284353584\n",
      "Epoch 160, train loss: 3.400967761990614e-05, val loss: 0.0002294565929332748\n",
      "Epoch 170, train loss: 3.340654438943602e-05, val loss: 0.00022945801902096719\n",
      "Epoch 180, train loss: 3.2851177820703015e-05, val loss: 0.0002319881896255538\n",
      "Epoch 190, train loss: 3.233825555071235e-05, val loss: 0.00023266552307177335\n",
      "Training time: 82.50569558143616\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7202253937721252, val loss: 0.08424252271652222\n",
      "Epoch 10, train loss: 0.036060530692338943, val loss: 0.14481885731220245\n",
      "Epoch 20, train loss: 0.022606423124670982, val loss: 0.06711264699697495\n",
      "Epoch 30, train loss: 0.013205979950726032, val loss: 0.12282273173332214\n",
      "Epoch 40, train loss: 0.009745472110807896, val loss: 0.07242665439844131\n",
      "Epoch 50, train loss: 0.0073733762837946415, val loss: 0.08632002770900726\n",
      "Epoch 60, train loss: 0.00581062538549304, val loss: 0.06192620098590851\n",
      "Epoch 70, train loss: 0.004529442638158798, val loss: 0.05206092819571495\n",
      "Epoch 80, train loss: 0.003371274098753929, val loss: 0.03956947103142738\n",
      "Epoch 90, train loss: 0.002175218891352415, val loss: 0.022697463631629944\n",
      "Epoch 100, train loss: 0.0010039719054475427, val loss: 0.009424898773431778\n",
      "Epoch 110, train loss: 0.00020363130897749215, val loss: 0.0011923020938411355\n",
      "Epoch 120, train loss: 8.200737647712231e-05, val loss: 0.000963491213042289\n",
      "Epoch 130, train loss: 9.943730401573703e-05, val loss: 0.0009752531186677516\n",
      "Epoch 140, train loss: 6.0312879213597625e-05, val loss: 0.00037413445534184575\n",
      "Epoch 150, train loss: 6.359286635415629e-05, val loss: 0.000352655682945624\n",
      "Epoch 160, train loss: 6.050494994269684e-05, val loss: 0.0003525528300087899\n",
      "Epoch 170, train loss: 5.92571341258008e-05, val loss: 0.0003865534672513604\n",
      "Epoch 180, train loss: 5.913518543820828e-05, val loss: 0.0003883971075993031\n",
      "Epoch 190, train loss: 5.865715866093524e-05, val loss: 0.00036997845745645463\n",
      "Training time: 82.01510763168335\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.5819313526153564, val loss: 0.4575707018375397\n",
      "Epoch 10, train loss: 0.01812145859003067, val loss: 0.0775531530380249\n",
      "Epoch 20, train loss: 0.015161301009356976, val loss: 0.04391475021839142\n",
      "Epoch 30, train loss: 0.006964513100683689, val loss: 0.00038049850263632834\n",
      "Epoch 40, train loss: 0.00018733982869889587, val loss: 0.001949979574419558\n",
      "Epoch 50, train loss: 0.00014533485227730125, val loss: 0.00041709764627739787\n",
      "Epoch 60, train loss: 0.00022778759011998773, val loss: 0.00032483908580616117\n",
      "Epoch 70, train loss: 0.00020540347031783313, val loss: 0.0003314661153126508\n",
      "Epoch 80, train loss: 9.142280032392591e-05, val loss: 0.00032836146419867873\n",
      "Epoch 90, train loss: 9.041546582011506e-05, val loss: 0.0004267061594873667\n",
      "Training time: 64.89959907531738\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5422631502151489, val loss: 0.020257893949747086\n",
      "Epoch 10, train loss: 0.017132803797721863, val loss: 0.09980472922325134\n",
      "Epoch 20, train loss: 0.016788115724921227, val loss: 0.22281962633132935\n",
      "Epoch 30, train loss: 0.015534290112555027, val loss: 0.09877803176641464\n",
      "Epoch 40, train loss: 0.010659737512469292, val loss: 0.11837922036647797\n",
      "Epoch 50, train loss: 0.0046791015192866325, val loss: 0.050990618765354156\n",
      "Epoch 60, train loss: 0.0001606999576324597, val loss: 0.0006413221126422286\n",
      "Epoch 70, train loss: 0.00024802552070468664, val loss: 0.0020395873580127954\n",
      "Epoch 80, train loss: 0.00030034934752620757, val loss: 0.001382146612741053\n",
      "Epoch 90, train loss: 8.214623085223138e-05, val loss: 0.0007635064539499581\n",
      "Training time: 65.8698501586914\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5675380825996399, val loss: 1.109321117401123\n",
      "Epoch 10, train loss: 0.029699724167585373, val loss: 0.26624611020088196\n",
      "Epoch 20, train loss: 0.016895102337002754, val loss: 0.18249912559986115\n",
      "Epoch 30, train loss: 0.009633518755435944, val loss: 0.09523797780275345\n",
      "Epoch 40, train loss: 0.0008244194905273616, val loss: 0.0022043189965188503\n",
      "Epoch 50, train loss: 0.00039874407229945064, val loss: 0.0005161729059182107\n",
      "Epoch 60, train loss: 0.000401422061258927, val loss: 0.003949425183236599\n",
      "Epoch 70, train loss: 0.0001201063278131187, val loss: 0.0006591214914806187\n",
      "Epoch 80, train loss: 6.338644016068429e-05, val loss: 0.0004660289559978992\n",
      "Epoch 90, train loss: 7.12860855855979e-05, val loss: 0.0005786194815300405\n",
      "Epoch 100, train loss: 6.749964086338878e-05, val loss: 0.0003258862707298249\n",
      "Epoch 110, train loss: 6.272396421991289e-05, val loss: 0.0004954884643666446\n",
      "Epoch 120, train loss: 6.08948030276224e-05, val loss: 0.00038528419099748135\n",
      "Epoch 130, train loss: 6.035459227859974e-05, val loss: 0.0004112401802558452\n",
      "Epoch 140, train loss: 5.9849146055057645e-05, val loss: 0.0004127349820919335\n",
      "Epoch 150, train loss: 5.933635839028284e-05, val loss: 0.0003973259008489549\n",
      "Epoch 160, train loss: 5.8809058828046545e-05, val loss: 0.0004075840406585485\n",
      "Epoch 170, train loss: 5.8295016060583293e-05, val loss: 0.00039693433791399\n",
      "Epoch 180, train loss: 5.794992466690019e-05, val loss: 0.00040596918552182615\n",
      "Epoch 190, train loss: 0.00037813905510120094, val loss: 0.0009816245874390006\n",
      "Training time: 133.29200887680054\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6747620105743408, val loss: 0.04826625436544418\n",
      "Epoch 10, train loss: 0.01461421512067318, val loss: 0.09894024580717087\n",
      "Epoch 20, train loss: 0.015858318656682968, val loss: 0.2241145670413971\n",
      "Epoch 30, train loss: 0.0161391980946064, val loss: 0.09957966208457947\n",
      "Epoch 40, train loss: 0.012162650004029274, val loss: 0.14094410836696625\n",
      "Epoch 50, train loss: 0.007575199007987976, val loss: 0.07437705248594284\n",
      "Epoch 60, train loss: 0.0033820115495473146, val loss: 0.026664502918720245\n",
      "Epoch 70, train loss: 0.0002921026898548007, val loss: 0.0018152303528040648\n",
      "Epoch 80, train loss: 0.00033117856946773827, val loss: 0.001987751107662916\n",
      "Epoch 90, train loss: 0.00016937874897848815, val loss: 0.0010773970279842615\n",
      "Epoch 100, train loss: 9.724485425977036e-05, val loss: 0.0004403512575663626\n",
      "Epoch 110, train loss: 9.718287037685513e-05, val loss: 0.0008304334478452802\n",
      "Epoch 120, train loss: 7.866964006097987e-05, val loss: 0.0004488633421715349\n",
      "Epoch 130, train loss: 7.818006270099431e-05, val loss: 0.00044527160935103893\n",
      "Epoch 140, train loss: 7.684969750698656e-05, val loss: 0.0005237287259660661\n",
      "Epoch 150, train loss: 7.570219895569608e-05, val loss: 0.00046488651423715055\n",
      "Epoch 160, train loss: 7.521684165112674e-05, val loss: 0.00045921863056719303\n",
      "Epoch 170, train loss: 7.463157089659944e-05, val loss: 0.00047026967513374984\n",
      "Epoch 180, train loss: 7.40671093808487e-05, val loss: 0.0004573040932882577\n",
      "Epoch 190, train loss: 7.353675027843565e-05, val loss: 0.0004539074143394828\n",
      "Training time: 130.28092193603516\n",
      "Tuning time: 2318.03467130661\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "tune_start_time = time.time()\n",
    "results = []\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "for model in models:\n",
    "    for h_dim in hidden_dim:\n",
    "        for n_layers in num_layers:\n",
    "            for n_epoch in num_epochs:\n",
    "                for lr in learning_rate:\n",
    "                    y_train_tune = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "                    y_val_tune = torch.from_numpy(y_val).type(torch.Tensor)\n",
    "                    print(f'Training {model.__name__} with hidden_dim={h_dim}, num_layers={n_layers}, num_epochs={n_epoch}, lr={lr}')\n",
    "                    model_instance = model(input_dim=input_dim, hidden_dim=h_dim, num_layers=n_layers, output_dim=output_dim).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimiser = torch.optim.Adam(model_instance.parameters(), lr=lr)\n",
    "                    train_loss, val_loss = train_model(model_instance, \n",
    "                                                       criterion, \n",
    "                                                       optimiser, \n",
    "                                                       x_train, \n",
    "                                                       y_train_tune, \n",
    "                                                       x_val = x_val, \n",
    "                                                       y_val = y_val_tune, \n",
    "                                                       num_epochs=n_epoch)\n",
    "                    \n",
    "                    # Check mse, save model if its the best\n",
    "                    mse_instance = np.mean(val_loss)\n",
    "                    \n",
    "                    if mse_instance < best_mse:\n",
    "                        best_mse = mse_instance\n",
    "                        best_model = model_instance\n",
    "                        \n",
    "                    results.append({\n",
    "                        'model': model.__name__,\n",
    "                        'hidden_dim': h_dim,\n",
    "                        'num_layers': n_layers,\n",
    "                        'num_epochs': n_epoch,\n",
    "                        'lr': lr,\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss\n",
    "                    })\n",
    "tune_time = time.time() - tune_start_time\n",
    "print(f'Tuning time: {tune_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>lr</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GRU</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RNN</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.029123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GRU</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.029760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.030552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  hidden_dim  num_layers  num_epochs     lr       mse\n",
       "58   GRU          64           2         200  0.010  0.020125\n",
       "66   GRU         128           2         200  0.010  0.020189\n",
       "19   RNN         128           2         200  0.001  0.029123\n",
       "54   GRU          32           3         200  0.010  0.029760\n",
       "70   GRU         128           3         200  0.010  0.030552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check best model\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['mse'] = results_df['val_loss'].apply(np.mean)\n",
    "clean_df = results_df.drop(columns=['train_loss', 'val_loss'])\n",
    "clean_df.sort_values(by='mse', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model values\n",
    "best_model_results = results_df.sort_values(by='mse', ascending=True).iloc[0]\n",
    "\n",
    "best_train_loss = best_model_results['train_loss']\n",
    "best_val_loss = best_model_results['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the best model\n",
    "plt.plot(best_train_loss, label='train loss')\n",
    "plt.plot(best_val_loss, label='val loss')\n",
    "plt.title('Best model loss graph')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model MSE: 0.0065952870063483715\n"
     ]
    }
   ],
   "source": [
    "y_test_best = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(x_test)\n",
    "    test_loss = criterion(test_outputs, y_test_best)\n",
    "    print(f'Best model MSE: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descale the data\n",
    "y_test_descaled = scaler.inverse_transform(y_test_best)\n",
    "y_test_pred = scaler.inverse_transform(test_outputs)\n",
    "\n",
    "plt.plot(y_test_descaled, label='True price')\n",
    "plt.plot(y_test_pred, label='Predicted price')\n",
    "plt.title('Best model prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
