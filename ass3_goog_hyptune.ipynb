{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>High</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adjusted Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19-08-2004</td>\n",
       "      <td>2.390042</td>\n",
       "      <td>2.490664</td>\n",
       "      <td>897427216</td>\n",
       "      <td>2.591785</td>\n",
       "      <td>2.499133</td>\n",
       "      <td>2.499133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20-08-2004</td>\n",
       "      <td>2.503118</td>\n",
       "      <td>2.515820</td>\n",
       "      <td>458857488</td>\n",
       "      <td>2.716817</td>\n",
       "      <td>2.697639</td>\n",
       "      <td>2.697639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23-08-2004</td>\n",
       "      <td>2.716070</td>\n",
       "      <td>2.758411</td>\n",
       "      <td>366857939</td>\n",
       "      <td>2.826406</td>\n",
       "      <td>2.724787</td>\n",
       "      <td>2.724787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24-08-2004</td>\n",
       "      <td>2.579581</td>\n",
       "      <td>2.770615</td>\n",
       "      <td>306396159</td>\n",
       "      <td>2.779581</td>\n",
       "      <td>2.611960</td>\n",
       "      <td>2.611960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25-08-2004</td>\n",
       "      <td>2.587302</td>\n",
       "      <td>2.614201</td>\n",
       "      <td>184645512</td>\n",
       "      <td>2.689918</td>\n",
       "      <td>2.640104</td>\n",
       "      <td>2.640104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Low      Open     Volume      High     Close  \\\n",
       "0  19-08-2004  2.390042  2.490664  897427216  2.591785  2.499133   \n",
       "1  20-08-2004  2.503118  2.515820  458857488  2.716817  2.697639   \n",
       "2  23-08-2004  2.716070  2.758411  366857939  2.826406  2.724787   \n",
       "3  24-08-2004  2.579581  2.770615  306396159  2.779581  2.611960   \n",
       "4  25-08-2004  2.587302  2.614201  184645512  2.689918  2.640104   \n",
       "\n",
       "   Adjusted Close  \n",
       "0        2.499133  \n",
       "1        2.697639  \n",
       "2        2.724787  \n",
       "3        2.611960  \n",
       "4        2.640104  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GOOG stock data\n",
    "\n",
    "path = 'data/stock_market_data/GOOG.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "price = data[['Close']].copy()\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (3215, 19, 1)\n",
      "y_train.shape = (3215, 1)\n",
      "x_val.shape = (459, 19, 1)\n",
      "y_val.shape = (459, 1)\n",
      "x_test.shape = (918, 19, 1)\n",
      "y_test.shape = (918, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and test set with sliding window method\n",
    "\n",
    "def split_data(stock, lookback, val_size=0.1):\n",
    "    data_raw = stock.to_numpy()  # Convert to numpy array\n",
    "    data = []\n",
    "\n",
    "    # Create sequences of length `lookback`\n",
    "    for index in range(len(data_raw) - lookback): \n",
    "        data.append(data_raw[index: index + lookback])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    test_set_size = int(np.round(0.2 * data.shape[0]))\n",
    "    val_set_size = int(np.round(val_size * data.shape[0]))\n",
    "    train_set_size = data.shape[0] - (test_set_size + val_set_size)\n",
    "\n",
    "    # Split into training, validation, and test sets\n",
    "    x_train = data[:train_set_size, :-1, :]\n",
    "    y_train = data[:train_set_size, -1, :]\n",
    "    \n",
    "    x_val = data[train_set_size:train_set_size+val_set_size, :-1, :]\n",
    "    y_val = data[train_set_size:train_set_size+val_set_size, -1, :]\n",
    "    \n",
    "    x_test = data[train_set_size+val_set_size:, :-1, :]\n",
    "    y_test = data[train_set_size+val_set_size:, -1, :]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "lookback = 20 # choose sequence length\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = split_data(price, lookback)\n",
    "\n",
    "# Check shapes\n",
    "print('x_train.shape =', x_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('x_val.shape =', x_val.shape)\n",
    "print('y_val.shape =', y_val.shape)\n",
    "print('x_test.shape =', x_test.shape)\n",
    "print('y_test.shape =', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "import time\n",
    "\n",
    "def train_model(model, \n",
    "                criterion, \n",
    "                optimiser, \n",
    "                x_train, y_train,\n",
    "                x_val=None, y_val=None, \n",
    "                num_epochs = 100):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(x_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        if x_val is not None:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(x_val)\n",
    "                val_epoch_loss = criterion(val_outputs, y_val)\n",
    "                val_loss.append(val_epoch_loss.item())\n",
    "        else:\n",
    "            val_loss.append(None)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            if x_val is not None:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}, val loss: {val_epoch_loss.item()}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch}, train loss: {loss.item()}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training time: {training_time}')\n",
    "    \n",
    "    return train_loss, val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RNN\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "x_val = torch.from_numpy(x_val).type(torch.Tensor)\n",
    "\n",
    "# Hyperparameters defining\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "# Tunable hyperparameters\n",
    "models = [RNN, LSTM, GRU]\n",
    "hidden_dim = [32, 64, 128]\n",
    "num_layers = [2, 3]\n",
    "num_epochs = [100, 200]\n",
    "learning_rate = [0.01, 0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.843235194683075, val loss: 0.022998029366135597\n",
      "Epoch 10, train loss: 0.037770964205265045, val loss: 0.3678644001483917\n",
      "Epoch 20, train loss: 0.01745638996362686, val loss: 0.22473447024822235\n",
      "Epoch 30, train loss: 0.016726475208997726, val loss: 0.151933491230011\n",
      "Epoch 40, train loss: 0.010581498965620995, val loss: 0.10869301855564117\n",
      "Epoch 50, train loss: 0.017108513042330742, val loss: 0.07912709563970566\n",
      "Epoch 60, train loss: 0.0058102477341890335, val loss: 0.017235904932022095\n",
      "Epoch 70, train loss: 0.0005928181344643235, val loss: 0.012719173915684223\n",
      "Epoch 80, train loss: 0.000642922124825418, val loss: 0.005425732582807541\n",
      "Epoch 90, train loss: 0.00031758748809807, val loss: 0.007941589690744877\n",
      "Training time: 6.202880144119263\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.2955033779144287, val loss: 0.003509025787934661\n",
      "Epoch 10, train loss: 0.0495731495320797, val loss: 0.3758220672607422\n",
      "Epoch 20, train loss: 0.01850910484790802, val loss: 0.1281009465456009\n",
      "Epoch 30, train loss: 0.015731757506728172, val loss: 0.16668902337551117\n",
      "Epoch 40, train loss: 0.015676358714699745, val loss: 0.18916699290275574\n",
      "Epoch 50, train loss: 0.014134960249066353, val loss: 0.13840442895889282\n",
      "Epoch 60, train loss: 0.01266617514193058, val loss: 0.15189193189144135\n",
      "Epoch 70, train loss: 0.011287248693406582, val loss: 0.12104924768209457\n",
      "Epoch 80, train loss: 0.009662045165896416, val loss: 0.1083369255065918\n",
      "Epoch 90, train loss: 0.007558696437627077, val loss: 0.07661101967096329\n",
      "Training time: 4.49944281578064\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 1.0351872444152832, val loss: 0.003150567412376404\n",
      "Epoch 10, train loss: 0.0179433673620224, val loss: 0.29840824007987976\n",
      "Epoch 20, train loss: 0.015609078109264374, val loss: 0.21289527416229248\n",
      "Epoch 30, train loss: 0.012480170466005802, val loss: 0.14133839309215546\n",
      "Epoch 40, train loss: 0.0008181131561286747, val loss: 0.06195937097072601\n",
      "Epoch 50, train loss: 0.0071064867079257965, val loss: 0.03392602875828743\n",
      "Epoch 60, train loss: 0.0019584917463362217, val loss: 0.004128343425691128\n",
      "Epoch 70, train loss: 0.0002560963621363044, val loss: 0.0010223686695098877\n",
      "Epoch 80, train loss: 0.00032505838316865265, val loss: 0.0008059308747760952\n",
      "Epoch 90, train loss: 0.00015471200458705425, val loss: 0.0038622641004621983\n",
      "Epoch 100, train loss: 0.00014615533291362226, val loss: 0.001696342253126204\n",
      "Epoch 110, train loss: 0.00011404554243199527, val loss: 0.003630775725468993\n",
      "Epoch 120, train loss: 9.86435916274786e-05, val loss: 0.002272249897941947\n",
      "Epoch 130, train loss: 8.960488776210696e-05, val loss: 0.0024480591528117657\n",
      "Epoch 140, train loss: 8.467079169349745e-05, val loss: 0.002070701215416193\n",
      "Epoch 150, train loss: 8.009418525034562e-05, val loss: 0.001985697541385889\n",
      "Epoch 160, train loss: 7.610718603245914e-05, val loss: 0.0018126103095710278\n",
      "Epoch 170, train loss: 7.257444667629898e-05, val loss: 0.0016546192346140742\n",
      "Epoch 180, train loss: 6.939855666132644e-05, val loss: 0.0015533989062532783\n",
      "Epoch 190, train loss: 6.648046110058203e-05, val loss: 0.0014213577378541231\n",
      "Training time: 9.337689876556396\n",
      "Training RNN with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.4871601164340973, val loss: 0.02989615499973297\n",
      "Epoch 10, train loss: 0.021279212087392807, val loss: 0.18963804841041565\n",
      "Epoch 20, train loss: 0.03318730741739273, val loss: 0.27906370162963867\n",
      "Epoch 30, train loss: 0.021551992744207382, val loss: 0.12080489844083786\n",
      "Epoch 40, train loss: 0.014935249462723732, val loss: 0.1728806495666504\n",
      "Epoch 50, train loss: 0.015129570849239826, val loss: 0.18578192591667175\n",
      "Epoch 60, train loss: 0.013672135770320892, val loss: 0.14174701273441315\n",
      "Epoch 70, train loss: 0.012381434440612793, val loss: 0.14719904959201813\n",
      "Epoch 80, train loss: 0.011088402010500431, val loss: 0.12525002658367157\n",
      "Epoch 90, train loss: 0.009377414360642433, val loss: 0.10462816804647446\n",
      "Epoch 100, train loss: 0.006959465332329273, val loss: 0.07155732810497284\n",
      "Epoch 110, train loss: 0.0034799242857843637, val loss: 0.02936689369380474\n",
      "Epoch 120, train loss: 0.0001236219541169703, val loss: 0.001213600393384695\n",
      "Epoch 130, train loss: 0.0003512005787342787, val loss: 0.005145874340087175\n",
      "Epoch 140, train loss: 0.00017545277660246938, val loss: 0.00032815412851050496\n",
      "Epoch 150, train loss: 9.200710337609053e-05, val loss: 0.0006569698452949524\n",
      "Epoch 160, train loss: 9.635626338422298e-05, val loss: 0.001978222280740738\n",
      "Epoch 170, train loss: 7.807205838616937e-05, val loss: 0.0008069302421063185\n",
      "Epoch 180, train loss: 7.82723946031183e-05, val loss: 0.0008102542487904429\n",
      "Epoch 190, train loss: 7.665620069019496e-05, val loss: 0.0011149131460115314\n",
      "Training time: 6.498689889907837\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.9802570939064026, val loss: 0.2076566070318222\n",
      "Epoch 10, train loss: 0.0340745784342289, val loss: 0.29342231154441833\n",
      "Epoch 20, train loss: 0.025759445503354073, val loss: 0.29938599467277527\n",
      "Epoch 30, train loss: 0.020237213000655174, val loss: 0.2702573239803314\n",
      "Epoch 40, train loss: 0.019131558015942574, val loss: 0.24178539216518402\n",
      "Epoch 50, train loss: 0.018847858533263206, val loss: 0.22734805941581726\n",
      "Epoch 60, train loss: 0.018241887912154198, val loss: 0.21813595294952393\n",
      "Epoch 70, train loss: 0.016537442803382874, val loss: 0.19373494386672974\n",
      "Epoch 80, train loss: 0.0010949356947094202, val loss: 0.05233268067240715\n",
      "Epoch 90, train loss: 0.027228865772485733, val loss: 0.383012980222702\n",
      "Training time: 4.57608699798584\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3168622553348541, val loss: 0.0034715773072093725\n",
      "Epoch 10, train loss: 0.055029116570949554, val loss: 0.38586747646331787\n",
      "Epoch 20, train loss: 0.023383816704154015, val loss: 0.12528374791145325\n",
      "Epoch 30, train loss: 0.01595623977482319, val loss: 0.2027258723974228\n",
      "Epoch 40, train loss: 0.015234951861202717, val loss: 0.17831546068191528\n",
      "Epoch 50, train loss: 0.01386998314410448, val loss: 0.1452634483575821\n",
      "Epoch 60, train loss: 0.011059651151299477, val loss: 0.1209937334060669\n",
      "Epoch 70, train loss: 0.00536020752042532, val loss: 0.04615246132016182\n",
      "Epoch 80, train loss: 0.001968222903087735, val loss: 0.012362104840576649\n",
      "Epoch 90, train loss: 0.0005713585997000337, val loss: 0.0033303461968898773\n",
      "Training time: 4.535910606384277\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6040160059928894, val loss: 0.12398409843444824\n",
      "Epoch 10, train loss: 0.021275145933032036, val loss: 0.20481650531291962\n",
      "Epoch 20, train loss: 0.019283892586827278, val loss: 0.20687952637672424\n",
      "Epoch 30, train loss: 0.01777675934135914, val loss: 0.18801236152648926\n",
      "Epoch 40, train loss: 0.015292659401893616, val loss: 0.1438542753458023\n",
      "Epoch 50, train loss: 0.0796370878815651, val loss: 0.0037580900825560093\n",
      "Epoch 60, train loss: 0.01979636400938034, val loss: 0.16363182663917542\n",
      "Epoch 70, train loss: 0.015984920784831047, val loss: 0.16234947741031647\n",
      "Epoch 80, train loss: 0.010277030989527702, val loss: 0.1087447926402092\n",
      "Epoch 90, train loss: 0.0007054861634969711, val loss: 0.005413643084466457\n",
      "Epoch 100, train loss: 0.007888219319283962, val loss: 0.02398598939180374\n",
      "Epoch 110, train loss: 0.0023827252443879843, val loss: 0.005104790907353163\n",
      "Epoch 120, train loss: 0.0007985298871062696, val loss: 0.0004508925194386393\n",
      "Epoch 130, train loss: 0.0005152239464223385, val loss: 0.003195373807102442\n",
      "Epoch 140, train loss: 0.00021872392971999943, val loss: 0.0004694448143709451\n",
      "Epoch 150, train loss: 9.322651021648198e-05, val loss: 0.002393830567598343\n",
      "Epoch 160, train loss: 6.98588410159573e-05, val loss: 0.0011801945511251688\n",
      "Epoch 170, train loss: 6.0760339692933485e-05, val loss: 0.001976208295673132\n",
      "Epoch 180, train loss: 5.1867209549527615e-05, val loss: 0.0015827735187485814\n",
      "Epoch 190, train loss: 4.8885063733905554e-05, val loss: 0.002009779214859009\n",
      "Training time: 9.017334222793579\n",
      "Training RNN with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8076836466789246, val loss: 0.09696891158819199\n",
      "Epoch 10, train loss: 0.05304420739412308, val loss: 0.47978776693344116\n",
      "Epoch 20, train loss: 0.018025489524006844, val loss: 0.18977029621601105\n",
      "Epoch 30, train loss: 0.026555486023426056, val loss: 0.1374712884426117\n",
      "Epoch 40, train loss: 0.019081922248005867, val loss: 0.24228979647159576\n",
      "Epoch 50, train loss: 0.016002921387553215, val loss: 0.17736512422561646\n",
      "Epoch 60, train loss: 0.015285360626876354, val loss: 0.1637902855873108\n",
      "Epoch 70, train loss: 0.013753088191151619, val loss: 0.16030216217041016\n",
      "Epoch 80, train loss: 0.01111368928104639, val loss: 0.10951850563287735\n",
      "Epoch 90, train loss: 0.005307367071509361, val loss: 0.03641191124916077\n",
      "Epoch 100, train loss: 0.002085252897813916, val loss: 0.015553954988718033\n",
      "Epoch 110, train loss: 0.0006997092277742922, val loss: 0.0027710217982530594\n",
      "Epoch 120, train loss: 0.00019795015396084636, val loss: 0.001413620077073574\n",
      "Epoch 130, train loss: 0.00018463538435753435, val loss: 0.0005801041261292994\n",
      "Epoch 140, train loss: 0.0001870294363470748, val loss: 0.00043060342431999743\n",
      "Epoch 150, train loss: 0.00017204567848239094, val loss: 0.000713591929525137\n",
      "Epoch 160, train loss: 0.0001586557482369244, val loss: 0.0004137172654736787\n",
      "Epoch 170, train loss: 0.000148817416629754, val loss: 0.0004970252048224211\n",
      "Epoch 180, train loss: 0.0001413755671819672, val loss: 0.00041199394036084414\n",
      "Epoch 190, train loss: 0.00013507115363609046, val loss: 0.00041635631350800395\n",
      "Training time: 9.145306587219238\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7778453826904297, val loss: 0.1995227336883545\n",
      "Epoch 10, train loss: 0.018185770139098167, val loss: 0.10418212413787842\n",
      "Epoch 20, train loss: 0.1558256447315216, val loss: 0.24099941551685333\n",
      "Epoch 30, train loss: 0.029612736776471138, val loss: 0.2479867935180664\n",
      "Epoch 40, train loss: 0.020537804812192917, val loss: 0.21418209373950958\n",
      "Epoch 50, train loss: 0.017659317702054977, val loss: 0.17105384171009064\n",
      "Epoch 60, train loss: 0.01293172687292099, val loss: 0.1337335854768753\n",
      "Epoch 70, train loss: 0.004951067268848419, val loss: 0.04597938805818558\n",
      "Epoch 80, train loss: 0.024957852438092232, val loss: 0.3827865719795227\n",
      "Epoch 90, train loss: 0.027155112475156784, val loss: 0.3153547942638397\n",
      "Training time: 5.502933502197266\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7569235563278198, val loss: 0.07756049185991287\n",
      "Epoch 10, train loss: 0.09565172344446182, val loss: 0.4325343370437622\n",
      "Epoch 20, train loss: 0.0356346033513546, val loss: 0.08132397383451462\n",
      "Epoch 30, train loss: 0.02027921937406063, val loss: 0.2263232320547104\n",
      "Epoch 40, train loss: 0.014453633688390255, val loss: 0.11152300238609314\n",
      "Epoch 50, train loss: 0.011317500844597816, val loss: 0.13720080256462097\n",
      "Epoch 60, train loss: 0.008048927411437035, val loss: 0.06705180555582047\n",
      "Epoch 70, train loss: 0.00173875130712986, val loss: 0.0032523430418223143\n",
      "Epoch 80, train loss: 0.0006081590545363724, val loss: 0.004832722712308168\n",
      "Epoch 90, train loss: 0.0005055502406321466, val loss: 0.0007130552548915148\n",
      "Training time: 5.486148834228516\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.8660638332366943, val loss: 0.08657649904489517\n",
      "Epoch 10, train loss: 0.02139650285243988, val loss: 0.483027845621109\n",
      "Epoch 20, train loss: 0.32441744208335876, val loss: 0.1342671662569046\n",
      "Epoch 30, train loss: 0.04897818714380264, val loss: 0.2995888888835907\n",
      "Epoch 40, train loss: 0.028740370646119118, val loss: 0.3546258807182312\n",
      "Epoch 50, train loss: 0.023946624249219894, val loss: 0.16850651800632477\n",
      "Epoch 60, train loss: 0.019073491916060448, val loss: 0.2552136778831482\n",
      "Epoch 70, train loss: 0.01905645802617073, val loss: 0.22303564846515656\n",
      "Epoch 80, train loss: 0.017074808478355408, val loss: 0.21217837929725647\n",
      "Epoch 90, train loss: 0.015079853124916553, val loss: 0.18513086438179016\n",
      "Epoch 100, train loss: 0.01769254170358181, val loss: 0.32286134362220764\n",
      "Epoch 110, train loss: 0.016816258430480957, val loss: 0.2589528262615204\n",
      "Epoch 120, train loss: 0.016332505270838737, val loss: 0.17072373628616333\n",
      "Epoch 130, train loss: 0.013506881892681122, val loss: 0.17239363491535187\n",
      "Epoch 140, train loss: 0.06324858963489532, val loss: 0.14914575219154358\n",
      "Epoch 150, train loss: 0.02627183124423027, val loss: 0.02332368865609169\n",
      "Epoch 160, train loss: 0.2279827743768692, val loss: 0.46457937359809875\n",
      "Epoch 170, train loss: 0.026141004636883736, val loss: 0.14391455054283142\n",
      "Epoch 180, train loss: 0.02020183391869068, val loss: 0.18675827980041504\n",
      "Epoch 190, train loss: 0.019692113623023033, val loss: 0.21539969742298126\n",
      "Training time: 11.022291660308838\n",
      "Training RNN with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8588230609893799, val loss: 0.09816024452447891\n",
      "Epoch 10, train loss: 0.10060906410217285, val loss: 0.4689924120903015\n",
      "Epoch 20, train loss: 0.040409255772829056, val loss: 0.09671460837125778\n",
      "Epoch 30, train loss: 0.023183561861515045, val loss: 0.2625173330307007\n",
      "Epoch 40, train loss: 0.017324727028608322, val loss: 0.14339092373847961\n",
      "Epoch 50, train loss: 0.015038197860121727, val loss: 0.19137679040431976\n",
      "Epoch 60, train loss: 0.013905789703130722, val loss: 0.14851577579975128\n",
      "Epoch 70, train loss: 0.012819867581129074, val loss: 0.15519337356090546\n",
      "Epoch 80, train loss: 0.011533945798873901, val loss: 0.12697209417819977\n",
      "Epoch 90, train loss: 0.009803535416722298, val loss: 0.1106673926115036\n",
      "Epoch 100, train loss: 0.007263114210218191, val loss: 0.0775725468993187\n",
      "Epoch 110, train loss: 0.002803757321089506, val loss: 0.019215527921915054\n",
      "Epoch 120, train loss: 0.0012307718861848116, val loss: 0.0190249215811491\n",
      "Epoch 130, train loss: 0.00037235597847029567, val loss: 0.0013593833427876234\n",
      "Epoch 140, train loss: 8.718622848391533e-05, val loss: 0.0018378527602180839\n",
      "Epoch 150, train loss: 9.545293869450688e-05, val loss: 0.0015493982937186956\n",
      "Epoch 160, train loss: 9.862918523140252e-05, val loss: 0.0006341810803860426\n",
      "Epoch 170, train loss: 8.822062227409333e-05, val loss: 0.0016788769280537963\n",
      "Epoch 180, train loss: 8.272261766251177e-05, val loss: 0.0009065641788765788\n",
      "Epoch 190, train loss: 8.088073082035407e-05, val loss: 0.0011575075332075357\n",
      "Training time: 15.600922584533691\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.9376111626625061, val loss: 0.1269005984067917\n",
      "Epoch 10, train loss: 0.9892992973327637, val loss: 0.47503721714019775\n",
      "Epoch 20, train loss: 0.09049805253744125, val loss: 0.11219180375337601\n",
      "Epoch 30, train loss: 0.019428381696343422, val loss: 0.24609124660491943\n",
      "Epoch 40, train loss: 0.020061202347278595, val loss: 0.2410697638988495\n",
      "Epoch 50, train loss: 0.0196515005081892, val loss: 0.24452723562717438\n",
      "Epoch 60, train loss: 0.018722599372267723, val loss: 0.21300722658634186\n",
      "Epoch 70, train loss: 0.01665009930729866, val loss: 0.20082268118858337\n",
      "Epoch 80, train loss: 0.02750682644546032, val loss: 0.28319594264030457\n",
      "Epoch 90, train loss: 0.022609099745750427, val loss: 0.2528247833251953\n",
      "Training time: 8.561525821685791\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3593968451023102, val loss: 0.010322940535843372\n",
      "Epoch 10, train loss: 0.024437911808490753, val loss: 0.12151593714952469\n",
      "Epoch 20, train loss: 0.022155005484819412, val loss: 0.28418663144111633\n",
      "Epoch 30, train loss: 0.018952002748847008, val loss: 0.15894639492034912\n",
      "Epoch 40, train loss: 0.016469353809952736, val loss: 0.2065032720565796\n",
      "Epoch 50, train loss: 0.01323540136218071, val loss: 0.13308514654636383\n",
      "Epoch 60, train loss: 0.00564114935696125, val loss: 0.03269599378108978\n",
      "Epoch 70, train loss: 0.00021298331557773054, val loss: 0.0019280918641015887\n",
      "Epoch 80, train loss: 0.0004961900995112956, val loss: 0.0004540139634627849\n",
      "Epoch 90, train loss: 0.00021292314340826124, val loss: 0.00472675496712327\n",
      "Training time: 8.336411952972412\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6613442301750183, val loss: 0.33367031812667847\n",
      "Epoch 10, train loss: 0.046545904129743576, val loss: 0.6166057586669922\n",
      "Epoch 20, train loss: 0.021164514124393463, val loss: 0.18276748061180115\n",
      "Epoch 30, train loss: 0.0192494485527277, val loss: 0.26477140188217163\n",
      "Epoch 40, train loss: 0.00829588808119297, val loss: 0.07780902832746506\n",
      "Epoch 50, train loss: 0.052851200103759766, val loss: 0.17298346757888794\n",
      "Epoch 60, train loss: 0.019890306517481804, val loss: 0.34867194294929504\n",
      "Epoch 70, train loss: 0.020704589784145355, val loss: 0.2552776038646698\n",
      "Epoch 80, train loss: 0.019924214109778404, val loss: 0.23149371147155762\n",
      "Epoch 90, train loss: 0.019481198862195015, val loss: 0.22884027659893036\n",
      "Epoch 100, train loss: 0.01891331374645233, val loss: 0.22923940420150757\n",
      "Epoch 110, train loss: 0.022637147456407547, val loss: 0.22746685147285461\n",
      "Epoch 120, train loss: 0.019616473466157913, val loss: 0.17624139785766602\n",
      "Epoch 130, train loss: 0.020563703030347824, val loss: 0.2714124321937561\n",
      "Epoch 140, train loss: 0.019830066710710526, val loss: 0.2091958373785019\n",
      "Epoch 150, train loss: 0.019534219056367874, val loss: 0.24170370399951935\n",
      "Epoch 160, train loss: 0.01951506733894348, val loss: 0.22730399668216705\n",
      "Epoch 170, train loss: 0.019442882388830185, val loss: 0.2307596653699875\n",
      "Epoch 180, train loss: 0.019309839233756065, val loss: 0.2250409573316574\n",
      "Epoch 190, train loss: 0.019087975844740868, val loss: 0.22385618090629578\n",
      "Training time: 16.389635801315308\n",
      "Training RNN with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6129209399223328, val loss: 0.0226752869784832\n",
      "Epoch 10, train loss: 0.016015075147151947, val loss: 0.1144806370139122\n",
      "Epoch 20, train loss: 0.016598286107182503, val loss: 0.23407284915447235\n",
      "Epoch 30, train loss: 0.01575746200978756, val loss: 0.1309482902288437\n",
      "Epoch 40, train loss: 0.01363979559391737, val loss: 0.17305967211723328\n",
      "Epoch 50, train loss: 0.011485273949801922, val loss: 0.10695315897464752\n",
      "Epoch 60, train loss: 0.007367302663624287, val loss: 0.07330216467380524\n",
      "Epoch 70, train loss: 0.0008430094458162785, val loss: 0.0004217168898321688\n",
      "Epoch 80, train loss: 0.0003552063135430217, val loss: 0.0032811700366437435\n",
      "Epoch 90, train loss: 0.0004546898999251425, val loss: 0.0006188294501043856\n",
      "Epoch 100, train loss: 0.0001979554072022438, val loss: 0.005761719308793545\n",
      "Epoch 110, train loss: 0.00011989623453700915, val loss: 0.0009397694375365973\n",
      "Epoch 120, train loss: 0.00011248428199905902, val loss: 0.001836656592786312\n",
      "Epoch 130, train loss: 0.0001105741030187346, val loss: 0.002097404794767499\n",
      "Epoch 140, train loss: 0.00010786326311063021, val loss: 0.00144692393951118\n",
      "Epoch 150, train loss: 0.00010491155262570828, val loss: 0.0018656100146472454\n",
      "Epoch 160, train loss: 0.00010256950918119401, val loss: 0.0014997102553024888\n",
      "Epoch 170, train loss: 0.00010063505760626867, val loss: 0.0016146558336913586\n",
      "Epoch 180, train loss: 9.88704850897193e-05, val loss: 0.001491460483521223\n",
      "Epoch 190, train loss: 9.718299406813458e-05, val loss: 0.001475707977078855\n",
      "Training time: 16.908020973205566\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.42520320415496826, val loss: 2.7919483184814453\n",
      "Epoch 10, train loss: 0.5435065627098083, val loss: 4.760836124420166\n",
      "Epoch 20, train loss: 0.23598507046699524, val loss: 0.06023241579532623\n",
      "Epoch 30, train loss: 1.0002022981643677, val loss: 2.180654525756836\n",
      "Epoch 40, train loss: 0.3576711118221283, val loss: 0.006555963307619095\n",
      "Epoch 50, train loss: 0.13122649490833282, val loss: 0.5590997338294983\n",
      "Epoch 60, train loss: 0.03686189651489258, val loss: 0.17561426758766174\n",
      "Epoch 70, train loss: 0.01970924250781536, val loss: 0.1985575407743454\n",
      "Epoch 80, train loss: 0.02196996472775936, val loss: 0.29503652453422546\n",
      "Epoch 90, train loss: 0.02149425446987152, val loss: 0.19924630224704742\n",
      "Training time: 13.619348049163818\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.61153644323349, val loss: 0.011046900413930416\n",
      "Epoch 10, train loss: 0.04696539416909218, val loss: 0.05981769785284996\n",
      "Epoch 20, train loss: 0.016752803698182106, val loss: 0.163007453083992\n",
      "Epoch 30, train loss: 0.01029165182262659, val loss: 0.13078482449054718\n",
      "Epoch 40, train loss: 0.008667496033012867, val loss: 0.0669558048248291\n",
      "Epoch 50, train loss: 0.0025714016519486904, val loss: 0.014552980661392212\n",
      "Epoch 60, train loss: 0.0009997687302529812, val loss: 0.003822770668193698\n",
      "Epoch 70, train loss: 0.00048232710105367005, val loss: 0.0014122026041150093\n",
      "Epoch 80, train loss: 0.0001838395546656102, val loss: 0.0031708190217614174\n",
      "Epoch 90, train loss: 7.362123142229393e-05, val loss: 0.0002825864066835493\n",
      "Training time: 13.546663522720337\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.542538583278656, val loss: 4.5013957023620605\n",
      "Epoch 10, train loss: 0.11649038642644882, val loss: 0.20422352850437164\n",
      "Epoch 20, train loss: 0.12632179260253906, val loss: 0.01533588208258152\n",
      "Epoch 30, train loss: 0.031688421964645386, val loss: 0.22923995554447174\n",
      "Epoch 40, train loss: 0.04034523665904999, val loss: 0.37121087312698364\n",
      "Epoch 50, train loss: 0.02131224423646927, val loss: 0.3058035969734192\n",
      "Epoch 60, train loss: 0.019765693694353104, val loss: 0.24951398372650146\n",
      "Epoch 70, train loss: 0.02003379724919796, val loss: 0.23000217974185944\n",
      "Epoch 80, train loss: 0.01987011544406414, val loss: 0.22712180018424988\n",
      "Epoch 90, train loss: 0.019726641476154327, val loss: 0.2289305329322815\n",
      "Epoch 100, train loss: 0.019661808386445045, val loss: 0.23122148215770721\n",
      "Epoch 110, train loss: 0.019639264792203903, val loss: 0.2328205704689026\n",
      "Epoch 120, train loss: 0.01963386870920658, val loss: 0.23344175517559052\n",
      "Epoch 130, train loss: 0.01963205821812153, val loss: 0.23315417766571045\n",
      "Epoch 140, train loss: 0.01962866447865963, val loss: 0.23235541582107544\n",
      "Epoch 150, train loss: 0.019624562934041023, val loss: 0.23163734376430511\n",
      "Epoch 160, train loss: 0.01962079294025898, val loss: 0.23140108585357666\n",
      "Epoch 170, train loss: 0.01961667835712433, val loss: 0.23154401779174805\n",
      "Epoch 180, train loss: 0.01961207017302513, val loss: 0.23165345191955566\n",
      "Epoch 190, train loss: 0.019606929272413254, val loss: 0.23154689371585846\n",
      "Training time: 24.95039701461792\n",
      "Training RNN with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.46735942363739014, val loss: 0.003295404138043523\n",
      "Epoch 10, train loss: 0.04039879888296127, val loss: 0.06798781454563141\n",
      "Epoch 20, train loss: 0.013397755101323128, val loss: 0.1391267478466034\n",
      "Epoch 30, train loss: 0.010186506435275078, val loss: 0.13552676141262054\n",
      "Epoch 40, train loss: 0.007114645559340715, val loss: 0.0592159628868103\n",
      "Epoch 50, train loss: 0.0006437732954509556, val loss: 0.005001620855182409\n",
      "Epoch 60, train loss: 0.0004894016310572624, val loss: 0.005649859085679054\n",
      "Epoch 70, train loss: 0.00014439839287661016, val loss: 0.0024288196582347155\n",
      "Epoch 80, train loss: 6.379677506629378e-05, val loss: 0.000267011666437611\n",
      "Epoch 90, train loss: 6.761809345334768e-05, val loss: 0.0006865538889542222\n",
      "Epoch 100, train loss: 5.283022619551048e-05, val loss: 0.00031907152151688933\n",
      "Epoch 110, train loss: 4.807812729268335e-05, val loss: 0.0005155311082489789\n",
      "Epoch 120, train loss: 4.773501859745011e-05, val loss: 0.00038334893179126084\n",
      "Epoch 130, train loss: 4.752956738229841e-05, val loss: 0.00048112915828824043\n",
      "Epoch 140, train loss: 4.738469942822121e-05, val loss: 0.0004117059288546443\n",
      "Epoch 150, train loss: 4.718946365755983e-05, val loss: 0.0004590954340528697\n",
      "Epoch 160, train loss: 4.69851256639231e-05, val loss: 0.00042727592517621815\n",
      "Epoch 170, train loss: 4.682308281189762e-05, val loss: 0.00044018772314302623\n",
      "Epoch 180, train loss: 4.666427776101045e-05, val loss: 0.0004283216258045286\n",
      "Epoch 190, train loss: 4.6508186642313376e-05, val loss: 0.000426972983404994\n",
      "Training time: 27.220027208328247\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.48049697279930115, val loss: 6.32347297668457\n",
      "Epoch 10, train loss: 0.984054446220398, val loss: 0.10492324084043503\n",
      "Epoch 20, train loss: 0.026568055152893066, val loss: 0.1309097856283188\n",
      "Epoch 30, train loss: 0.10127405822277069, val loss: 0.47469794750213623\n",
      "Epoch 40, train loss: 0.039255306124687195, val loss: 0.3726058006286621\n",
      "Epoch 50, train loss: 0.019721126183867455, val loss: 0.26969069242477417\n",
      "Epoch 60, train loss: 0.021309932693839073, val loss: 0.20484620332717896\n",
      "Epoch 70, train loss: 0.019815992563962936, val loss: 0.21258588135242462\n",
      "Epoch 80, train loss: 0.01972358115017414, val loss: 0.23324339091777802\n",
      "Epoch 90, train loss: 0.019730327650904655, val loss: 0.23932600021362305\n",
      "Training time: 21.054072380065918\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.3397417366504669, val loss: 0.10010570287704468\n",
      "Epoch 10, train loss: 0.0178145170211792, val loss: 0.283579021692276\n",
      "Epoch 20, train loss: 0.02078191377222538, val loss: 0.1529218554496765\n",
      "Epoch 30, train loss: 0.01389314979314804, val loss: 0.11897613108158112\n",
      "Epoch 40, train loss: 0.00515402015298605, val loss: 0.021670427173376083\n",
      "Epoch 50, train loss: 0.0010534980101510882, val loss: 0.005582265090197325\n",
      "Epoch 60, train loss: 7.074323366396129e-05, val loss: 0.003327789017930627\n",
      "Epoch 70, train loss: 0.00012977755977772176, val loss: 0.0004449942207429558\n",
      "Epoch 80, train loss: 8.547514880774543e-05, val loss: 0.002211357932537794\n",
      "Epoch 90, train loss: 8.149534551193938e-05, val loss: 0.0003358932735864073\n",
      "Training time: 20.679784536361694\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.552649736404419, val loss: 1.672986626625061\n",
      "Epoch 10, train loss: 0.10578332096338272, val loss: 0.16734500229358673\n",
      "Epoch 20, train loss: 0.04256102442741394, val loss: 0.21111921966075897\n",
      "Epoch 30, train loss: 0.024530626833438873, val loss: 0.24884794652462006\n",
      "Epoch 40, train loss: 0.019659817218780518, val loss: 0.28321734070777893\n",
      "Epoch 50, train loss: 0.021162066608667374, val loss: 0.28044793009757996\n",
      "Epoch 60, train loss: 0.02035670354962349, val loss: 0.23962676525115967\n",
      "Epoch 70, train loss: 0.01974434033036232, val loss: 0.21615755558013916\n",
      "Epoch 80, train loss: 0.019717905670404434, val loss: 0.2329302877187729\n",
      "Epoch 90, train loss: 0.019703075289726257, val loss: 0.23691914975643158\n",
      "Epoch 100, train loss: 0.01966569386422634, val loss: 0.22852380573749542\n",
      "Epoch 110, train loss: 0.01965971849858761, val loss: 0.2341110110282898\n",
      "Epoch 120, train loss: 0.01965952292084694, val loss: 0.23116527497768402\n",
      "Epoch 130, train loss: 0.019659511744976044, val loss: 0.23275861144065857\n",
      "Epoch 140, train loss: 0.01965950056910515, val loss: 0.23175248503684998\n",
      "Epoch 150, train loss: 0.019659506157040596, val loss: 0.23244205117225647\n",
      "Epoch 160, train loss: 0.019659502431750298, val loss: 0.23201556503772736\n",
      "Epoch 170, train loss: 0.01965947635471821, val loss: 0.2321908175945282\n",
      "Epoch 180, train loss: 0.019659454002976418, val loss: 0.23220020532608032\n",
      "Epoch 190, train loss: 0.01965944841504097, val loss: 0.23212791979312897\n",
      "Training time: 39.52774524688721\n",
      "Training RNN with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.657234251499176, val loss: 0.009897744283080101\n",
      "Epoch 10, train loss: 0.050506770610809326, val loss: 0.0991421714425087\n",
      "Epoch 20, train loss: 0.016153773292899132, val loss: 0.12092799693346024\n",
      "Epoch 30, train loss: 0.014788909815251827, val loss: 0.15758100152015686\n",
      "Epoch 40, train loss: 0.007766765542328358, val loss: 0.07526793330907822\n",
      "Epoch 50, train loss: 0.0018003255827352405, val loss: 0.0087244538590312\n",
      "Epoch 60, train loss: 0.0007205253932625055, val loss: 0.002253922400996089\n",
      "Epoch 70, train loss: 0.0003443191817495972, val loss: 0.004122699610888958\n",
      "Epoch 80, train loss: 0.0001427569950465113, val loss: 0.0004122846876271069\n",
      "Epoch 90, train loss: 7.615137292305008e-05, val loss: 0.0013163480907678604\n",
      "Epoch 100, train loss: 6.72482856316492e-05, val loss: 0.00035456105251796544\n",
      "Epoch 110, train loss: 6.157336611067876e-05, val loss: 0.0008386840927414596\n",
      "Epoch 120, train loss: 5.8567984524415806e-05, val loss: 0.0004907495458610356\n",
      "Epoch 130, train loss: 5.714178769267164e-05, val loss: 0.0006614840240217745\n",
      "Epoch 140, train loss: 5.641596362693235e-05, val loss: 0.0005384047399275005\n",
      "Epoch 150, train loss: 5.593923560809344e-05, val loss: 0.0005942101124674082\n",
      "Epoch 160, train loss: 5.544965824810788e-05, val loss: 0.0005559882847592235\n",
      "Epoch 170, train loss: 5.501001942320727e-05, val loss: 0.0005588378990069032\n",
      "Epoch 180, train loss: 5.457902807393111e-05, val loss: 0.0005444723065011203\n",
      "Epoch 190, train loss: 5.415809937403537e-05, val loss: 0.0005343961529433727\n",
      "Training time: 39.16911220550537\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.716059684753418, val loss: 0.04525423049926758\n",
      "Epoch 10, train loss: 0.04655345901846886, val loss: 0.09308105707168579\n",
      "Epoch 20, train loss: 0.024916423484683037, val loss: 0.27600643038749695\n",
      "Epoch 30, train loss: 0.020340466871857643, val loss: 0.17922291159629822\n",
      "Epoch 40, train loss: 0.018368255347013474, val loss: 0.22428882122039795\n",
      "Epoch 50, train loss: 0.017070217058062553, val loss: 0.19164928793907166\n",
      "Epoch 60, train loss: 0.01539185643196106, val loss: 0.17240241169929504\n",
      "Epoch 70, train loss: 0.006874899845570326, val loss: 0.05704346299171448\n",
      "Epoch 80, train loss: 0.003921095747500658, val loss: 0.047217804938554764\n",
      "Epoch 90, train loss: 0.0015602491330355406, val loss: 0.006778803654015064\n",
      "Training time: 5.665086269378662\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6099509000778198, val loss: 0.08004381507635117\n",
      "Epoch 10, train loss: 0.4091615378856659, val loss: 0.021854352205991745\n",
      "Epoch 20, train loss: 0.17217540740966797, val loss: 0.012883136048913002\n",
      "Epoch 30, train loss: 0.04734648019075394, val loss: 0.3597256541252136\n",
      "Epoch 40, train loss: 0.01635150983929634, val loss: 0.141076922416687\n",
      "Epoch 50, train loss: 0.019003625959157944, val loss: 0.1411205679178238\n",
      "Epoch 60, train loss: 0.016242418438196182, val loss: 0.20559637248516083\n",
      "Epoch 70, train loss: 0.014418551698327065, val loss: 0.16355231404304504\n",
      "Epoch 80, train loss: 0.01403480488806963, val loss: 0.1584731489419937\n",
      "Epoch 90, train loss: 0.01341209840029478, val loss: 0.1635272055864334\n",
      "Training time: 5.536373853683472\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 1.0665256977081299, val loss: 0.16754166781902313\n",
      "Epoch 10, train loss: 0.02942723035812378, val loss: 0.08074130117893219\n",
      "Epoch 20, train loss: 0.026069147512316704, val loss: 0.314714252948761\n",
      "Epoch 30, train loss: 0.021005384624004364, val loss: 0.16815820336341858\n",
      "Epoch 40, train loss: 0.018991436809301376, val loss: 0.24519075453281403\n",
      "Epoch 50, train loss: 0.01830187626183033, val loss: 0.1939246505498886\n",
      "Epoch 60, train loss: 0.017729461193084717, val loss: 0.21806700527668\n",
      "Epoch 70, train loss: 0.016905169934034348, val loss: 0.19127102196216583\n",
      "Epoch 80, train loss: 0.013702571392059326, val loss: 0.16815534234046936\n",
      "Epoch 90, train loss: 0.003932088613510132, val loss: 0.023573122918605804\n",
      "Epoch 100, train loss: 0.004124800208956003, val loss: 0.018032649531960487\n",
      "Epoch 110, train loss: 0.0010125660337507725, val loss: 0.041291579604148865\n",
      "Epoch 120, train loss: 0.0011846923734992743, val loss: 0.038047175854444504\n",
      "Epoch 130, train loss: 0.0007277065888047218, val loss: 0.03057648241519928\n",
      "Epoch 140, train loss: 0.0005485651199705899, val loss: 0.026512499898672104\n",
      "Epoch 150, train loss: 0.0004601786786224693, val loss: 0.021774951368570328\n",
      "Epoch 160, train loss: 0.0003511410905048251, val loss: 0.018474169075489044\n",
      "Epoch 170, train loss: 0.0002878422965295613, val loss: 0.014484534971415997\n",
      "Epoch 180, train loss: 0.00023490629973821342, val loss: 0.01121414452791214\n",
      "Epoch 190, train loss: 0.00019483474898152053, val loss: 0.008407188579440117\n",
      "Training time: 10.889508962631226\n",
      "Training LSTM with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9185299277305603, val loss: 0.2133857160806656\n",
      "Epoch 10, train loss: 0.6105278730392456, val loss: 0.08598975837230682\n",
      "Epoch 20, train loss: 0.24462135136127472, val loss: 0.0030394699424505234\n",
      "Epoch 30, train loss: 0.046237051486968994, val loss: 0.33522823452949524\n",
      "Epoch 40, train loss: 0.01318227406591177, val loss: 0.13415606319904327\n",
      "Epoch 50, train loss: 0.0208682119846344, val loss: 0.10349295288324356\n",
      "Epoch 60, train loss: 0.013888636603951454, val loss: 0.17592334747314453\n",
      "Epoch 70, train loss: 0.013040968216955662, val loss: 0.1527991145849228\n",
      "Epoch 80, train loss: 0.01278869342058897, val loss: 0.13081659376621246\n",
      "Epoch 90, train loss: 0.011996463872492313, val loss: 0.14227281510829926\n",
      "Epoch 100, train loss: 0.01140050683170557, val loss: 0.12985485792160034\n",
      "Epoch 110, train loss: 0.010787693783640862, val loss: 0.12170970439910889\n",
      "Epoch 120, train loss: 0.01007428951561451, val loss: 0.11618126183748245\n",
      "Epoch 130, train loss: 0.009252211079001427, val loss: 0.10495977103710175\n",
      "Epoch 140, train loss: 0.008296962827444077, val loss: 0.09560582786798477\n",
      "Epoch 150, train loss: 0.007161557208746672, val loss: 0.08278994262218475\n",
      "Epoch 160, train loss: 0.005775938741862774, val loss: 0.06874002516269684\n",
      "Epoch 170, train loss: 0.004049814771860838, val loss: 0.05134769529104233\n",
      "Epoch 180, train loss: 0.002019221195951104, val loss: 0.03157301992177963\n",
      "Epoch 190, train loss: 0.0004107272543478757, val loss: 0.013994859531521797\n",
      "Training time: 10.538684368133545\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.3315967321395874, val loss: 0.011388028040528297\n",
      "Epoch 10, train loss: 0.03786856681108475, val loss: 0.14947763085365295\n",
      "Epoch 20, train loss: 0.022121001034975052, val loss: 0.25811174511909485\n",
      "Epoch 30, train loss: 0.020812418311834335, val loss: 0.20541682839393616\n",
      "Epoch 40, train loss: 0.019897084683179855, val loss: 0.2450270652770996\n",
      "Epoch 50, train loss: 0.019245179370045662, val loss: 0.21542569994926453\n",
      "Epoch 60, train loss: 0.01519432757049799, val loss: 0.19346420466899872\n",
      "Epoch 70, train loss: 0.005753966048359871, val loss: 0.0495283342897892\n",
      "Epoch 80, train loss: 0.0037466345820575953, val loss: 0.07142159342765808\n",
      "Epoch 90, train loss: 0.002916756086051464, val loss: 0.08910682052373886\n",
      "Training time: 7.854466676712036\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.33004501461982727, val loss: 0.00745747284963727\n",
      "Epoch 10, train loss: 0.10970287770032883, val loss: 0.049736250191926956\n",
      "Epoch 20, train loss: 0.04707803577184677, val loss: 0.3794955015182495\n",
      "Epoch 30, train loss: 0.022198405116796494, val loss: 0.16174109280109406\n",
      "Epoch 40, train loss: 0.019240567460656166, val loss: 0.20284195244312286\n",
      "Epoch 50, train loss: 0.019329175353050232, val loss: 0.2430027574300766\n",
      "Epoch 60, train loss: 0.018162501975893974, val loss: 0.19821074604988098\n",
      "Epoch 70, train loss: 0.017533062025904655, val loss: 0.21071292459964752\n",
      "Epoch 80, train loss: 0.016952255740761757, val loss: 0.2030615508556366\n",
      "Epoch 90, train loss: 0.015941526740789413, val loss: 0.18867561221122742\n",
      "Training time: 7.385022401809692\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6020282506942749, val loss: 0.019549814984202385\n",
      "Epoch 10, train loss: 0.02231353148818016, val loss: 0.22713741660118103\n",
      "Epoch 20, train loss: 0.0212663933634758, val loss: 0.1802087277173996\n",
      "Epoch 30, train loss: 0.020704442635178566, val loss: 0.26211339235305786\n",
      "Epoch 40, train loss: 0.01954393833875656, val loss: 0.21572346985340118\n",
      "Epoch 50, train loss: 0.018911410123109818, val loss: 0.21794694662094116\n",
      "Epoch 60, train loss: 0.01731223054230213, val loss: 0.20933277904987335\n",
      "Epoch 70, train loss: 0.0047515761107206345, val loss: 0.040299125015735626\n",
      "Epoch 80, train loss: 0.0026463638059794903, val loss: 0.0853906124830246\n",
      "Epoch 90, train loss: 0.001285073347389698, val loss: 0.04010460153222084\n",
      "Epoch 100, train loss: 0.0004657526151277125, val loss: 0.025284936651587486\n",
      "Epoch 110, train loss: 0.00033782259561121464, val loss: 0.01535503938794136\n",
      "Epoch 120, train loss: 0.00017358909826725721, val loss: 0.008433789014816284\n",
      "Epoch 130, train loss: 0.00015917611017357558, val loss: 0.006120646372437477\n",
      "Epoch 140, train loss: 0.00014907759032212198, val loss: 0.0050138672813773155\n",
      "Epoch 150, train loss: 0.00012969961971975863, val loss: 0.0037827580235898495\n",
      "Epoch 160, train loss: 0.0001580403040861711, val loss: 0.002812949474900961\n",
      "Epoch 170, train loss: 0.00023037430946715176, val loss: 0.0029686016496270895\n",
      "Epoch 180, train loss: 0.0001530123845441267, val loss: 0.00346323917619884\n",
      "Epoch 190, train loss: 0.00012479862198233604, val loss: 0.0028739215340465307\n",
      "Training time: 15.684171438217163\n",
      "Training LSTM with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.40022408962249756, val loss: 0.019664205610752106\n",
      "Epoch 10, train loss: 0.21675434708595276, val loss: 0.005927571095526218\n",
      "Epoch 20, train loss: 0.02274433523416519, val loss: 0.3294854164123535\n",
      "Epoch 30, train loss: 0.01735972799360752, val loss: 0.15805281698703766\n",
      "Epoch 40, train loss: 0.01994650810956955, val loss: 0.1623431295156479\n",
      "Epoch 50, train loss: 0.018280141055583954, val loss: 0.22985605895519257\n",
      "Epoch 60, train loss: 0.016457175835967064, val loss: 0.17741532623767853\n",
      "Epoch 70, train loss: 0.015853440389037132, val loss: 0.18986213207244873\n",
      "Epoch 80, train loss: 0.015416005626320839, val loss: 0.18495629727840424\n",
      "Epoch 90, train loss: 0.01479637622833252, val loss: 0.1733318418264389\n",
      "Epoch 100, train loss: 0.013943412341177464, val loss: 0.16835498809814453\n",
      "Epoch 110, train loss: 0.012648056261241436, val loss: 0.1523139476776123\n",
      "Epoch 120, train loss: 0.010410992428660393, val loss: 0.12941831350326538\n",
      "Epoch 130, train loss: 0.00610447209328413, val loss: 0.08730726689100266\n",
      "Epoch 140, train loss: 0.0007866154774092138, val loss: 0.03730589151382446\n",
      "Epoch 150, train loss: 0.0010348688811063766, val loss: 0.02376534976065159\n",
      "Epoch 160, train loss: 0.0004456646856851876, val loss: 0.029749305918812752\n",
      "Epoch 170, train loss: 0.0003761347907129675, val loss: 0.025941401720046997\n",
      "Epoch 180, train loss: 0.00034628852154128253, val loss: 0.021853605285286903\n",
      "Epoch 190, train loss: 0.0003034437831956893, val loss: 0.02196134254336357\n",
      "Training time: 15.395719289779663\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.4618109464645386, val loss: 0.005867967382073402\n",
      "Epoch 10, train loss: 0.018183724954724312, val loss: 0.17061154544353485\n",
      "Epoch 20, train loss: 0.017633797600865364, val loss: 0.19146455824375153\n",
      "Epoch 30, train loss: 0.01175537146627903, val loss: 0.12647683918476105\n",
      "Epoch 40, train loss: 0.004681100603193045, val loss: 0.026055850088596344\n",
      "Epoch 50, train loss: 0.005642732139676809, val loss: 0.05451655015349388\n",
      "Epoch 60, train loss: 0.0007208831375464797, val loss: 0.011011374182999134\n",
      "Epoch 70, train loss: 0.0008298757602460682, val loss: 0.0041933683678507805\n",
      "Epoch 80, train loss: 0.00046856558765284717, val loss: 0.008716721087694168\n",
      "Epoch 90, train loss: 0.00019563989189919084, val loss: 0.0019836300052702427\n",
      "Training time: 12.573648452758789\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5373694896697998, val loss: 0.04953964054584503\n",
      "Epoch 10, train loss: 0.19997262954711914, val loss: 0.011575034819543362\n",
      "Epoch 20, train loss: 0.04606189951300621, val loss: 0.29161328077316284\n",
      "Epoch 30, train loss: 0.030400099232792854, val loss: 0.11186675727367401\n",
      "Epoch 40, train loss: 0.01813996210694313, val loss: 0.23390451073646545\n",
      "Epoch 50, train loss: 0.015224621631205082, val loss: 0.15901418030261993\n",
      "Epoch 60, train loss: 0.0139622762799263, val loss: 0.16977807879447937\n",
      "Epoch 70, train loss: 0.012502742931246758, val loss: 0.14710046350955963\n",
      "Epoch 80, train loss: 0.010048240423202515, val loss: 0.12698334455490112\n",
      "Epoch 90, train loss: 0.005429442040622234, val loss: 0.07556412369012833\n",
      "Training time: 12.292693614959717\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.8285655379295349, val loss: 0.03094891645014286\n",
      "Epoch 10, train loss: 0.15548495948314667, val loss: 0.022944141179323196\n",
      "Epoch 20, train loss: 0.03400903195142746, val loss: 0.31133630871772766\n",
      "Epoch 30, train loss: 0.023573193699121475, val loss: 0.1670004278421402\n",
      "Epoch 40, train loss: 0.019490541890263557, val loss: 0.2517455220222473\n",
      "Epoch 50, train loss: 0.018700670450925827, val loss: 0.21433259546756744\n",
      "Epoch 60, train loss: 0.01851774752140045, val loss: 0.21418745815753937\n",
      "Epoch 70, train loss: 0.018183156847953796, val loss: 0.2186623215675354\n",
      "Epoch 80, train loss: 0.01766924187541008, val loss: 0.20479370653629303\n",
      "Epoch 90, train loss: 0.016855968162417412, val loss: 0.2019188106060028\n",
      "Epoch 100, train loss: 0.014960198663175106, val loss: 0.18076930940151215\n",
      "Epoch 110, train loss: 0.003839259734377265, val loss: 0.0778159573674202\n",
      "Epoch 120, train loss: 0.02157922089099884, val loss: 0.06987990438938141\n",
      "Epoch 130, train loss: 0.007857519201934338, val loss: 0.09197832643985748\n",
      "Epoch 140, train loss: 0.003264568978920579, val loss: 0.07669597864151001\n",
      "Epoch 150, train loss: 0.001418471452780068, val loss: 0.04238566383719444\n",
      "Epoch 160, train loss: 0.0012426982866600156, val loss: 0.03237568587064743\n",
      "Epoch 170, train loss: 0.001028616912662983, val loss: 0.03214794024825096\n",
      "Epoch 180, train loss: 0.0007157099316827953, val loss: 0.03191414102911949\n",
      "Epoch 190, train loss: 0.0006228351849131286, val loss: 0.03326769173145294\n",
      "Training time: 25.040783882141113\n",
      "Training LSTM with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7176199555397034, val loss: 0.11664385348558426\n",
      "Epoch 10, train loss: 0.1655007302761078, val loss: 0.01405264064669609\n",
      "Epoch 20, train loss: 0.015856338664889336, val loss: 0.1398577094078064\n",
      "Epoch 30, train loss: 0.02605302259325981, val loss: 0.08905929327011108\n",
      "Epoch 40, train loss: 0.018201712518930435, val loss: 0.19688266515731812\n",
      "Epoch 50, train loss: 0.013514168560504913, val loss: 0.1156742125749588\n",
      "Epoch 60, train loss: 0.01176440343260765, val loss: 0.14830957353115082\n",
      "Epoch 70, train loss: 0.011056281626224518, val loss: 0.12454919517040253\n",
      "Epoch 80, train loss: 0.010383852757513523, val loss: 0.1247260719537735\n",
      "Epoch 90, train loss: 0.00962549913674593, val loss: 0.11188184469938278\n",
      "Epoch 100, train loss: 0.00874408707022667, val loss: 0.1050991490483284\n",
      "Epoch 110, train loss: 0.007700570393353701, val loss: 0.09103086590766907\n",
      "Epoch 120, train loss: 0.006419614423066378, val loss: 0.07840512692928314\n",
      "Epoch 130, train loss: 0.004749324172735214, val loss: 0.06048903614282608\n",
      "Epoch 140, train loss: 0.0024237411562353373, val loss: 0.03603139892220497\n",
      "Epoch 150, train loss: 0.0002661647740751505, val loss: 0.009584947489202023\n",
      "Epoch 160, train loss: 0.0004330175288487226, val loss: 0.00543412659317255\n",
      "Epoch 170, train loss: 0.0002379224606556818, val loss: 0.010533838532865047\n",
      "Epoch 180, train loss: 0.00021735619520768523, val loss: 0.009082002565264702\n",
      "Epoch 190, train loss: 0.00019263381545897573, val loss: 0.00658034672960639\n",
      "Training time: 25.39393901824951\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7005417346954346, val loss: 0.009550463408231735\n",
      "Epoch 10, train loss: 0.04267950728535652, val loss: 0.34196799993515015\n",
      "Epoch 20, train loss: 0.02138173207640648, val loss: 0.2148074209690094\n",
      "Epoch 30, train loss: 0.019491078332066536, val loss: 0.21426717936992645\n",
      "Epoch 40, train loss: 0.019650213420391083, val loss: 0.250090092420578\n",
      "Epoch 50, train loss: 0.01964646205306053, val loss: 0.21384933590888977\n",
      "Epoch 60, train loss: 0.019335731863975525, val loss: 0.23252347111701965\n",
      "Epoch 70, train loss: 0.018926214426755905, val loss: 0.22604160010814667\n",
      "Epoch 80, train loss: 0.009432612918317318, val loss: 0.14799152314662933\n",
      "Epoch 90, train loss: 0.0031208437867462635, val loss: 0.07506921887397766\n",
      "Training time: 19.779858350753784\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5466000437736511, val loss: 0.05356048792600632\n",
      "Epoch 10, train loss: 0.0959414690732956, val loss: 0.07297829538583755\n",
      "Epoch 20, train loss: 0.019751714542508125, val loss: 0.1205231100320816\n",
      "Epoch 30, train loss: 0.01797768846154213, val loss: 0.18135176599025726\n",
      "Epoch 40, train loss: 0.017354954034090042, val loss: 0.20609717071056366\n",
      "Epoch 50, train loss: 0.01691330038011074, val loss: 0.1709006428718567\n",
      "Epoch 60, train loss: 0.01593845523893833, val loss: 0.19659458100795746\n",
      "Epoch 70, train loss: 0.015059120953083038, val loss: 0.16991089284420013\n",
      "Epoch 80, train loss: 0.014035080559551716, val loss: 0.17011681199073792\n",
      "Epoch 90, train loss: 0.01234994176775217, val loss: 0.1506337970495224\n",
      "Training time: 20.88368511199951\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.4156274199485779, val loss: 0.0468631312251091\n",
      "Epoch 10, train loss: 0.06218886002898216, val loss: 0.13691279292106628\n",
      "Epoch 20, train loss: 0.023872211575508118, val loss: 0.1658429056406021\n",
      "Epoch 30, train loss: 0.019979946315288544, val loss: 0.2618609070777893\n",
      "Epoch 40, train loss: 0.01945348270237446, val loss: 0.21659992635250092\n",
      "Epoch 50, train loss: 0.01928192935883999, val loss: 0.23084387183189392\n",
      "Epoch 60, train loss: 0.019095350056886673, val loss: 0.22511304914951324\n",
      "Epoch 70, train loss: 0.018772440031170845, val loss: 0.2222774773836136\n",
      "Epoch 80, train loss: 0.018029484897851944, val loss: 0.2130117267370224\n",
      "Epoch 90, train loss: 0.007447251118719578, val loss: 0.12864752113819122\n",
      "Epoch 100, train loss: 0.010178289376199245, val loss: 0.07411913573741913\n",
      "Epoch 110, train loss: 0.001708346651867032, val loss: 0.06538226455450058\n",
      "Epoch 120, train loss: 0.0012204545782878995, val loss: 0.012496273964643478\n",
      "Epoch 130, train loss: 0.0005259836325421929, val loss: 0.015820972621440887\n",
      "Epoch 140, train loss: 0.0002534602244850248, val loss: 0.006826111115515232\n",
      "Epoch 150, train loss: 0.00017568709154147655, val loss: 0.001867865677922964\n",
      "Epoch 160, train loss: 0.00013586162822321057, val loss: 0.0015955570852383971\n",
      "Epoch 170, train loss: 0.00013562534877564758, val loss: 0.0013523598900064826\n",
      "Epoch 180, train loss: 0.0001286165352212265, val loss: 0.0019228709861636162\n",
      "Epoch 190, train loss: 0.00012703886022791266, val loss: 0.002101582707837224\n",
      "Training time: 41.696943283081055\n",
      "Training LSTM with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7177809476852417, val loss: 0.11097969859838486\n",
      "Epoch 10, train loss: 0.1671561449766159, val loss: 0.03166509047150612\n",
      "Epoch 20, train loss: 0.017033740878105164, val loss: 0.15810203552246094\n",
      "Epoch 30, train loss: 0.024247556924819946, val loss: 0.1613355427980423\n",
      "Epoch 40, train loss: 0.021420810371637344, val loss: 0.25002220273017883\n",
      "Epoch 50, train loss: 0.018831241875886917, val loss: 0.17017114162445068\n",
      "Epoch 60, train loss: 0.01722787693142891, val loss: 0.21751731634140015\n",
      "Epoch 70, train loss: 0.01637892983853817, val loss: 0.1809765249490738\n",
      "Epoch 80, train loss: 0.01564808376133442, val loss: 0.1907377690076828\n",
      "Epoch 90, train loss: 0.014724346809089184, val loss: 0.1704493910074234\n",
      "Epoch 100, train loss: 0.013278004713356495, val loss: 0.16047683358192444\n",
      "Epoch 110, train loss: 0.01050035934895277, val loss: 0.1290893256664276\n",
      "Epoch 120, train loss: 0.00479945819824934, val loss: 0.07200727611780167\n",
      "Epoch 130, train loss: 0.000681794248521328, val loss: 0.018183594569563866\n",
      "Epoch 140, train loss: 0.0004603037377819419, val loss: 0.017833033576607704\n",
      "Epoch 150, train loss: 0.0004591578326653689, val loss: 0.022232206538319588\n",
      "Epoch 160, train loss: 0.0002549327036831528, val loss: 0.01550967339426279\n",
      "Epoch 170, train loss: 0.00025564394309185445, val loss: 0.013996945694088936\n",
      "Epoch 180, train loss: 0.0002413296897429973, val loss: 0.0145031176507473\n",
      "Epoch 190, train loss: 0.00022887595696374774, val loss: 0.012884946539998055\n",
      "Training time: 42.43516516685486\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6311360001564026, val loss: 0.0429520457983017\n",
      "Epoch 10, train loss: 0.19972287118434906, val loss: 0.01244738046079874\n",
      "Epoch 20, train loss: 0.025884190574288368, val loss: 0.1317431479692459\n",
      "Epoch 30, train loss: 0.023913688957691193, val loss: 0.2948991656303406\n",
      "Epoch 40, train loss: 0.020502159371972084, val loss: 0.1847664713859558\n",
      "Epoch 50, train loss: 0.01824665255844593, val loss: 0.21965153515338898\n",
      "Epoch 60, train loss: 0.016719022765755653, val loss: 0.20667919516563416\n",
      "Epoch 70, train loss: 0.037772905081510544, val loss: 0.08116915076971054\n",
      "Epoch 80, train loss: 0.017480527982115746, val loss: 0.18101726472377777\n",
      "Epoch 90, train loss: 0.01615414209663868, val loss: 0.1631922721862793\n",
      "Training time: 42.491453886032104\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.7235201001167297, val loss: 0.10683989524841309\n",
      "Epoch 10, train loss: 0.13807819783687592, val loss: 0.40362975001335144\n",
      "Epoch 20, train loss: 0.04293644428253174, val loss: 0.07384686917066574\n",
      "Epoch 30, train loss: 0.020640909671783447, val loss: 0.19590164721012115\n",
      "Epoch 40, train loss: 0.015187106095254421, val loss: 0.12254565954208374\n",
      "Epoch 50, train loss: 0.012321584858000278, val loss: 0.1484214961528778\n",
      "Epoch 60, train loss: 0.010784846730530262, val loss: 0.12652193009853363\n",
      "Epoch 70, train loss: 0.009469536133110523, val loss: 0.10978938639163971\n",
      "Epoch 80, train loss: 0.007789629511535168, val loss: 0.09779801964759827\n",
      "Epoch 90, train loss: 0.004710061475634575, val loss: 0.06269615888595581\n",
      "Training time: 44.5712103843689\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6198354959487915, val loss: 0.011439553461968899\n",
      "Epoch 10, train loss: 0.11186477541923523, val loss: 0.07478776574134827\n",
      "Epoch 20, train loss: 0.03216349333524704, val loss: 0.1310546100139618\n",
      "Epoch 30, train loss: 0.023582005873322487, val loss: 0.2765255570411682\n",
      "Epoch 40, train loss: 0.01958189718425274, val loss: 0.20133136212825775\n",
      "Epoch 50, train loss: 0.018118014559149742, val loss: 0.2100052833557129\n",
      "Epoch 60, train loss: 0.01750234141945839, val loss: 0.21751052141189575\n",
      "Epoch 70, train loss: 0.015974050387740135, val loss: 0.18797053396701813\n",
      "Epoch 80, train loss: 0.00909555982798338, val loss: 0.1002436950802803\n",
      "Epoch 90, train loss: 0.026784183457493782, val loss: 0.2245112955570221\n",
      "Epoch 100, train loss: 0.01691281795501709, val loss: 0.1960410624742508\n",
      "Epoch 110, train loss: 0.016076959669589996, val loss: 0.1753675937652588\n",
      "Epoch 120, train loss: 0.014578410424292088, val loss: 0.18469153344631195\n",
      "Epoch 130, train loss: 0.009782304987311363, val loss: 0.1263732761144638\n",
      "Epoch 140, train loss: 0.001031877938657999, val loss: 0.04185129702091217\n",
      "Epoch 150, train loss: 0.0007911589345894754, val loss: 0.034492723643779755\n",
      "Epoch 160, train loss: 0.0007072506123222411, val loss: 0.03376536816358566\n",
      "Epoch 170, train loss: 0.0006287400610744953, val loss: 0.033401645720005035\n",
      "Epoch 180, train loss: 0.0005362702650018036, val loss: 0.033975839614868164\n",
      "Epoch 190, train loss: 0.00045392458559945226, val loss: 0.028604434803128242\n",
      "Training time: 85.98992037773132\n",
      "Training LSTM with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5838149189949036, val loss: 0.055272024124860764\n",
      "Epoch 10, train loss: 0.05061986297369003, val loss: 0.3729855418205261\n",
      "Epoch 20, train loss: 0.03504379093647003, val loss: 0.08561946451663971\n",
      "Epoch 30, train loss: 0.020017217844724655, val loss: 0.20377297699451447\n",
      "Epoch 40, train loss: 0.014780987054109573, val loss: 0.1256095916032791\n",
      "Epoch 50, train loss: 0.011897156946361065, val loss: 0.14487560093402863\n",
      "Epoch 60, train loss: 0.00958710815757513, val loss: 0.11713612824678421\n",
      "Epoch 70, train loss: 0.006627555936574936, val loss: 0.0760735273361206\n",
      "Epoch 80, train loss: 0.0009623991791158915, val loss: 0.0204171109944582\n",
      "Epoch 90, train loss: 0.0007187037845142186, val loss: 0.0055181654170155525\n",
      "Epoch 100, train loss: 0.0004674895608332008, val loss: 0.014653784222900867\n",
      "Epoch 110, train loss: 0.00020201446022838354, val loss: 0.005672752857208252\n",
      "Epoch 120, train loss: 0.00018184955115430057, val loss: 0.0048203538171947\n",
      "Epoch 130, train loss: 0.00015920470468699932, val loss: 0.005193538498133421\n",
      "Epoch 140, train loss: 0.00013399447198025882, val loss: 0.002841488691046834\n",
      "Epoch 150, train loss: 0.00011905025894520804, val loss: 0.002659283112734556\n",
      "Epoch 160, train loss: 0.00010979911894537508, val loss: 0.0018888985505327582\n",
      "Epoch 170, train loss: 0.00010353152174502611, val loss: 0.0014160481514409184\n",
      "Epoch 180, train loss: 9.929595398716629e-05, val loss: 0.0011512498604133725\n",
      "Epoch 190, train loss: 9.655439498601481e-05, val loss: 0.0008908548043109477\n",
      "Training time: 86.68402338027954\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.691596269607544, val loss: 0.027935046702623367\n",
      "Epoch 10, train loss: 0.17836548388004303, val loss: 0.06264665722846985\n",
      "Epoch 20, train loss: 0.04745742678642273, val loss: 0.3789319694042206\n",
      "Epoch 30, train loss: 0.019712232053279877, val loss: 0.27623122930526733\n",
      "Epoch 40, train loss: 0.022500095888972282, val loss: 0.18536046147346497\n",
      "Epoch 50, train loss: 0.020012449473142624, val loss: 0.2373487651348114\n",
      "Epoch 60, train loss: 0.01981700398027897, val loss: 0.2486349195241928\n",
      "Epoch 70, train loss: 0.019758816808462143, val loss: 0.2266252338886261\n",
      "Epoch 80, train loss: 0.019673140719532967, val loss: 0.22668534517288208\n",
      "Epoch 90, train loss: 0.019669411703944206, val loss: 0.2335393875837326\n",
      "Training time: 66.95634031295776\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.6138036251068115, val loss: 0.06526359915733337\n",
      "Epoch 10, train loss: 0.038592416793107986, val loss: 0.20486368238925934\n",
      "Epoch 20, train loss: 0.01964757777750492, val loss: 0.19858521223068237\n",
      "Epoch 30, train loss: 0.01696457900106907, val loss: 0.16693900525569916\n",
      "Epoch 40, train loss: 0.016208969056606293, val loss: 0.21252624690532684\n",
      "Epoch 50, train loss: 0.015543501824140549, val loss: 0.16256876289844513\n",
      "Epoch 60, train loss: 0.013846536166965961, val loss: 0.1741981953382492\n",
      "Epoch 70, train loss: 0.00959200132638216, val loss: 0.12309613078832626\n",
      "Epoch 80, train loss: 0.001352620660327375, val loss: 0.03353374823927879\n",
      "Epoch 90, train loss: 0.0009698602952994406, val loss: 0.01785908080637455\n",
      "Training time: 66.13893461227417\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.7613136768341064, val loss: 0.11445402354001999\n",
      "Epoch 10, train loss: 0.18775971233844757, val loss: 0.006665835622698069\n",
      "Epoch 20, train loss: 0.021809225901961327, val loss: 0.13276134431362152\n",
      "Epoch 30, train loss: 0.019948923960328102, val loss: 0.22257016599178314\n",
      "Epoch 40, train loss: 0.0271166805177927, val loss: 0.1609889566898346\n",
      "Epoch 50, train loss: 0.019779948517680168, val loss: 0.18289066851139069\n",
      "Epoch 60, train loss: 0.014384638518095016, val loss: 0.15721723437309265\n",
      "Epoch 70, train loss: 0.0010235396912321448, val loss: 0.17258167266845703\n",
      "Epoch 80, train loss: 0.0012596967862918973, val loss: 0.02745651826262474\n",
      "Epoch 90, train loss: 0.0005182513850741088, val loss: 0.020951824262738228\n",
      "Epoch 100, train loss: 0.0003612118889577687, val loss: 0.022046325728297234\n",
      "Epoch 110, train loss: 0.00025105566601268947, val loss: 0.017239883542060852\n",
      "Epoch 120, train loss: 0.00022311411157716066, val loss: 0.016894878819584846\n",
      "Epoch 130, train loss: 0.00019839855667669326, val loss: 0.01403397973626852\n",
      "Epoch 140, train loss: 0.00018920924048870802, val loss: 0.013213835656642914\n",
      "Epoch 150, train loss: 0.0001851431152317673, val loss: 0.012029258534312248\n",
      "Epoch 160, train loss: 0.0013047127285972238, val loss: 0.014166034758090973\n",
      "Epoch 170, train loss: 0.0011255507124587893, val loss: 0.011443686671555042\n",
      "Epoch 180, train loss: 0.000732158834580332, val loss: 0.010390828363597393\n",
      "Epoch 190, train loss: 0.00042790966108441353, val loss: 0.011767420917749405\n",
      "Training time: 136.15262055397034\n",
      "Training LSTM with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5843214988708496, val loss: 0.055234555155038834\n",
      "Epoch 10, train loss: 0.061005834490060806, val loss: 0.2787455916404724\n",
      "Epoch 20, train loss: 0.022959835827350616, val loss: 0.18775229156017303\n",
      "Epoch 30, train loss: 0.01730155572295189, val loss: 0.17933389544487\n",
      "Epoch 40, train loss: 0.016520678997039795, val loss: 0.21540072560310364\n",
      "Epoch 50, train loss: 0.015545789152383804, val loss: 0.161187082529068\n",
      "Epoch 60, train loss: 0.012427492067217827, val loss: 0.15235890448093414\n",
      "Epoch 70, train loss: 0.0027159936726093292, val loss: 0.03960094600915909\n",
      "Epoch 80, train loss: 0.0006121120532043278, val loss: 0.016842840239405632\n",
      "Epoch 90, train loss: 0.0008564580930396914, val loss: 0.02119932696223259\n",
      "Epoch 100, train loss: 0.00039235458825714886, val loss: 0.0041728210635483265\n",
      "Epoch 110, train loss: 0.0002214236737927422, val loss: 0.00752051780000329\n",
      "Epoch 120, train loss: 0.00015007154433988035, val loss: 0.0026604312006384134\n",
      "Epoch 130, train loss: 0.00013494440645445138, val loss: 0.0022845666389912367\n",
      "Epoch 140, train loss: 0.0001269057102035731, val loss: 0.0013366957427933812\n",
      "Epoch 150, train loss: 0.0001250171335414052, val loss: 0.0011309466790407896\n",
      "Epoch 160, train loss: 0.0001243447040906176, val loss: 0.000929623784031719\n",
      "Epoch 170, train loss: 0.00012407134636305273, val loss: 0.0009065004414878786\n",
      "Epoch 180, train loss: 0.00012389247422106564, val loss: 0.0008562253788113594\n",
      "Epoch 190, train loss: 0.000123696998343803, val loss: 0.0008744709193706512\n",
      "Training time: 129.90190291404724\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7263984680175781, val loss: 0.007959004491567612\n",
      "Epoch 10, train loss: 0.037353843450546265, val loss: 0.09361527115106583\n",
      "Epoch 20, train loss: 0.012972235679626465, val loss: 0.10262982547283173\n",
      "Epoch 30, train loss: 0.01327748503535986, val loss: 0.14083142578601837\n",
      "Epoch 40, train loss: 0.006705315317958593, val loss: 0.07638077437877655\n",
      "Epoch 50, train loss: 0.0029952486511319876, val loss: 0.02272365242242813\n",
      "Epoch 60, train loss: 0.0006597538013011217, val loss: 0.004141306038945913\n",
      "Epoch 70, train loss: 5.453833000501618e-05, val loss: 0.00030807132134214044\n",
      "Epoch 80, train loss: 0.00014768107212148607, val loss: 0.0005601804004982114\n",
      "Epoch 90, train loss: 6.32079245406203e-05, val loss: 0.0006411643698811531\n",
      "Training time: 10.014298915863037\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.33475810289382935, val loss: 0.005513766314834356\n",
      "Epoch 10, train loss: 0.04116564989089966, val loss: 0.10080330073833466\n",
      "Epoch 20, train loss: 0.03655523061752319, val loss: 0.28351089358329773\n",
      "Epoch 30, train loss: 0.014409872703254223, val loss: 0.13794496655464172\n",
      "Epoch 40, train loss: 0.015591828152537346, val loss: 0.1323813796043396\n",
      "Epoch 50, train loss: 0.013484823517501354, val loss: 0.17151972651481628\n",
      "Epoch 60, train loss: 0.012503301724791527, val loss: 0.1482335478067398\n",
      "Epoch 70, train loss: 0.012080155313014984, val loss: 0.1357978880405426\n",
      "Epoch 80, train loss: 0.011397410184144974, val loss: 0.1383889764547348\n",
      "Epoch 90, train loss: 0.01065702736377716, val loss: 0.12415212392807007\n",
      "Training time: 9.965008735656738\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6453160643577576, val loss: 0.0033442662097513676\n",
      "Epoch 10, train loss: 0.03616354241967201, val loss: 0.12696418166160583\n",
      "Epoch 20, train loss: 0.01679900661110878, val loss: 0.10553672164678574\n",
      "Epoch 30, train loss: 0.014397261664271355, val loss: 0.1520930677652359\n",
      "Epoch 40, train loss: 0.007351874373853207, val loss: 0.09204648435115814\n",
      "Epoch 50, train loss: 0.002564013469964266, val loss: 0.023672591894865036\n",
      "Epoch 60, train loss: 0.0008110590861178935, val loss: 0.02121424674987793\n",
      "Epoch 70, train loss: 0.00030846873414702713, val loss: 0.001289232517592609\n",
      "Epoch 80, train loss: 0.00012576767767313868, val loss: 0.0003015189722646028\n",
      "Epoch 90, train loss: 7.92000864748843e-05, val loss: 0.0016933277947828174\n",
      "Epoch 100, train loss: 5.538942423299886e-05, val loss: 0.0002556073304731399\n",
      "Epoch 110, train loss: 4.14746355090756e-05, val loss: 0.0004321433953009546\n",
      "Epoch 120, train loss: 4.145022467128001e-05, val loss: 0.0004876712046097964\n",
      "Epoch 130, train loss: 3.8977952499408275e-05, val loss: 0.0003193463198840618\n",
      "Epoch 140, train loss: 3.702117464854382e-05, val loss: 0.00037289928877726197\n",
      "Epoch 150, train loss: 3.5762976040132344e-05, val loss: 0.0003149969852529466\n",
      "Epoch 160, train loss: 3.4710417821770534e-05, val loss: 0.00028971213032491505\n",
      "Epoch 170, train loss: 3.3742468076525256e-05, val loss: 0.0002784593962132931\n",
      "Epoch 180, train loss: 3.2858519261935726e-05, val loss: 0.00025317544350400567\n",
      "Epoch 190, train loss: 3.2056294003268704e-05, val loss: 0.0002416931965854019\n",
      "Training time: 20.206751108169556\n",
      "Training GRU with hidden_dim=32, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.9253618717193604, val loss: 0.18977956473827362\n",
      "Epoch 10, train loss: 0.3803461492061615, val loss: 0.022486865520477295\n",
      "Epoch 20, train loss: 0.037467170506715775, val loss: 0.05954108014702797\n",
      "Epoch 30, train loss: 0.04380914196372032, val loss: 0.24068915843963623\n",
      "Epoch 40, train loss: 0.009884249418973923, val loss: 0.10501525551080704\n",
      "Epoch 50, train loss: 0.014155215583741665, val loss: 0.08046156167984009\n",
      "Epoch 60, train loss: 0.009532000869512558, val loss: 0.11559315025806427\n",
      "Epoch 70, train loss: 0.009747087024152279, val loss: 0.1172398254275322\n",
      "Epoch 80, train loss: 0.009025589562952518, val loss: 0.09727755934000015\n",
      "Epoch 90, train loss: 0.008629001677036285, val loss: 0.09737720340490341\n",
      "Epoch 100, train loss: 0.008279518224298954, val loss: 0.09681817144155502\n",
      "Epoch 110, train loss: 0.007870413362979889, val loss: 0.08872822672128677\n",
      "Epoch 120, train loss: 0.007456460501998663, val loss: 0.08446330577135086\n",
      "Epoch 130, train loss: 0.007013217080384493, val loss: 0.07967764884233475\n",
      "Epoch 140, train loss: 0.006533126346766949, val loss: 0.073175810277462\n",
      "Epoch 150, train loss: 0.006012619473040104, val loss: 0.06725816428661346\n",
      "Epoch 160, train loss: 0.005445004440844059, val loss: 0.06029869243502617\n",
      "Epoch 170, train loss: 0.004822620190680027, val loss: 0.05285453051328659\n",
      "Epoch 180, train loss: 0.004137981217354536, val loss: 0.044725898653268814\n",
      "Epoch 190, train loss: 0.0033862271811813116, val loss: 0.03587302938103676\n",
      "Training time: 19.727036952972412\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6211552619934082, val loss: 0.012776517309248447\n",
      "Epoch 10, train loss: 0.023455901071429253, val loss: 0.3009481430053711\n",
      "Epoch 20, train loss: 0.013686033897101879, val loss: 0.15396659076213837\n",
      "Epoch 30, train loss: 0.007871577516198158, val loss: 0.05010426416993141\n",
      "Epoch 40, train loss: 0.001944176503457129, val loss: 0.027289526537060738\n",
      "Epoch 50, train loss: 0.0006639778730459511, val loss: 0.002581316977739334\n",
      "Epoch 60, train loss: 0.00020079145906493068, val loss: 0.007419680245220661\n",
      "Epoch 70, train loss: 0.00014776179159525782, val loss: 0.0004332824028097093\n",
      "Epoch 80, train loss: 0.00011638538853731006, val loss: 0.001865822123363614\n",
      "Epoch 90, train loss: 7.686288154218346e-05, val loss: 0.0009739000233821571\n",
      "Training time: 14.626538515090942\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.46805813908576965, val loss: 0.030287493020296097\n",
      "Epoch 10, train loss: 0.06696755439043045, val loss: 0.06686536967754364\n",
      "Epoch 20, train loss: 0.04424823448061943, val loss: 0.3011028468608856\n",
      "Epoch 30, train loss: 0.016887666657567024, val loss: 0.131449893116951\n",
      "Epoch 40, train loss: 0.016034772619605064, val loss: 0.14321406185626984\n",
      "Epoch 50, train loss: 0.014469398185610771, val loss: 0.18066662549972534\n",
      "Epoch 60, train loss: 0.012413342483341694, val loss: 0.13873925805091858\n",
      "Epoch 70, train loss: 0.011184426955878735, val loss: 0.12856246531009674\n",
      "Epoch 80, train loss: 0.009625391103327274, val loss: 0.11339815706014633\n",
      "Epoch 90, train loss: 0.007522763218730688, val loss: 0.08385828137397766\n",
      "Training time: 14.570168256759644\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.46106261014938354, val loss: 0.02627250924706459\n",
      "Epoch 10, train loss: 0.02598917856812477, val loss: 0.2851395905017853\n",
      "Epoch 20, train loss: 0.014697864651679993, val loss: 0.2016870528459549\n",
      "Epoch 30, train loss: 0.009823701344430447, val loss: 0.11461891233921051\n",
      "Epoch 40, train loss: 0.0007596906507387757, val loss: 0.03735695779323578\n",
      "Epoch 50, train loss: 0.000666746636852622, val loss: 0.0016636939253658056\n",
      "Epoch 60, train loss: 0.00031969023984856904, val loss: 0.005244183819741011\n",
      "Epoch 70, train loss: 0.0001657317770877853, val loss: 0.000701019715052098\n",
      "Epoch 80, train loss: 0.00011704291682690382, val loss: 0.0015998074086382985\n",
      "Epoch 90, train loss: 8.839376096148044e-05, val loss: 0.0011742600472643971\n",
      "Epoch 100, train loss: 7.949788414407521e-05, val loss: 0.0010720026912167668\n",
      "Epoch 110, train loss: 7.451912097167224e-05, val loss: 0.0008828931604512036\n",
      "Epoch 120, train loss: 6.892374221934006e-05, val loss: 0.0007622100529260933\n",
      "Epoch 130, train loss: 6.423550075851381e-05, val loss: 0.0006729750311933458\n",
      "Epoch 140, train loss: 5.950576451141387e-05, val loss: 0.0005845091654919088\n",
      "Epoch 150, train loss: 5.372688974603079e-05, val loss: 0.00044644795707426965\n",
      "Epoch 160, train loss: 5.060502007836476e-05, val loss: 0.0003787245077546686\n",
      "Epoch 170, train loss: 4.82376926811412e-05, val loss: 0.00034077948657795787\n",
      "Epoch 180, train loss: 4.60388146166224e-05, val loss: 0.0003222486120648682\n",
      "Epoch 190, train loss: 4.407868254929781e-05, val loss: 0.0003043751639779657\n",
      "Training time: 29.17952799797058\n",
      "Training GRU with hidden_dim=32, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.8319099545478821, val loss: 0.1550966054201126\n",
      "Epoch 10, train loss: 0.16406764090061188, val loss: 0.010341336950659752\n",
      "Epoch 20, train loss: 0.043668799102306366, val loss: 0.31539592146873474\n",
      "Epoch 30, train loss: 0.013409730046987534, val loss: 0.15053889155387878\n",
      "Epoch 40, train loss: 0.019284240901470184, val loss: 0.10597244650125504\n",
      "Epoch 50, train loss: 0.012683581560850143, val loss: 0.1623905748128891\n",
      "Epoch 60, train loss: 0.01218945998698473, val loss: 0.1472979038953781\n",
      "Epoch 70, train loss: 0.011510899290442467, val loss: 0.12126453965902328\n",
      "Epoch 80, train loss: 0.010502866469323635, val loss: 0.12610219419002533\n",
      "Epoch 90, train loss: 0.009635655209422112, val loss: 0.11153411865234375\n",
      "Epoch 100, train loss: 0.008655397221446037, val loss: 0.09821721911430359\n",
      "Epoch 110, train loss: 0.0074682412669062614, val loss: 0.08524803817272186\n",
      "Epoch 120, train loss: 0.006007572636008263, val loss: 0.06572835892438889\n",
      "Epoch 130, train loss: 0.00418820371851325, val loss: 0.04403043910861015\n",
      "Epoch 140, train loss: 0.002038165694102645, val loss: 0.018863486126065254\n",
      "Epoch 150, train loss: 0.0002751820138655603, val loss: 0.0013934571761637926\n",
      "Epoch 160, train loss: 0.000238962602452375, val loss: 0.002316534984856844\n",
      "Epoch 170, train loss: 0.00010331998782930896, val loss: 0.0006054576369933784\n",
      "Epoch 180, train loss: 9.468862117500976e-05, val loss: 0.0005437342915683985\n",
      "Epoch 190, train loss: 8.468744636047632e-05, val loss: 0.00044046054244972765\n",
      "Training time: 29.177500247955322\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.45067447423934937, val loss: 0.09319101274013519\n",
      "Epoch 10, train loss: 0.01286376267671585, val loss: 0.23164768517017365\n",
      "Epoch 20, train loss: 0.02596220001578331, val loss: 0.08712123334407806\n",
      "Epoch 30, train loss: 0.009102228097617626, val loss: 0.09028874337673187\n",
      "Epoch 40, train loss: 0.008198840543627739, val loss: 0.08096450567245483\n",
      "Epoch 50, train loss: 0.0023720869794487953, val loss: 0.014703534543514252\n",
      "Epoch 60, train loss: 0.001040955656208098, val loss: 0.014486078172922134\n",
      "Epoch 70, train loss: 0.0003311285108793527, val loss: 0.0015324951382353902\n",
      "Epoch 80, train loss: 8.057086233748123e-05, val loss: 0.0007114647305570543\n",
      "Epoch 90, train loss: 9.186835086438805e-05, val loss: 0.001059367205016315\n",
      "Training time: 16.900153636932373\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.881351888179779, val loss: 0.15470337867736816\n",
      "Epoch 10, train loss: 0.047076739370822906, val loss: 0.10882677882909775\n",
      "Epoch 20, train loss: 0.0160228218883276, val loss: 0.15039895474910736\n",
      "Epoch 30, train loss: 0.027021408081054688, val loss: 0.07893539220094681\n",
      "Epoch 40, train loss: 0.015556985512375832, val loss: 0.18137088418006897\n",
      "Epoch 50, train loss: 0.011107921600341797, val loss: 0.11170928180217743\n",
      "Epoch 60, train loss: 0.01024953369051218, val loss: 0.12000787258148193\n",
      "Epoch 70, train loss: 0.009690588340163231, val loss: 0.1165330559015274\n",
      "Epoch 80, train loss: 0.008987139910459518, val loss: 0.10098269581794739\n",
      "Epoch 90, train loss: 0.008238270878791809, val loss: 0.09866108745336533\n",
      "Training time: 16.914608478546143\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.88779616355896, val loss: 0.003132807556539774\n",
      "Epoch 10, train loss: 0.042946022003889084, val loss: 0.17918793857097626\n",
      "Epoch 20, train loss: 0.018427951261401176, val loss: 0.1485578566789627\n",
      "Epoch 30, train loss: 0.007371153682470322, val loss: 0.09172406792640686\n",
      "Epoch 40, train loss: 0.003584860824048519, val loss: 0.03941381722688675\n",
      "Epoch 50, train loss: 0.00027009102632291615, val loss: 0.0003416916588321328\n",
      "Epoch 60, train loss: 0.0005704259965568781, val loss: 0.0021274148020893335\n",
      "Epoch 70, train loss: 9.398002293892205e-05, val loss: 0.0007607879815623164\n",
      "Epoch 80, train loss: 7.739376451354474e-05, val loss: 0.000269431242486462\n",
      "Epoch 90, train loss: 5.7589382777223364e-05, val loss: 0.0006653215968981385\n",
      "Epoch 100, train loss: 4.876676030107774e-05, val loss: 0.0002886149741243571\n",
      "Epoch 110, train loss: 4.4227945181773975e-05, val loss: 0.00025809623184613883\n",
      "Epoch 120, train loss: 4.161314063821919e-05, val loss: 0.0003171961579937488\n",
      "Epoch 130, train loss: 4.0570048440713435e-05, val loss: 0.00028741505229845643\n",
      "Epoch 140, train loss: 3.980079782195389e-05, val loss: 0.00026854051975533366\n",
      "Epoch 150, train loss: 3.899205330526456e-05, val loss: 0.0002763803640846163\n",
      "Epoch 160, train loss: 3.825137173407711e-05, val loss: 0.00026609000633470714\n",
      "Epoch 170, train loss: 3.754188946913928e-05, val loss: 0.00025748589541763067\n",
      "Epoch 180, train loss: 3.686786658363417e-05, val loss: 0.00025357765844091773\n",
      "Epoch 190, train loss: 3.622676376835443e-05, val loss: 0.00024585003848187625\n",
      "Training time: 33.70123219490051\n",
      "Training GRU with hidden_dim=64, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7875856757164001, val loss: 0.1272062212228775\n",
      "Epoch 10, train loss: 0.11433234810829163, val loss: 0.029144588857889175\n",
      "Epoch 20, train loss: 0.0441737100481987, val loss: 0.23532778024673462\n",
      "Epoch 30, train loss: 0.025070693343877792, val loss: 0.0679713785648346\n",
      "Epoch 40, train loss: 0.010706344619393349, val loss: 0.14277072250843048\n",
      "Epoch 50, train loss: 0.009736593812704086, val loss: 0.11405453085899353\n",
      "Epoch 60, train loss: 0.009114318527281284, val loss: 0.09105584025382996\n",
      "Epoch 70, train loss: 0.007808166556060314, val loss: 0.09478968381881714\n",
      "Epoch 80, train loss: 0.006498577073216438, val loss: 0.06985756754875183\n",
      "Epoch 90, train loss: 0.0051931217312812805, val loss: 0.060931991785764694\n",
      "Epoch 100, train loss: 0.0038207953330129385, val loss: 0.04144951328635216\n",
      "Epoch 110, train loss: 0.002392331138253212, val loss: 0.025783712044358253\n",
      "Epoch 120, train loss: 0.0010663227876648307, val loss: 0.010677139274775982\n",
      "Epoch 130, train loss: 0.0002069678739644587, val loss: 0.0015404296573251486\n",
      "Epoch 140, train loss: 8.284729847218841e-05, val loss: 0.0006093980045989156\n",
      "Epoch 150, train loss: 0.00010769941582111642, val loss: 0.0007151198224164546\n",
      "Epoch 160, train loss: 6.551298429258168e-05, val loss: 0.00036941212601959705\n",
      "Epoch 170, train loss: 6.641193613177165e-05, val loss: 0.00041565843275748193\n",
      "Epoch 180, train loss: 6.491612293757498e-05, val loss: 0.00039079823181964457\n",
      "Epoch 190, train loss: 6.290355668170378e-05, val loss: 0.00036529460339806974\n",
      "Training time: 33.804187059402466\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.7703887224197388, val loss: 0.13990101218223572\n",
      "Epoch 10, train loss: 0.0456853061914444, val loss: 0.44131606817245483\n",
      "Epoch 20, train loss: 0.029298067092895508, val loss: 0.31219881772994995\n",
      "Epoch 30, train loss: 0.015755733475089073, val loss: 0.19242273271083832\n",
      "Epoch 40, train loss: 0.015096203424036503, val loss: 0.12877051532268524\n",
      "Epoch 50, train loss: 0.009629478678107262, val loss: 0.08081387728452682\n",
      "Epoch 60, train loss: 0.0003046802303288132, val loss: 0.025595135986804962\n",
      "Epoch 70, train loss: 0.00082877412205562, val loss: 0.004101287107914686\n",
      "Epoch 80, train loss: 0.00019321950094308704, val loss: 0.005584742408245802\n",
      "Epoch 90, train loss: 0.00011846738925669342, val loss: 0.00043284305138513446\n",
      "Training time: 25.10231065750122\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.47716858983039856, val loss: 0.02391560934484005\n",
      "Epoch 10, train loss: 0.07719770073890686, val loss: 0.36510711908340454\n",
      "Epoch 20, train loss: 0.030942626297473907, val loss: 0.08424073457717896\n",
      "Epoch 30, train loss: 0.015427527949213982, val loss: 0.19734296202659607\n",
      "Epoch 40, train loss: 0.012255973182618618, val loss: 0.12189159542322159\n",
      "Epoch 50, train loss: 0.010407086461782455, val loss: 0.1287705898284912\n",
      "Epoch 60, train loss: 0.008516104891896248, val loss: 0.0922621339559555\n",
      "Epoch 70, train loss: 0.00597033416852355, val loss: 0.06844307482242584\n",
      "Epoch 80, train loss: 0.0024413717910647392, val loss: 0.02133416198194027\n",
      "Epoch 90, train loss: 9.23292973311618e-05, val loss: 0.0019374802941456437\n",
      "Training time: 25.424776792526245\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.6425154209136963, val loss: 0.14378906786441803\n",
      "Epoch 10, train loss: 0.048818785697221756, val loss: 0.4188036620616913\n",
      "Epoch 20, train loss: 0.024816012009978294, val loss: 0.2946591377258301\n",
      "Epoch 30, train loss: 0.015409828163683414, val loss: 0.16247376799583435\n",
      "Epoch 40, train loss: 0.011851401068270206, val loss: 0.08893688023090363\n",
      "Epoch 50, train loss: 0.0002441646938677877, val loss: 0.042789582163095474\n",
      "Epoch 60, train loss: 0.0011789757991209626, val loss: 0.006963582709431648\n",
      "Epoch 70, train loss: 0.0004166924918536097, val loss: 0.012180529534816742\n",
      "Epoch 80, train loss: 0.00019234162755310535, val loss: 0.00046521989861503243\n",
      "Epoch 90, train loss: 0.00010930768621619791, val loss: 0.0025169900618493557\n",
      "Epoch 100, train loss: 8.898544911062345e-05, val loss: 0.000496549648232758\n",
      "Epoch 110, train loss: 7.941845251480117e-05, val loss: 0.0010191549081355333\n",
      "Epoch 120, train loss: 7.5014031608589e-05, val loss: 0.000522006128448993\n",
      "Epoch 130, train loss: 7.108074350981042e-05, val loss: 0.0006204336532391608\n",
      "Epoch 140, train loss: 6.813587970100343e-05, val loss: 0.00046783764264546335\n",
      "Epoch 150, train loss: 6.55958938295953e-05, val loss: 0.00047850579721853137\n",
      "Epoch 160, train loss: 6.339225365081802e-05, val loss: 0.0004175332433078438\n",
      "Epoch 170, train loss: 6.140754703665152e-05, val loss: 0.0004041203937958926\n",
      "Epoch 180, train loss: 5.9614027122734115e-05, val loss: 0.00038004000089131296\n",
      "Epoch 190, train loss: 5.7982662838185206e-05, val loss: 0.0003673383907880634\n",
      "Training time: 50.245662212371826\n",
      "Training GRU with hidden_dim=64, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.5470215678215027, val loss: 0.040105655789375305\n",
      "Epoch 10, train loss: 0.04712223634123802, val loss: 0.3924110233783722\n",
      "Epoch 20, train loss: 0.025536587461829185, val loss: 0.10143687576055527\n",
      "Epoch 30, train loss: 0.013928528875112534, val loss: 0.18129241466522217\n",
      "Epoch 40, train loss: 0.012766901403665543, val loss: 0.14824305474758148\n",
      "Epoch 50, train loss: 0.01107406709343195, val loss: 0.12208253145217896\n",
      "Epoch 60, train loss: 0.008303046226501465, val loss: 0.0943952426314354\n",
      "Epoch 70, train loss: 0.004595423582941294, val loss: 0.051179371774196625\n",
      "Epoch 80, train loss: 0.00030738397617824376, val loss: 0.0012774162460118532\n",
      "Epoch 90, train loss: 0.000498360488563776, val loss: 0.0037448389921337366\n",
      "Epoch 100, train loss: 0.00019487985991872847, val loss: 0.0017060815589502454\n",
      "Epoch 110, train loss: 0.0001047151381499134, val loss: 0.0005988955381326377\n",
      "Epoch 120, train loss: 0.00010425670188851655, val loss: 0.0006561076152138412\n",
      "Epoch 130, train loss: 8.254400745499879e-05, val loss: 0.0004722938174381852\n",
      "Epoch 140, train loss: 8.305659866891801e-05, val loss: 0.00046653588651679456\n",
      "Epoch 150, train loss: 8.14653467386961e-05, val loss: 0.00044594937935471535\n",
      "Epoch 160, train loss: 7.998719956958666e-05, val loss: 0.0004456960887182504\n",
      "Epoch 170, train loss: 7.946763071231544e-05, val loss: 0.0004443657526280731\n",
      "Epoch 180, train loss: 7.88554098107852e-05, val loss: 0.0004387212684378028\n",
      "Epoch 190, train loss: 7.824165368219838e-05, val loss: 0.00044002404320053756\n",
      "Training time: 50.30681657791138\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.6436176300048828, val loss: 0.5026065111160278\n",
      "Epoch 10, train loss: 0.06197480484843254, val loss: 0.05005054548382759\n",
      "Epoch 20, train loss: 0.02286515384912491, val loss: 0.06487387418746948\n",
      "Epoch 30, train loss: 0.009972224943339825, val loss: 0.039651963859796524\n",
      "Epoch 40, train loss: 0.0006711697787977755, val loss: 0.008758393116295338\n",
      "Epoch 50, train loss: 0.0011708661913871765, val loss: 0.004969223868101835\n",
      "Epoch 60, train loss: 0.0003848352935165167, val loss: 0.0010147126158699393\n",
      "Epoch 70, train loss: 9.632498404243961e-05, val loss: 0.0004848228127229959\n",
      "Epoch 80, train loss: 9.538089216221124e-05, val loss: 0.0005115100066177547\n",
      "Epoch 90, train loss: 4.932130832457915e-05, val loss: 0.0002647323126439005\n",
      "Training time: 39.4725296497345\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5526145100593567, val loss: 0.02922411449253559\n",
      "Epoch 10, train loss: 0.048134852200746536, val loss: 0.21775253117084503\n",
      "Epoch 20, train loss: 0.025126220658421516, val loss: 0.08219333738088608\n",
      "Epoch 30, train loss: 0.015442493371665478, val loss: 0.1565321981906891\n",
      "Epoch 40, train loss: 0.01085616834461689, val loss: 0.08499877899885178\n",
      "Epoch 50, train loss: 0.007994432002305984, val loss: 0.0951816663146019\n",
      "Epoch 60, train loss: 0.0054696956649422646, val loss: 0.0584394596517086\n",
      "Epoch 70, train loss: 0.002936943434178829, val loss: 0.028765268623828888\n",
      "Epoch 80, train loss: 0.0004584391135722399, val loss: 0.002595428377389908\n",
      "Epoch 90, train loss: 0.0003700458037201315, val loss: 0.004087462555617094\n",
      "Training time: 40.34808850288391\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5313113331794739, val loss: 0.4653064012527466\n",
      "Epoch 10, train loss: 0.04767022654414177, val loss: 0.08873018622398376\n",
      "Epoch 20, train loss: 0.015833990648388863, val loss: 0.09384404122829437\n",
      "Epoch 30, train loss: 0.005604106932878494, val loss: 0.05369231104850769\n",
      "Epoch 40, train loss: 0.001914793043397367, val loss: 0.0004251872596796602\n",
      "Epoch 50, train loss: 0.0009843982988968492, val loss: 0.0005752724828198552\n",
      "Epoch 60, train loss: 0.00022220227401703596, val loss: 0.0016120047075673938\n",
      "Epoch 70, train loss: 0.00016743653395678848, val loss: 0.000634022697340697\n",
      "Epoch 80, train loss: 4.887067916570231e-05, val loss: 0.0002701783087104559\n",
      "Epoch 90, train loss: 4.2660092731239274e-05, val loss: 0.00023759753094054759\n",
      "Epoch 100, train loss: 4.1394581785425544e-05, val loss: 0.00022988105774857104\n",
      "Epoch 110, train loss: 3.869123611366376e-05, val loss: 0.00024786009453237057\n",
      "Epoch 120, train loss: 3.702191315824166e-05, val loss: 0.00022403558250516653\n",
      "Epoch 130, train loss: 3.624394958023913e-05, val loss: 0.0002333727607037872\n",
      "Epoch 140, train loss: 3.541731712175533e-05, val loss: 0.00022813100076746196\n",
      "Epoch 150, train loss: 3.464331166469492e-05, val loss: 0.0002299911284353584\n",
      "Epoch 160, train loss: 3.400967761990614e-05, val loss: 0.0002294565929332748\n",
      "Epoch 170, train loss: 3.340654438943602e-05, val loss: 0.00022945801902096719\n",
      "Epoch 180, train loss: 3.2851177820703015e-05, val loss: 0.0002319881896255538\n",
      "Epoch 190, train loss: 3.233825555071235e-05, val loss: 0.00023266552307177335\n",
      "Training time: 82.50569558143616\n",
      "Training GRU with hidden_dim=128, num_layers=2, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.7202253937721252, val loss: 0.08424252271652222\n",
      "Epoch 10, train loss: 0.036060530692338943, val loss: 0.14481885731220245\n",
      "Epoch 20, train loss: 0.022606423124670982, val loss: 0.06711264699697495\n",
      "Epoch 30, train loss: 0.013205979950726032, val loss: 0.12282273173332214\n",
      "Epoch 40, train loss: 0.009745472110807896, val loss: 0.07242665439844131\n",
      "Epoch 50, train loss: 0.0073733762837946415, val loss: 0.08632002770900726\n",
      "Epoch 60, train loss: 0.00581062538549304, val loss: 0.06192620098590851\n",
      "Epoch 70, train loss: 0.004529442638158798, val loss: 0.05206092819571495\n",
      "Epoch 80, train loss: 0.003371274098753929, val loss: 0.03956947103142738\n",
      "Epoch 90, train loss: 0.002175218891352415, val loss: 0.022697463631629944\n",
      "Epoch 100, train loss: 0.0010039719054475427, val loss: 0.009424898773431778\n",
      "Epoch 110, train loss: 0.00020363130897749215, val loss: 0.0011923020938411355\n",
      "Epoch 120, train loss: 8.200737647712231e-05, val loss: 0.000963491213042289\n",
      "Epoch 130, train loss: 9.943730401573703e-05, val loss: 0.0009752531186677516\n",
      "Epoch 140, train loss: 6.0312879213597625e-05, val loss: 0.00037413445534184575\n",
      "Epoch 150, train loss: 6.359286635415629e-05, val loss: 0.000352655682945624\n",
      "Epoch 160, train loss: 6.050494994269684e-05, val loss: 0.0003525528300087899\n",
      "Epoch 170, train loss: 5.92571341258008e-05, val loss: 0.0003865534672513604\n",
      "Epoch 180, train loss: 5.913518543820828e-05, val loss: 0.0003883971075993031\n",
      "Epoch 190, train loss: 5.865715866093524e-05, val loss: 0.00036997845745645463\n",
      "Training time: 82.01510763168335\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.01\n",
      "Epoch 0, train loss: 0.5819313526153564, val loss: 0.4575707018375397\n",
      "Epoch 10, train loss: 0.01812145859003067, val loss: 0.0775531530380249\n",
      "Epoch 20, train loss: 0.015161301009356976, val loss: 0.04391475021839142\n",
      "Epoch 30, train loss: 0.006964513100683689, val loss: 0.00038049850263632834\n",
      "Epoch 40, train loss: 0.00018733982869889587, val loss: 0.001949979574419558\n",
      "Epoch 50, train loss: 0.00014533485227730125, val loss: 0.00041709764627739787\n",
      "Epoch 60, train loss: 0.00022778759011998773, val loss: 0.00032483908580616117\n",
      "Epoch 70, train loss: 0.00020540347031783313, val loss: 0.0003314661153126508\n",
      "Epoch 80, train loss: 9.142280032392591e-05, val loss: 0.00032836146419867873\n",
      "Epoch 90, train loss: 9.041546582011506e-05, val loss: 0.0004267061594873667\n",
      "Training time: 64.89959907531738\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=100, lr=0.001\n",
      "Epoch 0, train loss: 0.5422631502151489, val loss: 0.020257893949747086\n",
      "Epoch 10, train loss: 0.017132803797721863, val loss: 0.09980472922325134\n",
      "Epoch 20, train loss: 0.016788115724921227, val loss: 0.22281962633132935\n",
      "Epoch 30, train loss: 0.015534290112555027, val loss: 0.09877803176641464\n",
      "Epoch 40, train loss: 0.010659737512469292, val loss: 0.11837922036647797\n",
      "Epoch 50, train loss: 0.0046791015192866325, val loss: 0.050990618765354156\n",
      "Epoch 60, train loss: 0.0001606999576324597, val loss: 0.0006413221126422286\n",
      "Epoch 70, train loss: 0.00024802552070468664, val loss: 0.0020395873580127954\n",
      "Epoch 80, train loss: 0.00030034934752620757, val loss: 0.001382146612741053\n",
      "Epoch 90, train loss: 8.214623085223138e-05, val loss: 0.0007635064539499581\n",
      "Training time: 65.8698501586914\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.01\n",
      "Epoch 0, train loss: 0.5675380825996399, val loss: 1.109321117401123\n",
      "Epoch 10, train loss: 0.029699724167585373, val loss: 0.26624611020088196\n",
      "Epoch 20, train loss: 0.016895102337002754, val loss: 0.18249912559986115\n",
      "Epoch 30, train loss: 0.009633518755435944, val loss: 0.09523797780275345\n",
      "Epoch 40, train loss: 0.0008244194905273616, val loss: 0.0022043189965188503\n",
      "Epoch 50, train loss: 0.00039874407229945064, val loss: 0.0005161729059182107\n",
      "Epoch 60, train loss: 0.000401422061258927, val loss: 0.003949425183236599\n",
      "Epoch 70, train loss: 0.0001201063278131187, val loss: 0.0006591214914806187\n",
      "Epoch 80, train loss: 6.338644016068429e-05, val loss: 0.0004660289559978992\n",
      "Epoch 90, train loss: 7.12860855855979e-05, val loss: 0.0005786194815300405\n",
      "Epoch 100, train loss: 6.749964086338878e-05, val loss: 0.0003258862707298249\n",
      "Epoch 110, train loss: 6.272396421991289e-05, val loss: 0.0004954884643666446\n",
      "Epoch 120, train loss: 6.08948030276224e-05, val loss: 0.00038528419099748135\n",
      "Epoch 130, train loss: 6.035459227859974e-05, val loss: 0.0004112401802558452\n",
      "Epoch 140, train loss: 5.9849146055057645e-05, val loss: 0.0004127349820919335\n",
      "Epoch 150, train loss: 5.933635839028284e-05, val loss: 0.0003973259008489549\n",
      "Epoch 160, train loss: 5.8809058828046545e-05, val loss: 0.0004075840406585485\n",
      "Epoch 170, train loss: 5.8295016060583293e-05, val loss: 0.00039693433791399\n",
      "Epoch 180, train loss: 5.794992466690019e-05, val loss: 0.00040596918552182615\n",
      "Epoch 190, train loss: 0.00037813905510120094, val loss: 0.0009816245874390006\n",
      "Training time: 133.29200887680054\n",
      "Training GRU with hidden_dim=128, num_layers=3, num_epochs=200, lr=0.001\n",
      "Epoch 0, train loss: 0.6747620105743408, val loss: 0.04826625436544418\n",
      "Epoch 10, train loss: 0.01461421512067318, val loss: 0.09894024580717087\n",
      "Epoch 20, train loss: 0.015858318656682968, val loss: 0.2241145670413971\n",
      "Epoch 30, train loss: 0.0161391980946064, val loss: 0.09957966208457947\n",
      "Epoch 40, train loss: 0.012162650004029274, val loss: 0.14094410836696625\n",
      "Epoch 50, train loss: 0.007575199007987976, val loss: 0.07437705248594284\n",
      "Epoch 60, train loss: 0.0033820115495473146, val loss: 0.026664502918720245\n",
      "Epoch 70, train loss: 0.0002921026898548007, val loss: 0.0018152303528040648\n",
      "Epoch 80, train loss: 0.00033117856946773827, val loss: 0.001987751107662916\n",
      "Epoch 90, train loss: 0.00016937874897848815, val loss: 0.0010773970279842615\n",
      "Epoch 100, train loss: 9.724485425977036e-05, val loss: 0.0004403512575663626\n",
      "Epoch 110, train loss: 9.718287037685513e-05, val loss: 0.0008304334478452802\n",
      "Epoch 120, train loss: 7.866964006097987e-05, val loss: 0.0004488633421715349\n",
      "Epoch 130, train loss: 7.818006270099431e-05, val loss: 0.00044527160935103893\n",
      "Epoch 140, train loss: 7.684969750698656e-05, val loss: 0.0005237287259660661\n",
      "Epoch 150, train loss: 7.570219895569608e-05, val loss: 0.00046488651423715055\n",
      "Epoch 160, train loss: 7.521684165112674e-05, val loss: 0.00045921863056719303\n",
      "Epoch 170, train loss: 7.463157089659944e-05, val loss: 0.00047026967513374984\n",
      "Epoch 180, train loss: 7.40671093808487e-05, val loss: 0.0004573040932882577\n",
      "Epoch 190, train loss: 7.353675027843565e-05, val loss: 0.0004539074143394828\n",
      "Training time: 130.28092193603516\n",
      "Tuning time: 2318.03467130661\n"
     ]
    }
   ],
   "source": [
    "# Grid search\n",
    "tune_start_time = time.time()\n",
    "results = []\n",
    "best_mse = float('inf')\n",
    "best_model = None\n",
    "for model in models:\n",
    "    for h_dim in hidden_dim:\n",
    "        for n_layers in num_layers:\n",
    "            for n_epoch in num_epochs:\n",
    "                for lr in learning_rate:\n",
    "                    y_train_tune = torch.from_numpy(y_train).type(torch.Tensor)\n",
    "                    y_val_tune = torch.from_numpy(y_val).type(torch.Tensor)\n",
    "                    print(f'Training {model.__name__} with hidden_dim={h_dim}, num_layers={n_layers}, num_epochs={n_epoch}, lr={lr}')\n",
    "                    model_instance = model(input_dim=input_dim, hidden_dim=h_dim, num_layers=n_layers, output_dim=output_dim).to(device)\n",
    "                    criterion = nn.MSELoss()\n",
    "                    optimiser = torch.optim.Adam(model_instance.parameters(), lr=lr)\n",
    "                    train_loss, val_loss = train_model(model_instance, \n",
    "                                                       criterion, \n",
    "                                                       optimiser, \n",
    "                                                       x_train, \n",
    "                                                       y_train_tune, \n",
    "                                                       x_val = x_val, \n",
    "                                                       y_val = y_val_tune, \n",
    "                                                       num_epochs=n_epoch)\n",
    "                    \n",
    "                    # Check mse, save model if its the best\n",
    "                    mse_instance = np.mean(val_loss)\n",
    "                    \n",
    "                    if mse_instance < best_mse:\n",
    "                        best_mse = mse_instance\n",
    "                        best_model = model_instance\n",
    "                        \n",
    "                    results.append({\n",
    "                        'model': model.__name__,\n",
    "                        'hidden_dim': h_dim,\n",
    "                        'num_layers': n_layers,\n",
    "                        'num_epochs': n_epoch,\n",
    "                        'lr': lr,\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss\n",
    "                    })\n",
    "tune_time = time.time() - tune_start_time\n",
    "print(f'Tuning time: {tune_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>lr</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>GRU</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.020189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RNN</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.029123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GRU</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.029760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.030552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RNN</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.033798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.033919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>GRU</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.034273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.036491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>GRU</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.037144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  hidden_dim  num_layers  num_epochs     lr       mse\n",
       "58   GRU          64           2         200  0.010  0.020125\n",
       "66   GRU         128           2         200  0.010  0.020189\n",
       "19   RNN         128           2         200  0.001  0.029123\n",
       "54   GRU          32           3         200  0.010  0.029760\n",
       "70   GRU         128           3         200  0.010  0.030552\n",
       "23   RNN         128           3         200  0.001  0.033798\n",
       "68   GRU         128           3         100  0.010  0.033919\n",
       "50   GRU          32           2         200  0.010  0.034273\n",
       "2    RNN          32           2         200  0.010  0.036491\n",
       "67   GRU         128           2         200  0.001  0.037144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check best model\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['mse'] = results_df['val_loss'].apply(np.mean)\n",
    "clean_df = results_df.drop(columns=['train_loss', 'val_loss'])\n",
    "clean_df.sort_values(by='mse', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model values\n",
    "best_model_results = results_df.sort_values(by='mse', ascending=True).iloc[0]\n",
    "\n",
    "best_train_loss = best_model_results['train_loss']\n",
    "best_val_loss = best_model_results['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the best model\n",
    "plt.plot(best_train_loss, label='train loss')\n",
    "plt.plot(best_val_loss, label='val loss')\n",
    "plt.title('Best model loss graph')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model MSE: 0.0065952870063483715\n"
     ]
    }
   ],
   "source": [
    "y_test_best = torch.from_numpy(y_test).type(torch.Tensor)\n",
    "\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = best_model(x_test)\n",
    "    test_loss = criterion(test_outputs, y_test_best)\n",
    "    print(f'Best model MSE: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descale the data\n",
    "y_test_descaled = scaler.inverse_transform(y_test_best)\n",
    "y_test_pred = scaler.inverse_transform(test_outputs)\n",
    "\n",
    "plt.plot(y_test_pred, label='Predicted')\n",
    "plt.plot(y_test_descaled, label='Actual')\n",
    "plt.title('Best model prediction')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
